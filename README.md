# CVPR-2023-Papers
![1ad4f8f92d9208b0f4b579e426b2dcd](https://user-images.githubusercontent.com/62801906/225788627-781870be-cc92-4054-b865-e2556b88cefc.jpg)

官网链接：

## 历年综述论文分类汇总戳这里↘️[CV-Surveys](https://github.com/52CV/CV-Surveys)施工中~~~~~~~~~~

## 2023 年论文分类汇总戳这里
↘️[CVPR-2023-Papers](https://github.com/52CV/CVPR-2023-Papers)
↘️[WACV-2023-Papers](https://github.com/52CV/WACV-2023-Papers)

## 2022 年论文分类汇总戳这里
↘️[CVPR-2022-Papers](https://github.com/52CV/CVPR-2022-Papers)
↘️[WACV-2022-Papers](https://github.com/52CV/WACV-2022-Papers)
↘️[ECCV-2022-Papers](https://github.com/52CV/ECCV-2022-Papers)

## 2021年论文分类汇总戳这里
↘️[ICCV-2021-Papers](https://github.com/52CV/ICCV-2021-Papers)
↘️[CVPR-2021-Papers](https://github.com/52CV/CVPR-2021-Papers)

## 2020 年论文分类汇总戳这里
↘️[CVPR-2020-Papers](https://github.com/52CV/CVPR-2020-Papers)
↘️[ECCV-2020-Papers](https://github.com/52CV/ECCV-2020-Papers)

## 目录

|:cat:|:dog:|:tiger:|:wolf:|
|------|------|------|------|
|[1.其它](#1)|[2.Image Segmentation(图像分割)](#2)|[3.Image Progress(图像处理)](#4)|[4.Image Captioning(图像字幕)](#)|
|[5.Object Detection(目标检测)](#5)|[6.Object Tracking(目标跟踪)](#6)|[7.Point Cloud(点云)](#7)|[8.Action Detection(人体动作检测与识别)](#8)|
|[9.Human Pose Estimation(人体姿态估计)](#9)|[10.3D(三维视觉)](#10)|[11.Face](#11)|[12.Image-to-Image Translation(图像到图像翻译)](#12)|
|[13.GAN](#13)|[14.Video](#14)|[15.Transformer](#15)|[16.Semi/self-supervised learning(半/自监督)](#16)|
|[17.Medical Image(医学影像)](#17)|[18.Person Re-Identification(人员重识别)](#18)|[19.Neural Architecture Search(神经架构搜索)](#19)|[20.Autonomous vehicles(自动驾驶)](#20)|
|[21.UAV/Remote Sensing/Satellite Image(无人机/遥感/卫星图像)](#21)|[22.Image Synthesis/Generation(图像合成)](#22)|[23.Image Retrieval(图像检索)](#23)|[24.Super-Resolution(超分辨率)](#24)|
|[25.Fine-Grained/Image Classification(细粒度/图像分类)](#25)|[26.GCN/GNN](#26)|[27.Pose Estimation(物体姿势估计)](#27)|[28.Style Transfer(风格迁移)](#28)|
|[29.Augmented Reality/Virtual Reality/Robotics(增强/虚拟现实/机器人)](#29)|[30.Visual Answer Questions(视觉问答)](#30)|[31.Vision-Language(视觉语言)](#31)|[32.Data Augmentation(数据增强)](#32)|
|[33.Human-Object Interaction(人物交互)](#33)|[34.Model Compression/Knowledge Distillation/Pruning(模型压缩/知识蒸馏/剪枝)](#34)|[35.OCR](#35)|[36.Optical Flow(光流估计)](#36)|
|[37.Contrastive Learning(对比学习)](#37)|[38.Meta-Learning(元学习)](#38)|[39.Continual Learning(持续学习)](#39)|[40.Adversarial Learning(对抗学习)](#40)|
|[41.Incremental Learning(增量学习)](#41)|[42.Metric Learning(度量学习)](#42)|[43.Multi-Task Learning(多任务学习)](#43)|[44.Federated Learning(联邦学习)](#44)|
|[45.Dense Prediction(密集预测)](#45)|[46.Scene Graph Generation(场景图生成)](#46)|[47.Few/Zero-Shot Learning/DG/Adaptation(小/零样本/域泛化/适应)](#47)|[48.NLP(自然语言处理)](#48)|
|[49.Image Geo-localization(图像地理定位)](#49)|[50.Anomaly Detection(异常检测)](#50)|[51.光学、几何、光场成像](#51)|[52.Human Motion Forecasting(人体运动预测)](#52)|
|[53.Sign Language Translation(手语翻译)](#53)|[54.Benchmark/Dataset(基准/数据集)](#54)|[55.Novel View Synthesis(视图合成)](#55)|[56.Sound](#56)|
|[57.Gaze Estimation(视线估计)](#57)|[58.Neural rendering(神经渲染)](#58)|[59.Image\Video Compression(图像视频压缩)](#59)|[60.Industrial Anomaly Detection(工业缺陷检测)](#60)|
|[61.Object Re-identification(物体重识别)](#61)|[62.Object Counting(物体计数)](#62)|[63.edge detection(边缘检测)](#64)|
|[65.Scene flow estimation(场景流估计)](#65)|[66.Clustering(聚类)](#66)|

# 获奖论文:loudspeaker::loudspeaker::loudspeaker:
### :trophy:Best Paper
* [Planning-oriented Autonomous Driving](https://arxiv.org/abs/2212.10156)<br>:house:[project](https://opendrivelab.github.io/UniAD/)
* [Visual Programming: Compositional visual reasoning without training](https://arxiv.org/abs/2211.11559)
### :trophy:Best student Paper
* [3D Registration with Maximal Cliques](http://arxiv.org/abs/2305.10854v1)
### :trophy:Honorable Mention
* [DynIBaR: Neural Dynamic Image-Based Rendering](https://arxiv.org/abs/2211.11082)<br>:house:[project](http://dynibar.github.io/)
### :trophy:Honorable Mention(Student)
* [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/abs/2208.12242)<br>:house:[project](https://dreambooth.github.io/)

## 计算机图形学
* [Learning Anchor Transformations for 3D Garment Animation](http://arxiv.org/abs/2304.00761v1)<br>:star:[code](https://semanticdh.github.io/AnchorDEF)
* [Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion](http://arxiv.org/abs/2304.01893v1)<br>:star:[code](https://nv-tlabs.github.io/trace-pace)
* [CloSET: Modeling Clothed Humans on Continuous Surface with Explicit Template Decomposition](http://arxiv.org/abs/2304.03167v1)<br>:house:[project](https://www.liuyebin.com/closet)
* [FLEX: Full-Body Grasping Without Full-Body Grasps](https://openaccess.thecvf.com/content/CVPR2023/papers/Tendulkar_FLEX_Full-Body_Grasping_Without_Full-Body_Grasps_CVPR_2023_paper.pdf)<br>:house:[project](flex.cs.columbia.edu)

## thermal imaging technology(热敏成像技术)
* [What Happened 3 Seconds Ago? Inferring the Past with Thermal Imaging](https://arxiv.org/abs/2304.13651)<br>:star:[code](https://github.com/ZitianTang/Thermal-IM)

## Image/Video Editing(图像/视频编辑)
* [PREIM3D: 3D Consistent Precise Image Attribute Editing from a Single Image](https://arxiv.org/abs/2304.10263)<br>:house:[project](https://mybabyyh.github.io/Preim3D/)
* 文本驱动的视频编辑
  * [Shape-aware Text-driven Layered Video Editing](https://arxiv.org/abs/2301.13173)<br>:house:[project](https://text-video-edit.github.io/)
* Image Editing(图像编辑)
  * [CoralStyleCLIP: Co-optimized Region and Layer Selection for Image Editing](https://arxiv.org/abs/2303.05031)
  * [SIEDOB: Semantic Image Editing by Disentangling Object and Background](http://arxiv.org/abs/2303.13062v1)
  * [NULL-Text Inversion for Editing Real Images Using Guided Diffusion Models](https://arxiv.org/abs/2211.09794)
  * [InstructPix2Pix: Learning To Follow Image Editing Instructions](https://arxiv.org/abs/2211.09800)<br>:house:[project](https://www.timothybrooks.com/instruct-pix2pix)
  * [Local 3D Editing via 3D Distillation of CLIP Knowledge](https://openaccess.thecvf.com/content/CVPR2023/papers/Hyung_Local_3D_Editing_via_3D_Distillation_of_CLIP_Knowledge_CVPR_2023_paper.pdf)
  * 基于样本的图像编辑
    * [Paint by Example: Exemplar-based Image Editing with Diffusion Models](https://arxiv.org/abs/2211.13227)<br>:star:[code](https://github.com/Fantasy-Studio/Paint-by-Example)


## sketch(草图)
* [Photo Pre-Training, but for Sketch](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Photo_Pre-Training_but_for_Sketch_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/KeLi-SketchX/Photo-Pre-Training-But-for-Sketch)
* [Restoration of Hand-Drawn Architectural Drawings Using Latent Space Mapping With Degradation Generator](https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_Restoration_of_Hand-Drawn_Architectural_Drawings_Using_Latent_Space_Mapping_With_CVPR_2023_paper.pdf)
* [SECAD-Net: Self-Supervised CAD Reconstruction by Learning Sketch-Extrude Operations](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SECAD-Net_Self-Supervised_CAD_Reconstruction_by_Learning_Sketch-Extrude_Operations_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/BunnySoCrazy/SECAD-Net)

## IP protection(知识产权保护)
* [Model Barrier: A Compact Un-Transferable Isolation Domain for Model Intellectual Property Protection](https://arxiv.org/abs/2303.11078)
* [Effective Ambiguity Attack Against Passport-Based DNN Intellectual Property Protection Schemes Through Fully Connected Layer Substitution](https://arxiv.org/abs/2303.11595)

## Semantic Scene Completion(语义场景补全)
* [Semantic Scene Completion With Cleaner Self](https://arxiv.org/abs/2303.09977)
* [VoxFormer: Sparse Voxel Transformer for Camera-Based 3D Semantic Scene Completion](https://arxiv.org/abs/2302.12251)<br>:star:[code](https://github.com/NVlabs/VoxFormer)

## Machine Learning(机器学习)
* [Cooperation or Competition: Avoiding Player Domination for Multi-Target Robustness via Adaptive Budgets](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Cooperation_or_Competition_Avoiding_Player_Domination_for_Multi-Target_Robustness_via_CVPR_2023_paper.pdf )
* [Multi-Agent Automated Machine Learning](https://arxiv.org/abs/2210.09084)
* [Towards Better Decision Forests: Forest Alternating Optimization](https://openaccess.thecvf.com/content/CVPR2023/papers/Carreira-Perpinan_Towards_Better_Decision_Forests_Forest_Alternating_Optimization_CVPR_2023_paper.pdf)
* [ERM-KTP: Knowledge-Level Machine Unlearning via Knowledge Transfer](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_ERM-KTP_Knowledge-Level_Machine_Unlearning_via_Knowledge_Transfer_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/RUIYUN-ML/ERM-KTP)
* [A Whac-a-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One Amplifies Others](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_A_Whac-a-Mole_Dilemma_Shortcuts_Come_in_Multiples_Where_Mitigating_One_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/Whac-A-Mole)
* 新类别发现
  * [Bootstrap Your Own Prior: Towards Distribution-Agnostic Novel Class Discovery](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Bootstrap_Your_Own_Prior_Towards_Distribution-Agnostic_Novel_Class_Discovery_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/muliyangm/BYOP)
* 迁移学习
  * [Visual Prompt Tuning for Generative Transfer Learning](https://arxiv.org/abs/2210.00990)
  * [A Data-Based Perspective on Transfer Learning](https://arxiv.org/abs/2207.05739)<br>:star:[code](https://github.com/MadryLab/data-transfer)
  * [Manipulating Transfer Learning for Property Inference](https://arxiv.org/abs/2303.11643)<br>:star:[code](https://github.com/yulongt23/Transfer-Inference)

## Neural Radiance Fields(神经辐射场)
* [NeRFLight: Fast and Light Neural Radiance Fields using a Shared Feature Grid](https://openaccess.thecvf.com/content/CVPR2023/papers/Rivas-Manzaneque_NeRFLight_Fast_and_Light_Neural_Radiance_Fields_Using_a_Shared_CVPR_2023_paper.pdf)
* [GazeNeRF: 3D-Aware Gaze Redirection With Neural Radiance Fields](https://arxiv.org/abs/2212.04823)<br>:star:[code](https://github.com/AlessandroRuzzi/GazeNeRF)
* [SPARF: Neural Radiance Fields from Sparse and Noisy Poses](https://arxiv.org/abs/2211.11738)<br>:star:[code](https://github.com/google-research/sparf)
* [Masked Wavelet Representation for Compact Neural Radiance Fields](https://arxiv.org/abs/2212.09069)<br>:star:[code](https://github.com/daniel03c1/masked_wavelet_nerf)
* [MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures](https://arxiv.org/abs/2208.00277)<br>:star:[code](https://github.com/google-research/jax3d/tree/main/jax3d/projects/mobilenerf)
* [AligNeRF: High-Fidelity Neural Radiance Fields via Alignment-Aware Training](https://arxiv.org/abs/2211.09682)<br>:house:[project](https://yifanjiang.net/alignerf)
* [JacobiNeRF: NeRF Shaping With Mutual Information Gradients](https://arxiv.org/abs/2304.00341)
* [Robust Dynamic Radiance Fields](https://arxiv.org/abs/2301.02239)<br>:house:[project](https://robust-dynrf.github.io/)
* [Exact-NeRF: An Exploration of a Precise Volumetric Parameterization for Neural Radiance Fields](https://openaccess.thecvf.com/content/CVPR2023/papers/Isaac-Medina_Exact-NeRF_An_Exploration_of_a_Precise_Volumetric_Parameterization_for_Neural_CVPR_2023_paper.pdf)
* [PaletteNeRF: Palette-Based Appearance Editing of Neural Radiance Fields](https://arxiv.org/abs/2212.10699)
* [EditableNeRF: Editing Topologically Varying Neural Radiance Fields by Key Points](https://arxiv.org/abs/2212.04247)<br>:house:[project](https://chengwei-zheng.github.io/EditableNeRF/)
* [SinGRAF: Learning a 3D Generative Radiance Field for a Single Scene](https://arxiv.org/abs/2211.17260)<br>:house:[project](https://www.computationalimaging.org/publications/singraf/)
* [ShadowNeuS: Neural SDF Reconstruction by Shadow Ray Supervision](https://arxiv.org/abs/2211.14086)<br>:star:[code](https://github.com/gerwang/ShadowNeuS)
* [Flow supervision for Deformable NeRF](https://arxiv.org/abs/2303.16333)
* [Local-to-Global Registration for Bundle-Adjusting Neural Radiance Fields](https://arxiv.org/abs/2211.11505)<br>:house:[project](https://rover-xingyu.github.io/L2G-NeRF/)
* [EventNeRF: Neural Radiance Fields From a Single Colour Event Camera](https://arxiv.org/abs/2206.11896)<br>:house:[project](https://4dqv.mpi-inf.mpg.de/EventNeRF)
* [SeaThru-NeRF: Neural Radiance Fields in Scattering Media](https://openaccess.thecvf.com/content/CVPR2023/papers/Levy_SeaThru-NeRF_Neural_Radiance_Fields_in_Scattering_Media_CVPR_2023_paper.pdf)
* [SteerNeRF: Accelerating NeRF Rendering via Smooth Viewpoint Trajectory](https://arxiv.org/abs/2212.08476)
* [Complementary Intrinsics From Neural Radiance Fields and CNNs for Outdoor Scene Relighting](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Complementary_Intrinsics_From_Neural_Radiance_Fields_and_CNNs_for_Outdoor_CVPR_2023_paper.pdf)
* [Point2Pix: Photo-Realistic Point Cloud Rendering via Neural Radiance Fields](https://arxiv.org/abs/2303.16482)
* [Removing Objects From Neural Radiance Fields](https://arxiv.org/abs/2212.11966)
* [Grid-guided Neural Radiance Fields for Large Urban Scenes](http://arxiv.org/abs/2303.14001v1)<br>:star:[code](https://city-super.github.io/gridnerf/)
* [GM-NeRF: Learning Generalizable Model-based Neural Radiance Fields from Multi-view Images](http://arxiv.org/abs/2303.13777v1)
* [HandNeRF: Neural Radiance Fields for Animatable Interacting Hands](http://arxiv.org/abs/2303.13825v1)
* [NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects](http://arxiv.org/abs/2303.14435v1)<br>:star:[code](https://github.com/JokerYan/NeRF-DS)
* [JAWS: Just A Wild Shot for Cinematic Transfer in Neural Radiance Fields](http://arxiv.org/abs/2303.15427v1)<br>:house:[project](http://www.lix.polytechnique.fr/vista/projects/2023_cvpr_wang)
* [Multi-Space Neural Radiance Fields](http://arxiv.org/abs/2305.04268v1)<br>:star:[code](https://zx-yin.github.io/msnerf)
* [DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields](https://arxiv.org/abs/2303.14478)<br>:star:[code](https://aibluefisher.github.io/dbarf)
* [StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields](https://arxiv.org/abs/2303.10598)<br>:house:[project](https://kunhao-liu.github.io/StyleRF/)
* [Temporal Interpolation Is All You Need for Dynamic Neural Radiance Fields](https://arxiv.org/abs/2302.09311)<br>:house:[project](https://sungheonpark.github.io/tempinterpnerf)
* [SPIn-NeRF: Multiview Segmentation and Perceptual Inpainting With Neural Radiance Fields](https://openaccess.thecvf.com/content/CVPR2023/papers/Mirzaei_SPIn-NeRF_Multiview_Segmentation_and_Perceptual_Inpainting_With_Neural_Radiance_Fields_CVPR_2023_paper.pdf)
* [F2-NeRF: Fast Neural Radiance Field Training With Free Camera Trajectories](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_F2-NeRF_Fast_Neural_Radiance_Field_Training_With_Free_Camera_Trajectories_CVPR_2023_paper.pdf)<br>:house:[project](totoro97.github.io/projects/f2-nerf)
* [Clothed Human Performance Capture with a Double-layer Neural Radiance Fields](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Clothed_Human_Performance_Capture_With_a_Double-Layer_Neural_Radiance_Fields_CVPR_2023_paper.pdf)
* [DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models](https://arxiv.org/abs/2302.12231)
* 去模糊
  * [BAD-NeRF: Bundle Adjusted Deblur Neural Radiance Fields](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_BAD-NeRF_Bundle_Adjusted_Deblur_Neural_Radiance_Fields_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/WU-CVGL/BAD-NeRF)
  * [DP-NeRF: Deblurred Neural Radiance Field With Physical Scene Priors](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_DP-NeRF_Deblurred_Neural_Radiance_Field_With_Physical_Scene_Priors_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/dogyoonlee/DP-NeRF)<br>:house:[project](https://dogyoonlee.github.io/dpnerf/)

## open-set recognition(开集识别)
* [Glocal Energy-based Learning for Few-Shot Open-Set Recognition](http://arxiv.org/abs/2304.11855v1)

## visual reasoning(视觉推理)
* [Visual Programming: Compositional visual reasoning without training](https://arxiv.org/abs/2211.11559)<br>:trophy:Best Paper
* [Abstract Visual Reasoning: An Algebraic Approach for Solving Raven's Progressive Matrices](https://arxiv.org/abs/2303.11730)<br>:star:[code](https://github.com/Xu-Jingyi/AlgebraicMR)
* [Super-CLEVR: A Virtual Benchmark To Diagnose Domain Robustness in Visual Reasoning](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Super-CLEVR_A_Virtual_Benchmark_To_Diagnose_Domain_Robustness_in_Visual_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Lizw14/Super-CLEVR)
* [Unicode Analogies: An Anti-Objectivist Visual Reasoning Challenge](https://openaccess.thecvf.com/content/CVPR2023/papers/Spratley_Unicode_Analogies_An_Anti-Objectivist_Visual_Reasoning_Challenge_CVPR_2023_paper.pdf)

## Image Forgery Detection
* [Hierarchical Fine-Grained Image Forgery Detection and Localization](http://arxiv.org/abs/2303.17111v1)<br>:star:[code](https://github.com/CHELSEA234/HiFi_IFDL)
* [Detecting and Grounding Multi-Modal Media Manipulation](http://arxiv.org/abs/2304.02556v1)<br>:star:[code](https://rshaojimmy.github.io/Projects/MultiModal-DeepFake)<br>:star:[code](https://github.com/rshaojimmy/MultiModal-DeepFake)虚假信息检测
* [Evading DeepFake Detectors via Adversarial Statistical Consistency](http://arxiv.org/abs/2304.11670v1)
* [Edge-Aware Regional Message Passing Controller for Image Forgery Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Edge-Aware_Regional_Message_Passing_Controller_for_Image_Forgery_Localization_CVPR_2023_paper.pdf)
* [TruFor: Leveraging all-round clues for trustworthy image forgery detection and localization](https://arxiv.org/abs/2212.10957)<br>:house:[project](https://grip-unina.github.io/TruFor/)
* Deepfake Detection
  * [Implicit Identity Leakage: The Stumbling Block to Improving Deepfake Detection Generalization](https://arxiv.org/abs/2210.14457)<br>:star:[code](https://github.com/megvii-research/CADDM)


## Reinforcement learning(强化学习)
* [PIRLNav: Pretraining with Imitation and RL Finetuning for ObjectNav](https://arxiv.org/abs/2301.07302)
* [Local-Guided Global: Paired Similarity Representation for Visual Reinforcement Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_Local-Guided_Global_Paired_Similarity_Representation_for_Visual_Reinforcement_Learning_CVPR_2023_paper.pdf)
* [Fusing Pre-Trained Language Models With Multimodal Prompts Through Reinforcement Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Fusing_Pre-Trained_Language_Models_With_Multimodal_Prompts_Through_Reinforcement_Learning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/JiwanChung/esper)
* [Galactic: Scaling End-to-End Reinforcement Learning for Rearrangement at 100k Steps-per-Second](https://openaccess.thecvf.com/content/CVPR2023/papers/Berges_Galactic_Scaling_End-to-End_Reinforcement_Learning_for_Rearrangement_at_100k_Steps-per-Second_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/galactic)
* [Frustratingly Easy Regularization on Representation Can Boost Deep Reinforcement Learning](https://arxiv.org/abs/2205.14557)<br>:house:[project](https://sites.google.com/view/peer-cvpr2023/)

## Lifelong Learning(终身学习)
* [Task Difficulty Aware Parameter Allocation & Regularization for Lifelong Learning](http://arxiv.org/abs/2304.05288v1)<br>:star:[code](https://github.com/WenjinW/PAR)

## Active Learning(主动学习)
* [Re-thinking Federated Active Learning based on Inter-class Diversity](http://arxiv.org/abs/2303.12317v1)
* [Box-Level Active Detection](http://arxiv.org/abs/2303.13089v1)<br>:star:[code](https://github.com/lyumengyao/blad)
* [Are Binary Annotations Sufficient? Video Moment Retrieval via Hierarchical Uncertainty-Based Active Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_Are_Binary_Annotations_Sufficient_Video_Moment_Retrieval_via_Hierarchical_Uncertainty-Based_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/renjie-liang/HUAL)
* [Re-Thinking Federated Active Learning Based on Inter-Class Diversity](http://arxiv.org/abs/2303.12317)

<a name="66"/>

## 66.Clustering(聚类)
* [DivClust: Controlling Diversity in Deep Clustering](http://arxiv.org/abs/2304.01042v1)
* MVC
  * [On the Effects of Self-supervision and Contrastive Alignment in Deep Multi-view Clustering](https://arxiv.org/abs/2303.09877)<br>:star:[code](https://github.com/DanielTrosten/DeepMVC)
  * [GCFAgg: Global and Cross-View Feature Aggregation for Multi-View Clustering](https://arxiv.org/abs/2305.06799)
  * [Sample-Level Multi-View Graph Clustering](https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Sample-Level_Multi-View_Graph_Clustering_CVPR_2023_paper.pdf)
  * [On the Effects of Self-Supervision and Contrastive Alignment in Deep Multi-View Clustering](https://openaccess.thecvf.com/content/CVPR2023/papers/Trosten_On_the_Effects_of_Self-Supervision_and_Contrastive_Alignment_in_Deep_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/DanielTrosten/DeepMVC)
  

<a name="65"/>

## 65.Scene flow estimation(场景流估计)
* [Hidden Gems: 4D Radar Scene Flow Learning Using Cross-Modal Supervision](https://arxiv.org/pdf/2303.00462.pdf)<br>:star:[code](https://github.com/Toytiny/CMFlow)
* [Self-Supervised 3D Scene Flow Estimation Guided by Superpoints](http://arxiv.org/abs/2305.02528v1)
* [Unsupervised Cumulative Domain Adaptation for Foggy Scene Optical Flow](https://arxiv.org/abs/2303.07564)

<a name="64"/>

## 64.Motion Retargeting(动作重定向)
* [Skinned Motion Retargeting with Residual Perception of Motion Semantics & Geometry](https://arxiv.org/abs/2303.08658)<br>:star:[code](https://github.com/Kebii/R2ET)

<a name="63"/>

## 63.edge detection(边缘检测)
* edge detection
  * [The Treasure Beneath Multiple Annotations: An Uncertainty-aware Edge Detector](https://arxiv.org/abs/2303.11828)<br>:star:[code](https://github.com/ZhouCX117/UAED)

<a name="62"/>

## 62.Object Counting(物体计数)
* [Zero-shot Object Counting](https://arxiv.org/abs/2303.02001)<br>:star:[code](https://github.com/cvlab-stonybrook/zero-shot-counting)
* [Indiscernible Object Counting in Underwater Scenes](http://arxiv.org/abs/2304.11677v1)<br>:star:[code](https://github.com/GuoleiSun/Indiscernible-Object-Counting)

<a name="61"/>

## 61.Object Re-identification(物体重识别)
* [MSINet: Twins Contrastive Search of Multi-Scale Interaction for Object ReID](https://arxiv.org/abs/2303.07065)<br>:star:[code](https://github.com/vimar-gu/MSINet)
* [Large-scale Training Data Search for Object Re-identification](http://arxiv.org/abs/2303.16186v1)<br>:star:[code](https://github.com/yorkeyao/SnP)
* [Adaptive Sparse Pairwise Loss for Object Re-Identification](http://arxiv.org/abs/2303.18247v1)<br>:star:[code](https://github.com/Astaxanthin/AdaSP)


<a name="60"/>

## 60.Industrial Anomaly Detection(工业缺陷检测)
* 缺陷定位
  * [PyramidFlow: High-Resolution Defect Contrastive Localization using Pyramid Normalizing Flow](https://arxiv.org/abs/2303.02595)
* 工业异常检测
  * [Multimodal Industrial Anomaly Detection via Hybrid Fusion](https://arxiv.org/pdf/2303.00601.pdf)<br>:star:[code](https://github.com/nomewang/M3DM)

<a name="59"/>

## 59.Image\Video Compression(图像视频压缩)
* [Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger](https://arxiv.org/pdf/2302.14677.pdf)
* [Context-Based Trit-Plane Coding for Progressive Image Compression](https://arxiv.org/abs/2303.05715)<br>:star:[code](https://github.com/seungminjeon-github/CTC)
* [Learned Image Compression with Mixed Transformer-CNN Architectures](http://arxiv.org/abs/2303.14978v1)<br>:star:[code](https://github.com/jmliu206/LIC_TCM)
* [LVQAC: Lattice Vector Quantization Coupled with Spatially Adaptive Companding for Efficient Learned Image Compression](http://arxiv.org/abs/2304.12319v1)
* [Optimization-Inspired Cross-Attention Transformer for Compressive Sensing](http://arxiv.org/abs/2304.13986v1)<br>:star:[code](https://github.com/songjiechong/OCTUF)
* [Multi-Realism Image Compression With a Conditional Generator](https://arxiv.org/abs/2212.13824)
* [AccelIR: Task-aware Image Compression for Accelerating Neural Restoration](https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_AccelIR_Task-Aware_Image_Compression_for_Accelerating_Neural_Restoration_CVPR_2023_paper.pdf)
* 视频压缩
  * [Towards Scalable Neural Representation for Diverse Videos](http://arxiv.org/abs/2303.14124v1)
  * [HNeRV: A Hybrid Neural Representation for Videos](http://arxiv.org/abs/2304.02633v1)<br>:star:[code](https://haochen-rye.github.io/HNeRV)<br>:star:[code](https://github.com/haochen-rye/HNeRV) 
  * [Video Compression With Entropy-Constrained Neural Representations](https://openaccess.thecvf.com/content/CVPR2023/papers/Gomes_Video_Compression_With_Entropy-Constrained_Neural_Representations_CVPR_2023_paper.pdf)
  * [Complexity-Guided Slimmable Decoder for Efficient Deep Video Compression](https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Complexity-Guided_Slimmable_Decoder_for_Efficient_Deep_Video_Compression_CVPR_2023_paper.pdf)
  * [EfficientSCI: Densely Connected Network with Space-time Factorization for Large-scale Video Snapshot Compressive Imaging](https://arxiv.org/abs/2305.10006)<br>:star:[code](https://github.com/ucaswangls/EfficientSCI.git)
  * [MMVC: Learned Multi-Mode Video Compression with Block-based Prediction Mode Selection and Density-Adaptive Entropy Coding](https://arxiv.org/abs/2304.02273)
  * [Neural Video Compression With Diverse Contexts](https://arxiv.org/abs/2302.14402)<br>:star:[code](https://github.com/microsoft/DCVC)
  （ [Motion Information Propagation for Neural Video Compression](https://openaccess.thecvf.com/content/CVPR2023/papers/Qi_Motion_Information_Propagation_for_Neural_Video_Compression_CVPR_2023_paper.pdf)
* 矢量量化
  * [NVTC: Nonlinear Vector Transform Coding](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_NVTC_Nonlinear_Vector_Transform_Coding_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/USTC-IMCL/NVTC)   

<a name="58"/>

## 58.Neural rendering(神经渲染)
* [NeUDF: Leaning Neural Unsigned Distance Fields With Volume Rendering](http://arxiv.org/abs/2304.10080)
* [DiffRF: Rendering-Guided 3D Radiance Field Diffusion](https://openaccess.thecvf.com/content/CVPR2023/papers/Muller_DiffRF_Rendering-Guided_3D_Radiance_Field_Diffusion_CVPR_2023_paper.pdf)<br>:house:[project](https://sirwyver.github.io/DiffRF/)
* [Unsupervised Continual Semantic Adaptation Through Neural Rendering](https://arxiv.org/abs/2211.13969)
* [Neural Fields Meet Explicit Geometric Representations for Inverse Rendering of Urban Scenes](https://arxiv.org/abs/2304.03266)<br>:house:[project]  (https://nv-tlabs.github.io/fegr/)
* [UV Volumes for Real-Time Rendering of Editable Free-View Human Performance](https://arxiv.org/abs/2203.14402)<br>:house:[project](https://fanegg.github.io/UV-Volumes)
* [Inverse Rendering of Translucent Objects Using Physical and Neural Renderers](https://arxiv.org/abs/2305.08336)
* [ORCa: Glossy Objects As Radiance-Field Cameras](https://arxiv.org/abs/2212.04531)<br>:house:[project](https://ktiwary2.github.io/objectsascam/)
* [MAIR: Multi-View Attention Inverse Rendering With 3D Spatially-Varying Lighting Estimation](https://arxiv.org/abs/2303.12368)<br>:house:[project](https://bring728.github.io/mair.project/)
* [FlexNeRF: Photorealistic Free-viewpoint Rendering of Moving Humans from Sparse Views](https://arxiv.org/abs/2303.14368)<br>:house:[project](https://flex-nerf.github.io/)
* [Learning To Render Novel Views From Wide-Baseline Stereo Pairs](https://arxiv.org/abs/2304.08463)<br>:house:[project](https://yilundu.github.io/wide_baseline/)
* [NeRFLiX: High-Quality Neural View Synthesis by Learning a Degradation-Driven Inter-viewpoint MiXer](https://arxiv.org/abs/2303.06919)<br>:house:[project](https://redrock303.github.io/nerflix/)
* [FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization](https://arxiv.org/abs/2303.07418)<br>:house:[project](https://jiawei-yang.github.io/FreeNeRF/)
* [Local Implicit Ray Function for Generalizable Radiance Field Representation](http://arxiv.org/abs/2304.12746v1)<br>:star:[code](https://xhuangcv.github.io/lirf/)
* [FitMe: Deep Photorealistic 3D Morphable Model Avatars](http://arxiv.org/abs/2305.09641v1)<br>:star:[code](https://lattas.github.io/fitme)
* [Pointersect: Neural Rendering with Cloud-Ray Intersection](http://arxiv.org/abs/2304.12390v1)
* [Inverse Rendering of Translucent Objects using Physical and Neural Renderers](http://arxiv.org/abs/2305.08336v1)
* [Semantic Ray: Learning a Generalizable Semantic Field with Cross-Reprojection Attention](http://arxiv.org/abs/2303.13014v1)<br>:star:[code](https://liuff19.github.io/S-Ray/)
* [ABLE-NeRF: Attention-Based Rendering with Learnable Embeddings for Neural Radiance Field](http://arxiv.org/abs/2303.13817v1)
* [WildLight: In-the-wild Inverse Rendering with a Flashlight](http://arxiv.org/abs/2303.14190v1)<br>:star:[code](https://junxuan-li.github.io/wildlight-website/)
* [FlexNeRF: Photorealistic Free-viewpoint Rendering of Moving Humans from Sparse Views](http://arxiv.org/abs/2303.14368v1)<br>:star:[code](https://flex-nerf.github.io/)
* [NeFII: Inverse Rendering for Reflectance Decomposition with Near-Field Indirect Illumination](http://arxiv.org/abs/2303.16617v1)
* [MonoHuman: Animatable Human Neural Field from Monocular Video](http://arxiv.org/abs/2304.02001v1)<br>:star:[code](https://yzmblog.github.io/projects/MonoHuman/)
* [PlenVDB: A Memory Efficient VDB-Based Radiance Fields for Fast Training and Rendering]<br>:house:[project](https://plenvdb.github.io/)论文未公开<br>在 iPhone12 手机上达到了对于输出 1280x720 分辨率的画面每秒 30 帧的速率。
* [Neural Residual Radiance Fields for Streamably Free-Viewpoint Videos](http://arxiv.org/abs/2304.04452v1)<br>:star:[code](https://aoliao12138.github.io/ReRF/)
* [PlenVDB: Memory Efficient VDB-Based Radiance Fields for Fast Training and Rendering](https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_PlenVDB_Memory_Efficient_VDB-Based_Radiance_Fields_for_Fast_Training_and_CVPR_2023_paper.pdf)
* [NeFII: Inverse Rendering for Reflectance Decomposition With Near-Field Indirect Illumination](https://arxiv.org/abs/2303.16617)

<a name="57"/>

## 57.Gaze Estimation(视线估计)
* [NeRF-Gaze: A Head-Eye Redirection Parametric Model for Gaze Estimation](https://arxiv.org/abs/2212.14710)
* [Source-free Adaptive Gaze Estimation by Uncertainty Reduction](https://openaccess.thecvf.com/content/CVPR2023/papers/Cai_Source-Free_Adaptive_Gaze_Estimation_by_Uncertainty_Reduction_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/caixin1998/UnReGA)
* [ReDirTrans: Latent-to-Latent Translation for Gaze and Head Redirection](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_ReDirTrans_Latent-to-Latent_Translation_for_Gaze_and_Head_Redirection_CVPR_2023_paper.pdf)
  
<a name="56"/>

## 56.Sound + Vision(声音与视觉)
* [Conditional Generation of Audio from Video via Foley Analogies](http://arxiv.org/abs/2304.08490v1)<br>:star:[code](https://xypb.github.io/CondFoleyGen/)
* 扬声器检测
  * [A Light Weight Model for Active Speaker Detection](https://arxiv.org/abs/2303.04439)<br>:star:[code](https://github.com/Junhua-Liao/Light-ASD)
* 视听语音识别
  * [Watch or Listen: Robust Audio-Visual Speech Recognition with Visual Corruption Modeling and Reliability Scoring](https://arxiv.org/abs/2303.08536)<br>:star:[code](https://github.com/joannahong/AV-RelScore)
  * [Collecting Cross-Modal Presence-Absence Evidence for Weakly-Supervised Audio-Visual Event Perception](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Collecting_Cross-Modal_Presence-Absence_Evidence_for_Weakly-Supervised_Audio-Visual_Event_Perception_CVPR_2023_paper.pdf)<br>:star:[code](github.com/MengyuanChen21/CVPR2023-CMPAE)
  * [AVFormer: Injecting Vision into Frozen Speech Models for Zero-Shot AV-ASR](http://arxiv.org/abs/2303.16501v1)
  * [SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision](http://arxiv.org/abs/2303.17200v1)
* 视听定位
  * [Learning Audio-Visual Source Localization via False Negative Aware Contrastive Learning](https://arxiv.org/abs/2303.11302)<br>:star:[code](https://github.com/weixuansun/FNAC-AVL)
  * [Audio-Visual Grouping Network for Sound Localization from Mixtures](http://arxiv.org/abs/2303.17056v1)<br>:star:[code](https://github.com/stoneMo/AVGN)
* 音频源分离
  * [Language-Guided Audio-Visual Source Separation via Trimodal Consistency](http://arxiv.org/abs/2303.16342v1)
  * [iQuery: Instruments As Queries for Audio-Visual Sound Separation](https://arxiv.org/abs/2212.03814)
* 声音合成
  * [Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos](http://arxiv.org/abs/2303.16897v1)<br>:star:[code](https://sukun1045.github.io/video-physics-sound-diffusion/)
* 电影音频描述
  * [AutoAD: Movie Description in Context](http://arxiv.org/abs/2303.16899v1)<br>:house:[project](https://www.robots.ox.ac.uk/~vgg/research/autoad/)
* 从声音中生成场景图像
  * [Sound to Visual Scene Generation by Audio-to-Visual Latent Alignment](http://arxiv.org/abs/2303.17490v1)
* 视听异常检测
  * [Self-Supervised Video Forensics by Audio-Visual Anomaly Detection](https://arxiv.org/abs/2301.01767)<br>:star:[code](https://cfeng16.github.io/audio-visual-forensics)
* 电影配音
  * [Learning To Dub Movies via Hierarchical Prosody Models](https://arxiv.org/abs/2212.04054)
* 舞蹈生成
  * [EDGE: Editable Dance Generation From Music](https://arxiv.org/abs/2211.10658)<br>:house:[project](https://edge-dance.github.io/)
* 视频显著性预测
  * [CASP-Net: Rethinking Video Saliency Prediction From an Audio-Visual Consistency Perceptual Perspective](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_CASP-Net_Rethinking_Video_Saliency_Prediction_From_an_Audio-Visual_Consistency_Perceptual_CVPR_2023_paper.pdf)
* 音频驱动的肖像动画
  * [DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation](http://arxiv.org/abs/2301.03786)

<a name="55"/>

## 55.Novel View Synthesis(视图合成)
* [MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis from Sparse Inputs](https://arxiv.org/abs/2302.08788)<br>:house:[project](https://shawn615.github.io/mixnerf/)
* [NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis](https://arxiv.org/abs/2301.08556)<br>:house:[project](https://bland.website/spartn)
* [NeRDi: Single-View NeRF Synthesis With Language-Guided Diffusion As General Image Priors](https://arxiv.org/abs/2212.03267)
* [Novel-View Acoustic Synthesis](https://arxiv.org/abs/2301.08730)<br>:house:[project](https://vision.cs.utexas.edu/projects/nvas)
* [Cross-Guided Optimization of Radiance Fields With Multi-View Image Super-Resolution for High-Resolution Novel View Synthesis](https://openaccess.thecvf.com/content/CVPR2023/papers/Yoon_Cross-Guided_Optimization_of_Radiance_Fields_With_Multi-View_Image_Super-Resolution_for_CVPR_2023_paper.pdf)
* [Frequency-Modulated Point Cloud Rendering with Easy Editing](https://arxiv.org/abs/2303.07596)<br>:star:[code](https://github.com/yizhangphd/FreqPCR)
* [Learning Neural Duplex Radiance Fields for Real-Time View Synthesis](http://arxiv.org/abs/2304.10537v1)<br>:house:[project](http://raywzy.com/NDRF)
* [ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of Real World Objects](http://arxiv.org/abs/2304.10448v1)<br>:star:[code](https://eyecan-ai.github.io/rene)
* [Balanced Spherical Grid for Egocentric View Synthesis](http://arxiv.org/abs/2303.12408v1)
* [Progressively Optimized Local Radiance Fields for Robust View Synthesis](http://arxiv.org/abs/2303.13791v1)<br>:star:[code](https://localrf.github.io/)
* [F$^{2}$-NeRF: Fast Neural Radiance Field Training with Free Camera Trajectories](http://arxiv.org/abs/2303.15951v1)<br>:star:[code](https://totoro97.github.io/projects/f2-nerf)
* [Enhanced Stable View Synthesis](http://arxiv.org/abs/2303.17094v1)
* [Consistent View Synthesis with Pose-Guided Diffusion Models](http://arxiv.org/abs/2303.17598v1)<br>:star:[code](https://poseguided-diffusion.github.io/)
* [Learning to Render Novel Views from Wide-Baseline Stereo Pairs](http://arxiv.org/abs/2304.08463v1)<br>:star:[code](https://yilundu.github.io/wide_baseline/)
 * [Painting 3D Nature in 2D: View Synthesis of Natural Scenes From a Single Semantic Mask](https://arxiv.org/abs/2302.07224)<br>:house:[project](https://zju3dv.github.io/paintingnature/)
 * [NoPe-NeRF: Optimising Neural Radiance Field With No Pose Prior](https://openaccess.thecvf.com/content/CVPR2023/papers/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.pdf)<br>:house:[project](https://nope-nerf.active.vision)
 * [Multiscale Tensor Decomposition and Rendering Equation Encoding for View Synthesis](https://arxiv.org/abs/2303.03808)<br>:star:[code](https://github.com/imkanghan/nrff)
* [Efficient View Synthesis and 3D-Based Multi-Frame Denoising With Multiplane Feature Representations](https://arxiv.org/abs/2303.18139)
* [NeRFVS: Neural Radiance Fields for Free View Synthesis via Geometry Scaffolds](https://arxiv.org/abs/2304.06287)
* [DINER: Depth-aware Image-based NEural Radiance fields](https://arxiv.org/abs/2211.16630)<br>:house:[project](https://malteprinzler.github.io/projects/diner/diner.html)
* [RefSR-NeRF: Towards High Fidelity and Super Resolution View Synthesis](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_RefSR-NeRF_Towards_High_Fidelity_and_Super_Resolution_View_Synthesis_CVPR_2023_paper.pdf)<br>:star:[code](https://gitee.com/mindspore/models/tree/master/research/cv/RefSR-NeRF)
* [VDN-NeRF: Resolving Shape-Radiance Ambiguity via View-Dependence Normalization](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_VDN-NeRF_Resolving_Shape-Radiance_Ambiguity_via_View-Dependence_Normalization_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/BoifZ/VDN-NeRF)
* [DynIBaR: Neural Dynamic Image-Based Rendering](https://arxiv.org/abs/2211.11082)<br>:house:[project](http://dynibar.github.io/)<br>:trophy:Honorable Mention
* [Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable Categories](https://arxiv.org/abs/2211.03889)

<a name="54"/>

## 54.Benchmark/Dataset(基准/数据集)
* [Towards Artistic Image Aesthetics Assessment: A Large-Scale Dataset and a New Method](http://arxiv.org/abs/2303.15166)
* [ScaleDet: A Scalable Multi-Dataset Object Detector](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_ScaleDet_A_Scalable_Multi-Dataset_Object_Detector_CVPR_2023_paper.pdf)
* [JRDB-Pose: A Large-Scale Dataset for Multi-Person Pose Estimation and Tracking](https://openaccess.thecvf.com/content/CVPR2023/papers/Vendrow_JRDB-Pose_A_Large-Scale_Dataset_for_Multi-Person_Pose_Estimation_and_Tracking_CVPR_2023_paper.pdf)<br>:sunflower:[dataset](https://jrdb.erc.monash.edu/)
* [Architecture, Dataset and Model-Scale Agnostic Data-Free Meta-Learning](https://arxiv.org/abs/2303.11183)
* [DF-Platter: Multi-Face Heterogeneous Deepfake Dataset](https://openaccess.thecvf.com/content/CVPR2023/papers/Narayan_DF-Platter_Multi-Face_Heterogeneous_Deepfake_Dataset_CVPR_2023_paper.pdf)<br>:sunflower:[dataset](http://iab-rubric.org/df-platter-database)
* [HandsOff: Labeled Dataset Generation With No Additional Human Annotations](https://arxiv.org/abs/2212.12645)<br>:sunflower:[dataset](http://austinxu87.github.io/handsoff)
* [M6Doc: A Large-Scale Multi-Format, Multi-Type, Multi-Layout, Multi-Language, Multi-Annotation Category Dataset for Modern Document Layout Analysis](https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_M6Doc_A_Large-Scale_Multi-Format_Multi-Type_Multi-Layout_Multi-Language_Multi-Annotation_Category_Dataset_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/HCIILAB/M6Doc)
* [ShapeTalk: A Language Dataset and Framework for 3D Shape Edits and Deformations](https://openaccess.thecvf.com/content/CVPR2023/papers/Achlioptas_ShapeTalk_A_Language_Dataset_and_Framework_for_3D_Shape_Edits_CVPR_2023_paper.pdf)<br>:sunflower:[dataset](https://changeit3d.github.io/)
* [NewsNet: A Novel Dataset for Hierarchical Temporal Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_NewsNet_A_Novel_Dataset_for_Hierarchical_Temporal_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/NewsNet-Benchmark/NewsNet)
* [MISC210K: A Large-Scale Dataset for Multi-Instance Semantic Correspondence](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_MISC210K_A_Large-Scale_Dataset_for_Multi-Instance_Semantic_Correspondence_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/YXSUNMADMAX/MISC210K)
* [StarCraftImage: A Dataset for Prototyping Spatial Reasoning Methods for Multi-Agent Environments](https://openaccess.thecvf.com/content/CVPR2023/papers/Kulinski_StarCraftImage_A_Dataset_for_Prototyping_Spatial_Reasoning_Methods_for_Multi-Agent_CVPR_2023_paper.pdf)<br>:house:[project](https://starcraftdata.davidinouye.com/)
* [Habitat-Matterport 3D Semantics Dataset](https://arxiv.org/abs/2210.05633)
* [CNVid-3.5M: Build, Filter, and Pre-Train the Large-Scale Public Chinese Video-Text Dataset](https://openaccess.thecvf.com/content/CVPR2023/papers/Gan_CNVid-3.5M_Build_Filter_and_Pre-Train_the_Large-Scale_Public_Chinese_Video-Text_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/CNVid/CNVid-3.5M)<br>大规模公共中文视频文本数据集
* [FLAG3D: A 3D Fitness Activity Dataset With Language Instruction](https://arxiv.org/abs/2212.04638)<br>:house:[project](https://andytang15.github.io/FLAG3D)
* [Multi-Label Compound Expression Recognition: C-EXPR Database & Network](https://openaccess.thecvf.com/content/CVPR2023/papers/Kollias_Multi-Label_Compound_Expression_Recognition_C-EXPR_Database__Network_CVPR_2023_paper.pdf)
* [ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation](https://arxiv.org/abs/2204.13662)<br>:house:[project](https://arctic.is.tue.mpg.de/)<br>手物体操作的数据集
* [xFBD: Focused Building Damage Dataset and Analysis](https://arxiv.org/abs/2212.13876)<br>建筑物损坏数据集
* [Spring: A High-Resolution High-Detail Dataset and Benchmark for Scene Flow, Optical Flow and Stereo](https://arxiv.org/abs/2303.01943)<br>:sunflower:[dataset](https://spring-benchmark.org/)
* [Human-Art: A Versatile Human-Centric Dataset Bridging Natural and Artificial Scenes](https://arxiv.org/abs/2303.02760)
* [HairStep: Transfer Synthetic to Real Using Strand and Depth Maps for Single-View 3D Hair Modeling](https://arxiv.org/abs/2303.02700)<br>:sunflower:[dataset](https://paulyzheng.github.io/research/hairstep/)
* [CUDA: Convolution-based Unlearnable Datasets](https://arxiv.org/abs/2303.04278)<br>:sunflower:[dataset](https://github.com/vinusankars/Convolution-based-Unlearnability)
* [MVImgNet: A Large-scale Dataset of Multi-view Images](https://arxiv.org/abs/2303.06042)<br>:sunflower:[dataset](https://gaplab.cuhk.edu.cn/projects/MVImgNet/)
* [V2V4Real: A Real-world Large-scale Dataset for Vehicle-to-Vehicle Cooperative Perception](https://arxiv.org/abs/2303.07601)<br>:sunflower:[dataset](https://github.com/ucla-mobility/V2V4Real)<br>Vehicle-to-Vehicle(V2V)感知
* [Polynomial Implicit Neural Representations For Large Diverse Datasets](https://arxiv.org/abs/2303.11424)<br>:sunflower:[dataset](https://github.com/Rajhans0/Poly_INR)
* [MaskCon: Masked Contrastive Learning for Coarse-Labelled Dataset](http://arxiv.org/abs/2303.12756v1)<br>:sunflower:[dataset](https://github.com/MrChenFeng/MaskCon_CVPR2023)
* [RaBit: Parametric Modeling of 3D Biped Cartoon Characters with a Topological-consistent Dataset](http://arxiv.org/abs/2303.12564v1)<br>:sunflower:[dataset](https://gaplab.cuhk.edu.cn/projects/RaBit/)
* [Fantastic Breaks: A Dataset of Paired 3D Scans of Real-World Broken Objects and Their Complete Counterparts](http://arxiv.org/abs/2303.14152v1)<br>:star:[code](https://terascale-all-sensing-research-studio.github.io/FantasticBreaks)
* [ARKitTrack: A New Diverse Dataset for Tracking Using Mobile RGB-D Data](http://arxiv.org/abs/2303.13885v1)<br>:star:[code](https://arkittrack.github.io)
* [CelebV-Text: A Large-Scale Facial Text-Video Dataset](http://arxiv.org/abs/2303.14717v1)<br>:star:[code](https://celebv-text.github.io/)<br>人脸文本到视频生成
* [Towards Artistic Image Aesthetics Assessment: a Large-scale Dataset and a New Method](http://arxiv.org/abs/2303.15166v1)<br>:star:[code](https://github.com/Dreemurr-T/BAID.git)<br>艺术图像美学评估
* [CIMI4D: A Large Multimodal Climbing Motion Dataset under Human-scene Interactions](http://arxiv.org/abs/2303.17948v1)<br>:house:[project](http://www.lidarhumanmotion.net/cimi4d/)<br>攀爬动作数据集
* [Uncurated Image-Text Datasets: Shedding Light on Demographic Bias](http://arxiv.org/abs/2304.02828v1)
* [AutoShot: A Short Video Dataset and State-of-the-Art Shot Boundary Detection](http://arxiv.org/abs/2304.06116v1)<br>:star:[code](https://github.com/wentaozhu/AutoShot.git)<br>:house:[project](https://paperswithcode.com/paper/autoshot-a-short-video-dataset-and-state-of)公共短视频镜头边界检测数据集
* [V2X-Seq: A Large-Scale Sequential Dataset for Vehicle-Infrastructure Cooperative Perception and Forecasting](http://arxiv.org/abs/2305.05938v1)<br>:star:[code](https://github.com/AIR-THU/DAIR-V2X-Seq)
* [WEDGE: A multi-weather autonomous driving dataset built from generative vision-language models](http://arxiv.org/abs/2305.07528v1)<br>:star:[code](https://infernolia.github.io/WEDGE)用于极端天气条件下的物体检测和天气分类任务的合成数据集
* [CLOTH4D: A Dataset for Clothed Human Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Zou_CLOTH4D_A_Dataset_for_Clothed_Human_Reconstruction_CVPR_2023_paper.pdf)<br>:sunflower:[dataset](http://www.github.com/AemikaChow/AiDLab-fAshIon-Data)<br>用于穿衣服人体重建的数据集
* [OmniCity: Omnipotent City Understanding With Multi-Level and Multi-View Images](https://arxiv.org/abs/2208.00928)<br>:sunflower:[dataset](https://city-super.github.io/omnicity)<br>从多层次和多视图图像中获取全能城市理解的新数据集。
* [RealImpact: A Dataset of Impact Sound Fields for Real Objects](https://openaccess.thecvf.com/content/CVPR2023/papers/Clarke_RealImpact_A_Dataset_of_Impact_Sound_Fields_for_Real_Objects_CVPR_2023_paper.pdf)<br>:star:[code](https://samuelpclarke.com/realimpact/)
* [BEDLAM: A Synthetic Dataset of Bodies Exhibiting Detailed Lifelike Animated Motion](https://openaccess.thecvf.com/content/CVPR2023/papers/Black_BEDLAM_A_Synthetic_Dataset_of_Bodies_Exhibiting_Detailed_Lifelike_Animated_CVPR_2023_paper.pdf)<br>:house:[project](https://bedlam.is.tue.mpg.de/)
* [GFIE:A Dataset and Baseline for Gaze-Following From 2D to 3D in Indoor Environments](https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_GFIE_A_Dataset_and_Baseline_for_Gaze-Following_From_2D_to_CVPR_2023_paper.pdf)<br>:house:[project](https://sites.google.com/view/gfie)
* Benchmark(基准)
  * [A Soma Segmentation Benchmark in Full Adult Fly Brain](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_A_Soma_Segmentation_Benchmark_in_Full_Adult_Fly_Brain_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/liuxy1103/EMADS)
  * [MammalNet: A Large-Scale Video Benchmark for Mammal Recognition and Behavior Understanding](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_MammalNet_A_Large-Scale_Video_Benchmark_for_Mammal_Recognition_and_Behavior_CVPR_2023_paper.pdf)
  * [Omni3D: A Large Benchmark and Model for 3D Object Detection in the Wild](https://arxiv.org/abs/2207.10660)<br>:house:[project](https://omni3d.garrickbrazil.com/)
  * [Advancing Visual Grounding With Scene Knowledge: Benchmark and Method](https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Advancing_Visual_Grounding_With_Scene_Knowledge_Benchmark_and_Method_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/zhjohnchan/SK-VG)
  * [The ObjectFolder Benchmark: Multisensory Learning With Neural and Real Objects](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_The_ObjectFolder_Benchmark_Multisensory_Learning_With_Neural_and_Real_Objects_CVPR_2023_paper.pdf)<br>:star:[code](https://objectfolder.stanford.edu/)
  * [Meta Omnium: A Benchmark for General-Purpose Learning-to-Learn](http://arxiv.org/abs/2305.07625v1)<br>:star:[code](https://edi-meta-learning.github.io/meta-omnium)
  * [A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation](https://arxiv.org/abs/2303.09165)<br>:star:[code](https://github.com/huitangtang/On_the_Utility_of_Synthetic_Data)
  * [GeoNet: Benchmarking Unsupervised Adaptation across Geographies](http://arxiv.org/abs/2303.15443v1)<br>:star:[code](https://tarun005.github.io/GeoNet)
  * [PosterLayout: A New Benchmark and Approach for Content-aware Visual-Textual Presentation Layout](http://arxiv.org/abs/2303.15937v1)<br>:star:[code](https://github.com/PKU-ICST-MIPL/PosterLayout-CVPR2023)
  * [Dense-Localizing Audio-Visual Events in Untrimmed Videos: A Large-Scale Benchmark and Baseline](http://arxiv.org/abs/2303.12930v1)
  * [ANetQA: A Large-scale Benchmark for Fine-grained Compositional Reasoning over Untrimmed Videos](https://arxiv.org/abs/2305.02519)<br>:house:[project](https://milvlg.github.io/anetqa/)
  * Image Similarity
  * [GeneCIS: A Benchmark for General Conditional Image Similarity](https://openaccess.thecvf.com/content/CVPR2023/papers/Vaze_GeneCIS_A_Benchmark_for_General_Conditional_Image_Similarity_CVPR_2023_paper.pdf)<br>:house:[project](sgvaze.github.io/genecis)
  * [ANetQA: A Large-scale Benchmark for Fine-grained Compositional Reasoning over Untrimmed Videos](http://arxiv.org/abs/2305.02519v1)<br>:star:[code](https://milvlg.github.io/anetqa/)
  * [Ultra-High Resolution Segmentation with Ultra-Rich Context: A Novel Benchmark](http://arxiv.org/abs/2305.10899v1)<br>:star:[code](https://github.com/jankyee/URUR)
  * [NewsNet: A Novel Benchmark for Hierarchical Temporal Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_NewsNet_A_Novel_Dataset_for_Hierarchical_Temporal_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/NewsNet-Benchmark/NewsNet)
  * [Ultra-High Resolution Segmentation With Ultra-Rich Context: A Novel Benchmark](https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_Ultra-High_Resolution_Segmentation_With_Ultra-Rich_Context_A_Novel_Benchmark_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/jankyee/URUR)
  * [PosterLayout: A New Benchmark and Approach for Content-Aware Visual-Textual Presentation Layout](https://arxiv.org/abs/2303.15937)<br>:star:[code](https://github.com/PKU-ICST-MIPL/PosterLayout-CVPR2023)
  * [Meta Omnium: A Benchmark for General-Purpose Learning-To-Learn](https://arxiv.org/abs/2305.07625)<br>:star:[code](https://edi-meta-learning.github.io/meta-omnium)
  * [RefTeacher: A Strong Baseline for Semi-Supervised Referring Expression Comprehension](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_RefTeacher_A_Strong_Baseline_for_Semi-Supervised_Referring_Expression_Comprehension_CVPR_2023_paper.pdf)<br>:house:[project](https://refteacher.github.io/)

<a name="53"/>

## 53.Sign Language (手语)
* [Ham2Pose: Animating Sign Language Notation Into Pose Sequences](https://openaccess.thecvf.com/content/CVPR2023/papers/Arkushin_Ham2Pose_Animating_Sign_Language_Notation_Into_Pose_Sequences_CVPR_2023_paper.pdf)<br>:house:[project](https://rotem-shalev.github.io/ham-to-pose)
* 手语翻译
  * [Gloss Attention for Gloss-Free Sign Language Translation](https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_Gloss_Attention_for_Gloss-Free_Sign_Language_Translation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/YinAoXiong/GASLT)
* 手语识别
  * [Continuous Sign Language Recognition with Correlation Network](https://arxiv.org/abs/2303.03202)<br>:star:[code](https://github.com/hulianyuyy/CorrNet)
  * [Reconstructing Signing Avatars From Video Using Linguistic Priors](https://arxiv.org/abs/2304.10482)<br>:house:[project](http://sgnify.is.tue.mpg.de/)
  * [Distilling Cross-Temporal Contexts for Continuous Sign Language Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Distilling_Cross-Temporal_Contexts_for_Continuous_Sign_Language_Recognition_CVPR_2023_paper.pdf)
  * [CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition With Variational Alignment](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_CVT-SLR_Contrastive_Visual-Textual_Transformation_for_Sign_Language_Recognition_With_Variational_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/binbinjiang/CVT-SLR)
  * [Natural Language-Assisted Sign Language Recognition](https://arxiv.org/abs/2303.12080)<br>:star:[code](https://github.com/FangyunWei/SLRT)
  * [Continuous Sign Language Recognition With Correlation Network](https://arxiv.org/abs/2303.03202)<br>:star:[code](https://github.com/hulianyuyy/CorrNet)
* 手语检索
  * [CiCo: Domain-Aware Sign Language Retrieval via Cross-Lingual Contrastive Learning](http://arxiv.org/abs/2303.12793v1)<br>:star:[code](https://github.com/FangyunWei/SLRT)

<a name="52"/>

## 52.Human Motion(人体运动)
* [Semi-Weakly Supervised Object Kinematic Motion Prediction](https://arxiv.org/abs/2303.17774)
* [The Wisdom of Crowds: Temporal Progressive Attention for Early Action Prediction](https://arxiv.org/abs/2204.13340)
* [MotionDiffuser: Controllable Multi-Agent Motion Prediction using Diffusion](https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_MotionDiffuser_Controllable_Multi-Agent_Motion_Prediction_Using_Diffusion_CVPR_2023_paper.pdf)
* 人体运动预测
  * [EqMotion: Equivariant Multi-agent Motion Prediction with Invariant Interaction Reasoning](https://arxiv.org/abs/2303.10876)<br>:star:[code](https://github.com/MediaBrain-SJTU/EqMotion)
  * [DeFeeNet: Consecutive 3D Human Motion Prediction with Deviation Feedback](http://arxiv.org/abs/2304.04496v1)
  * [Decompose More and Aggregate Better: Two Closer Looks at Frequency Representation Learning for Human Motion Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Decompose_More_and_Aggregate_Better_Two_Closer_Looks_at_Frequency_CVPR_2023_paper.pdf)
* 人体运动合成
  * [Generating Human Motion From Textual Descriptions With Discrete Representations](https://arxiv.org/abs/2301.06052)<br>:house:[project](https://mael-zys.github.io/T2M-GPT/)
  * [UDE: A Unified Driving Engine for Human Motion Generation](https://arxiv.org/abs/2211.16016)<br>:star:[code](https://github.com/zixiangzhou916/UDE/)
  * [Mofusion: A Framework for Denoising-Diffusion-Based Motion Synthesis](https://arxiv.org/abs/2212.04495)<br>:house:[project](https://vcai.mpi-inf.mpg.de/projects/MoFusion)
* 3D HM
  * [Generating Holistic 3D Human Motion from Speech](https://arxiv.org/abs/2212.04420)<br>:house:[project](https://talkshow.is.tue.mpg.de/)

<a name="51"/>

## 51.Computed Imaging(计算成像，如光学、几何、光场成像等)
* [Physics-Guided ISO-Dependent Sensor Noise Modeling for Extreme Low-Light Photography](https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Physics-Guided_ISO-Dependent_Sensor_Noise_Modeling_for_Extreme_Low-Light_Photography_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/happycaoyue/LLD)
* [TRACE: 5D Temporal Regression of Avatars With Dynamic Cameras in 3D Environments](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_TRACE_5D_Temporal_Regression_of_Avatars_With_Dynamic_Cameras_in_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Arthur151/DynaCam)
* [High-Fidelity Event-Radiance Recovery via Transient Event Frequency](https://openaccess.thecvf.com/content/CVPR2023/papers/Han_High-Fidelity_Event-Radiance_Recovery_via_Transient_Event_Frequency_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/hjynwa/TEF)
* [Real-Time Neural Light Field on Mobile Devices](https://arxiv.org/abs/2212.08057)<br>:house:[project](https://snap-research.github.io/MobileR2L/)
* [Accidental Light Probes](https://arxiv.org/abs/2301.05211)<br>:house:[project](https://kovenyu.com/ALP/)
* [DyLiN: Making Light Field Networks Dynamic](http://arxiv.org/abs/2303.14243v1)<br>:star:[code](https://dylin2023.github.io)
* [Learning Rotation-Equivariant Features for Visual Correspondence](http://arxiv.org/abs/2303.15472v1)<br>:house:[project](http://cvlab.postech.ac.kr/research/RELF)
* [Role of Transients in Two-Bounce Non-Line-of-Sight Imaging](https://arxiv.org/abs/2304.01308)
* [Revisiting Rolling Shutter Bundle Adjustment: Toward Accurate and Fast Solution](https://arxiv.org/abs/2209.08503)
* 相机姿势估计
  * [SliceMatch: Geometry-Guided Aggregation for Cross-View Pose Estimation](https://arxiv.org/abs/2211.14651)
  * [SparsePose: Sparse-View Camera Pose Regression and Refinement](https://arxiv.org/abs/2211.16991)
  * [Pose Synchronization Under Multiple Pair-Wise Relative Poses](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Pose_Synchronization_Under_Multiple_Pair-Wise_Relative_Poses_CVPR_2023_paper.pdf)
* 快门校正
  * [EvShutter: Transforming Events for Unconstrained Rolling Shutter Correction](https://openaccess.thecvf.com/content/CVPR2023/papers/Erbach_EvShutter_Transforming_Events_for_Unconstrained_Rolling_Shutter_Correction_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/juliuserbach/EvShutter)
* 相机校准
  * [Perspective Fields for Single Image Camera Calibration](https://arxiv.org/abs/2212.03239)<br>:house:[project](https://jinlinyi.github.io/PerspectiveFields/)
* 几何估计
  * [Adaptive Annealing for Robust Geometric Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Sidhartha_Adaptive_Annealing_for_Robust_Geometric_Estimation_CVPR_2023_paper.pdf)
* 相机定位
  *[NeuMap: Neural Coordinate Mapping by Auto-Transdecoder for Camera Localization](https://arxiv.org/abs/2211.11177)<br>:star:[code](https://github.com/Tangshitao/NeuMap)

<a name="50"/>

## 50.Anomaly Detection(异常检测)
* [Prototypical Residual Networks for Anomaly Detection and Localization](https://arxiv.org/abs/2212.02031)
* [OpenMix: Exploring Outlier Samples for Misclassification Detection](https://arxiv.org/abs/2303.17093)<br>:star:[code](https://github.com/Impression2805/OpenMix)
* [Explicit Boundary Guided Semi-Push-Pull Contrastive Learning for Supervised Anomaly Detection](https://arxiv.org/abs/2207.01463)<br>:star:[code](https://github.com/xcyao00/BGAD)
* [Diversity-Measurable Anomaly Detection](https://arxiv.org/abs/2303.05047)
* [SimpleNet: A Simple Network for Image Anomaly Detection and Localization](http://arxiv.org/abs/2303.15140v1)<br>:star:[code](https://github.com/DonaldRR/SimpleNet)
* [DeSTSeg: Segmentation Guided Denoising Student-Teacher for Anomaly Detection](https://arxiv.org/abs/2211.11317)
* [WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation](http://arxiv.org/abs/2303.14814v1)
* OOD
  * [Uncertainty-Aware Optimal Transport for Semantically Coherent Out-of-Distribution Detection](https://arxiv.org/abs/2303.10449)<br>:star:[code](https://github.com/LuFan31/ET-OOD)
  * [Block Selection Method for Using Feature Norm in Out-of-Distribution Detection](https://arxiv.org/abs/2212.02295)<br>:star:[code](https://github.com/gist-ailab/block-selection-for-OOD-detection)
  * [Distribution Shift Inversion for Out-of-Distribution Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Distribution_Shift_Inversion_for_Out-of-Distribution_Prediction_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/yu-rp/Distribution-Shift-Iverson)
  * [Are Data-Driven Explanations Robust Against Out-of-Distribution Data?](https://arxiv.org/abs/2303.16390)
  * [LINe: Out-of-Distribution Detection by Leveraging Important Neurons](http://arxiv.org/abs/2303.13995v1)
  * [Rethinking Out-of-Distribution (OOD) Detection: Masked Image Modeling Is All You Need](https://arxiv.org/abs/2302.02615)<br>:star:[code](https://github.com/JulietLJY/MOOD)
  * [Balanced Energy Regularization Loss for Out-of-Distribution Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_Balanced_Energy_Regularization_Loss_for_Out-of-Distribution_Detection_CVPR_2023_paper.pdf)
  * [Decoupling MaxLogit for Out-of-Distribution Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Decoupling_MaxLogit_for_Out-of-Distribution_Detection_CVPR_2023_paper.pdf)
  * [Detection of Out-of-Distribution Samples Using Binary Neuron Activation Patterns](https://arxiv.org/abs/2212.14268)
  * [GEN: Pushing the Limits of Softmax-Based Out-of-Distribution Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_GEN_Pushing_the_Limits_of_Softmax-Based_Out-of-Distribution_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/XixiLiu95/GEN)

<a name="49"/>

## 49.Image Geo-localization(图像地理位置识别)
* [Where We Are and What We're Looking At: Query Based Worldwide Image Geo-localization Using Hierarchies and Scenes](https://arxiv.org/abs/2303.04249)

<a name="48"/>

## 48.NLP(自然语言处理)
* [Images Speak in Images: A Generalist Painter for In-Context Visual Learning](https://arxiv.org/abs/2212.02499)<br>:star:[code](https://github.com/baaivision/Painter)
* 反讽检测(检测文本（或图像，如漫画等其他模态）中是否存在讽刺)
  * [DIP: Dual Incongruity Perceiving Network for Sarcasm Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Wen_DIP_Dual_Incongruity_Perceiving_Network_for_Sarcasm_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/downdric/MSD)
* NLQ
  * [NaQ: Leveraging Narrations as Queries to Supervise Episodic Memory](https://arxiv.org/abs/2301.00746)<br>:star:[code](http://vision.cs.utexas.edu/projects/naq)
* Visual Grounding(视觉指代)
  * [Language Adaptive Weight Generation for Multi-Task Visual Grounding](https://openaccess.thecvf.com/content/CVPR2023/papers/Su_Language_Adaptive_Weight_Generation_for_Multi-Task_Visual_Grounding_CVPR_2023_paper.pdf)
* Referring Expression Comprehension(指代表达理解)
  * [RefCLIP: A Universal Teacher for Weakly Supervised Referring Expression Comprehension](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_RefCLIP_A_Universal_Teacher_for_Weakly_Supervised_Referring_Expression_Comprehension_CVPR_2023_paper.pdf)<br>:house:[project](https://refclip.github.io/)

<a name="47"/>

## 47.Few/Zero-Shot Learning/Domain Generalization/Adaptation(小/零样本/域泛化/域适应)
* DG
  * [Towards Domain Generalization for Multi-view 3D Object Detection in Bird-Eye-View](https://arxiv.org/abs/2303.01686)
  * [Promoting Semantic Connectivity: Dual Nearest Neighbors Contrastive Learning for Unsupervised Domain Generalization](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Promoting_Semantic_Connectivity_Dual_Nearest_Neighbors_Contrastive_Learning_for_Unsupervised_CVPR_2023_paper.pdf)
  * [Federated Domain Generalization With Generalization Adjustment](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Federated_Domain_Generalization_With_Generalization_Adjustment_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/MediaBrain-SJTU/FedDG-GA)
  * [Decompose, Adjust, Compose: Effective Normalization by Playing With Frequency for Domain Generalization](https://arxiv.org/abs/2303.02328)
  * [NICO++: Towards Better Benchmarking for Domain Generalization](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_NICO_Towards_Better_Benchmarking_for_Domain_Generalization_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/xxgege/NICO-plus)
  * [Improved Test-Time Adaptation for Domain Generalization](http://arxiv.org/abs/2304.04494v1)<br>:star:[code](https://github.com/liangchen527/ITTA)
  * [Modality-Agnostic Debiasing for Single Domain Generalization](https://arxiv.org/abs/2303.07123)
  * [Neuron Structure Modeling for Generalizable Remote Physiological Measurement](https://arxiv.org/abs/2303.05955)<br>:star:[code](https://github.com/LuPaoPao/NEST)
  * [Sharpness-Aware Gradient Matching for Domain Generalization](https://arxiv.org/abs/2303.10353)<br>:star:[code](https://github.com/Wang-pengfei/SAGM)
  * [Improving Generalization with Domain Convex Game](http://arxiv.org/abs/2303.13297v1)
  * [Generalist: Decoupling Natural and Robust Generalization](http://arxiv.org/abs/2303.13813v1)<br>:star:[code](https://github.com/PKU-ML/Generalist)
  * [ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency Transform for Domain Generalization](https://arxiv.org/abs/2303.11674)<br>:star:[code](https://github.com/lingeringlight/ALOFT/)
  * [Deep Frequency Filtering for Domain Generalization](https://arxiv.org/abs/2203.12198)
  * [Progressive Random Convolutions for Single Domain Generalization](http://arxiv.org/abs/2304.00424v1)
  * [Meta-causal Learning for Single Domain Generalization](http://arxiv.org/abs/2304.03709v1)
* DA
  * [Guiding Pseudo-labels with Uncertainty Estimation for Test-Time Adaptation](https://arxiv.org/abs/2303.03770)<br>:star:[code](https://github.com/MattiaLitrico/Guiding-Pseudo-labels-with-Uncertainty-Estimation-for-Test-Time-Adaptation)
  * [Source-Free Video Domain Adaptation With Spatial-Temporal-Historical Consistency Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Source-Free_Video_Domain_Adaptation_With_Spatial-Temporal-Historical_Consistency_Learning_CVPR_2023_paper.pdf)
  * [DARE-GRAM: Unsupervised Domain Adaptation Regression by Aligning Inverse Gram Matrices](https://arxiv.org/abs/2303.13325)<br>:star:[code](https://github.com/ismailnejjar/DARE-GRAM)
  * [Dual-Bridging With Adversarial Noise Generation for Domain Adaptive rPPG Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Dual-Bridging_With_Adversarial_Noise_Generation_for_Domain_Adaptive_rPPG_Estimation_CVPR_2023_paper.pdf )
  * [MIC: Masked Image Consistency for Context-Enhanced Domain Adaptation](https://arxiv.org/abs/2212.01322)<br>:star:[code](https://github.com/lhoyer/MIC)
  * [DATE: Domain Adaptive Product Seeker for E-commerce](http://arxiv.org/abs/2304.03669v1)<br>:star:[code](https://github.com/Taobao-live/Product-Seeking)
  * [Adjustment and Alignment for Unbiased Open Set Domain Adaptation](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Adjustment_and_Alignment_for_Unbiased_Open_Set_Domain_Adaptation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/CityU-AIM-Group/Anna)
  * [Patch-Mix Transformer for Unsupervised Domain Adaptation: A Game Perspective](http://arxiv.org/abs/2303.13434v1)
  * [MHPL: Minimum Happy Points Learning for Active Source Free Domain Adaptation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MHPL_Minimum_Happy_Points_Learning_for_Active_Source_Free_Domain_CVPR_2023_paper.pdf)
  * [COT: Unsupervised Domain Adaptation with Clustering and Optimal Transport](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_COT_Unsupervised_Domain_Adaptation_With_Clustering_and_Optimal_Transport_CVPR_2023_paper.pdf)
  * [Upcycling Models under Domain and Category Shift](https://arxiv.org/abs/2303.07110)<br>:star:[code](https://github.com/ispc-lab/GLC)
  * [C-SFDA: A Curriculum Learning Aided Self-Training Framework for Efficient Source Free Domain Adaptation](http://arxiv.org/abs/2303.17132v1)
  * [TeSLA: Test-Time Self-Learning With Automatic Adversarial Augmentation](https://arxiv.org/abs/2303.09870)<br>:star:[code](https://github.com/devavratTomar/TeSLA)
  * [OSAN: A One-Stage Alignment Network to Unify Multimodal Alignment and Unsupervised Domain Adaptation](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_OSAN_A_One-Stage_Alignment_Network_To_Unify_Multimodal_Alignment_and_CVPR_2023_paper.pdf)
  * [MOT: Masked Optimal Transport for Partial Domain Adaptation](https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_MOT_Masked_Optimal_Transport_for_Partial_Domain_Adaptation_CVPR_2023_paper.pdf)
  * [Feature Alignment and Uniformity for Test Time Adaptation](https://arxiv.org/abs/2303.10902)
* ZSL
  * [Bi-Directional Distribution Alignment for Transductive Zero-Shot Learning](https://arxiv.org/abs/2303.08698)<br>:star:[code](https://github.com/Zhicaiwww/Bi-VAEGAN)
  * [Progressive Semantic-Visual Mutual Adaption for Generalized Zero-Shot Learning](http://arxiv.org/abs/2303.15322v1)<br>:star:[code](https://github.com/ManLiuCoder/PSVMA)
  * [Learning Attention as Disentangler for Compositional Zero-shot Learning](http://arxiv.org/abs/2303.15111v1)<br>:star:[code](https://haoosz.github.io/ade-czsl/)
  * [Zero-shot Model Diagnosis](http://arxiv.org/abs/2303.15441v1)
  * [Learning Conditional Attributes for Compositional Zero-Shot Learning](https://arxiv.org/abs/2305.17940)<br>:star:[code](https://github.com/wqshmzh/CANet-CZSL)
  * [(ML)$^2$P-Encoder: On Exploration of Channel-Class Correlation for Multi-Label Zero-Shot Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_ML2P-Encoder_On_Exploration_of_Channel-Class_Correlation_for_Multi-Label_Zero-Shot_Learning_CVPR_2023_paper.pdf)<br>:star:[code](github.com/simonzmliu/cvpr23_mlzsl)
* FSL
  * [Transductive Few-shot Learning with Prototype-based Label Propagation by Iterative Graph Refinement](http://arxiv.org/abs/2304.11598v1)
  * [Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners](https://arxiv.org/abs/2303.02151)<br>:star:[code](https://github.com/ZrrSkywalker/CaFo)
  * [Revisiting Prototypical Network for Cross Domain Few-Shot Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Revisiting_Prototypical_Network_for_Cross_Domain_Few-Shot_Learning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/NWPUZhoufei/LDP-Net)
  * [Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning With Multimodal Models](https://arxiv.org/abs/2301.06267)<br>:house:[project](https://linzhiqiu.github.io/papers/cross_modal/)
  * [Open-Set Likelihood Maximization for Few-Shot Learning](https://arxiv.org/abs/2301.08390)
  * [StyleAdv: Meta Style Adversarial Training for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2302.09309)<br>:star:[code](https://github.com/lovelyqian/StyleAdv-CDFSL)

<a name="46"/>

## 46.Scene Graph Generation(场景图生成)
* [IS-GGT: Iterative Scene Graph Generation With Generative Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/Kundu_IS-GGT_Iterative_Scene_Graph_Generation_With_Generative_Transformers_CVPR_2023_paper.pdf)
* [Prototype-based Embedding Network for Scene Graph Generation](https://arxiv.org/abs/2303.07096)<br>:star:[code](https://github.com/VL-Group/PENET)
* [Devil's on the Edges: Selective Quad Attention for Scene Graph Generation](http://arxiv.org/abs/2304.03495v1)<br>:house:[project](https://cvlab.postech.ac.kr/research/SQUAT/)
* [Learning To Generate Language-Supervised and Open-Vocabulary Scene Graph Using Pre-Trained Visual-Semantic Space](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_To_Generate_Language-Supervised_and_Open-Vocabulary_Scene_Graph_Using_Pre-Trained_CVPR_2023_paper.pdf)
* [Panoptic Video Scene Graph Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Panoptic_Video_Scene_Graph_Generation_CVPR_2023_paper.pdf)
* [Fast Contextual Scene Graph Generation With Unbiased Context Augmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Fast_Contextual_Scene_Graph_Generation_With_Unbiased_Context_Augmentation_CVPR_2023_paper.pdf)

<a name="45"/>

## 45.Dense Prediction(密集预测)
* [Ensemble-Based Blackbox Attacks on Dense Prediction](https://arxiv.org/abs/2303.14304)<br>:star:[code](https://github.com/CSIPlab/EBAD)
* [DejaVu: Conditional Regenerative Learning to Enhance Dense Prediction](https://arxiv.org/abs/2303.01573)
* [Ensemble-based Blackbox Attacks on Dense Prediction](http://arxiv.org/abs/2303.14304v1)<br>:star:[code](https://github.com/CSIPlab/EBAD)
* [Probabilistic Prompt Learning for Dense Prediction](http://arxiv.org/abs/2304.00779v1)
* [1% VS 100%: Parameter-Efficient Low Rank Adapter for Dense Predictions](https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_1_VS_100_Parameter-Efficient_Low_Rank_Adapter_for_Dense_Predictions_CVPR_2023_paper.pdf)
* [DPF: Learning Dense Prediction Fields With Weak Supervision](https://arxiv.org/abs/2303.16890)<br>:star:[code](https://github.com/cxx226/DPF)
* 密集检测
  * [One-to-Few Label Assignment for End-to-End Dense Detection](https://arxiv.org/abs/2303.11567)<br>:star:[code](https://github.com/strongwolf/o2f)
* 密集目标定位
  * [Learning Multi-Modal Class-Specific Tokens for Weakly Supervised Dense Object Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Learning_Multi-Modal_Class-Specific_Tokens_for_Weakly_Supervised_Dense_Object_Localization_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/xulianuwa/MMCST)

<a name="44"/>

## 44.Federated Learning(联邦学习)
* [Federated Learning With Data-Agnostic Distribution Fusion](https://openaccess.thecvf.com/content/CVPR2023/papers/Duan_Federated_Learning_With_Data-Agnostic_Distribution_Fusion_CVPR_2023_paper.pdf)
* [How To Prevent the Poor Performance Clients for Personalized Federated Learning?](https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_How_To_Prevent_the_Poor_Performance_Clients_for_Personalized_Federated_CVPR_2023_paper.pdf)
* [GradMA: A Gradient-Memory-Based Accelerated Federated Learning With Alleviated Catastrophic Forgetting](https://arxiv.org/abs/2302.14307)
* [Bias-Eliminating Augmentation Learning for Debiased Federated Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Bias-Eliminating_Augmentation_Learning_for_Debiased_Federated_Learning_CVPR_2023_paper.pdf)
* [Make Landscape Flatter in Differentially Private Federated Learning](https://arxiv.org/abs/2303.11242)
* [The Resource Problem of Using Linear Layer Leakage Attack in Federated Learning](http://arxiv.org/abs/2303.14868v1)
* [Rethinking Federated Learning With Domain Shift: A Prototype View](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/WenkeHuang/RethinkFL)
* [On the Effectiveness of Partial Variance Reduction in Federated Learning With Heterogeneous Data](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_On_the_Effectiveness_of_Partial_Variance_Reduction_in_Federated_Learning_CVPR_2023_paper.pdf)
* [Elastic Aggregation for Federated Optimization](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Elastic_Aggregation_for_Federated_Optimization_CVPR_2023_paper.pdf)
* [FedDM: Iterative Distribution Matching for Communication-Efficient Federated Learning](https://arxiv.org/abs/2207.09653)
* [Adaptive Channel Sparsity for Federated Learning Under System Heterogeneity](https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_Adaptive_Channel_Sparsity_for_Federated_Learning_Under_System_Heterogeneity_CVPR_2023_paper.pdf)
* [ScaleFL: Resource-Adaptive Federated Learning With Heterogeneous Clients](https://openaccess.thecvf.com/content/CVPR2023/papers/Ilhan_ScaleFL_Resource-Adaptive_Federated_Learning_With_Heterogeneous_Clients_CVPR_2023_paper.pdf)
* [Reliable and Interpretable Personalized Federated Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Reliable_and_Interpretable_Personalized_Federated_Learning_CVPR_2023_paper.pdf)

<a name="43"/>

## 43.Multi-Task Learning(多任务学习)
* [Dynamic Neural Network for Multi-Task Learning Searching Across Diverse Network Topologies](https://arxiv.org/abs/2303.06856)
* [AdaMTL: Adaptive Input-dependent Inference for Efficient Multi-Task Learning](http://arxiv.org/abs/2304.08594v1)<br>:star:[code](https://github.com/scale-lab/AdaMTL.git)
* [Mod-Squad: Designing Mixtures of Experts As Modular Multi-Task Learners]<br>:house:[project](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Mod-Squad_Designing_Mixtures_of_Experts_As_Modular_Multi-Task_Learners_CVPR_2023_paper.pdf)(https://vis-www.cs.umass.edu/mod-squad/)
* [Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing With Non-Learnable Primitives](https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_Mitigating_Task_Interference_in_Multi-Task_Learning_via_Explicit_Task_Routing_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/zhichao-lu/etr-nlp-mtl)
* [Hierarchical Prompt Learning for Multi-Task Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Hierarchical_Prompt_Learning_for_Multi-Task_Learning_CVPR_2023_paper.pdf)

<a name="42"/>

## 42.Metric Learning(度量学习)
* [Advancing Deep Metric Learning Through Multiple Batch Norms And Multi-Targeted Adversarial Examples](https://arxiv.org/abs/2211.16253)
* [Deep Factorized Metric Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Deep_Factorized_Metric_Learning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/wangck20/DFML)
* [Deep Semi-Supervised Metric Learning With Mixed Label Propagation](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhuang_Deep_Semi-Supervised_Metric_Learning_With_Mixed_Label_Propagation_CVPR_2023_paper.pdf)
* [Cross-Image-Attention for Conditional Embeddings in Deep Metric Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Kotovenko_Cross-Image-Attention_for_Conditional_Embeddings_in_Deep_Metric_Learning_CVPR_2023_paper.pdf)

<a name="41"/>

## 41.Incremental Learning(增量学习)
* [AttriCLIP: A Non-Incremental Learner for Incremental Knowledge Learning](https://arxiv.org/abs/2305.11488)<br>:star:[code](https://github.com/bhrqw/AttriCLIP)
* [GKEAL: Gaussian Kernel Embedded Analytic Learning for Few-Shot Class Incremental Task](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhuang_GKEAL_Gaussian_Kernel_Embedded_Analytic_Learning_for_Few-Shot_Class_Incremental_CVPR_2023_paper.pdf)
* 类增量学习
  * [Dense Network Expansion for Class Incremental Learning](http://arxiv.org/abs/2303.12696v1)
  * [Class-Incremental Exemplar Compression for Class-Incremental Learning](http://arxiv.org/abs/2303.14042v1)<br>:star:[code](https://github.com/xfflzl/CIM-CIL)
  * [Rebalancing Batch Normalization for Exemplar-based Class-Incremental Learning](https://arxiv.org/abs/2201.12559)
  * [Learning with Fantasy: Semantic-Aware Virtual Contrastive Constraint for Few-Shot Class-Incremental Learning](http://arxiv.org/abs/2304.00426v1)<br>:star:[code](https://github.com/zysong0113/SAVC)
  * [On the Stability-Plasticity Dilemma of Class-Incremental Learning](http://arxiv.org/abs/2304.01663v1)
  * [Few-Shot Class-Incremental Learning via Class-Aware Bilateral Distillation](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Few-Shot_Class-Incremental_Learning_via_Class-Aware_Bilateral_Distillation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/LinglanZhao/BiDistFSCIL)
  * [Multi-Centroid Task Descriptor for Dynamic Class Incremental Inference](https://openaccess.thecvf.com/content/CVPR2023/papers/Cai_Multi-Centroid_Task_Descriptor_for_Dynamic_Class_Incremental_Inference_CVPR_2023_paper.pdf)
  * [DKT: Diverse Knowledge Transfer Transformer for Class Incremental Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_DKT_Diverse_Knowledge_Transfer_Transformer_for_Class_Incremental_Learning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/MIV-XJTU/DKT)
  
<a name="40"/>

## 40.Adversarial Learning(对抗学习)
* [Seasoning Model Soups for Robustness to Adversarial and Natural Distribution Shifts](https://arxiv.org/abs/2302.10164)
* [Dynamic Generative Targeted Attacks With Pattern Injection](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Dynamic_Generative_Targeted_Attacks_With_Pattern_Injection_CVPR_2023_paper.pdf)
* [FIANCEE: Faster Inference of Adversarial Networks via Conditional Early Exits](https://arxiv.org/abs/2304.10306)
* [Enhancing the Self-Universality for Transferable Targeted Attacks](https://arxiv.org/abs/2209.03716)<br>:star:[code](https://github.com/zhipeng-wei/Self-Universality)
* [Exploring the Relationship Between Architectural Design and Adversarially Robust Generalization](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Exploring_the_Relationship_Between_Architectural_Design_and_Adversarially_Robust_Generalization_CVPR_2023_paper.pdf)<br>:house:[project](http://robust.art/)
* [Revisiting Residual Networks for Adversarial Robustness](https://arxiv.org/abs/2212.11005)<br>:star:[code](https://github.com/zhichao-lu/robust-residual-network)
* [Feature Separation and Recalibration for Adversarial Robustness](http://arxiv.org/abs/2303.13846v1)<br>:star:[code](https://github.com/wkim97/FSR)
* [CFA: Class-wise Calibrated Fair Adversarial Training](http://arxiv.org/abs/2303.14460v1)<br>:star:[code](https://github.com/PKU-ML/CFA)
* [Towards Compositional Adversarial Robustness: Generalizing Adversarial Training to Composite Semantic Perturbations](https://arxiv.org/abs/2202.04235)<br>:house:[project](https://hsiung.cc/CARBEN/)
* [Efficient Loss Function by Minimizing the Detrimental Effect of Floating-Point Errors on Gradient-Based Attacks](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Efficient_Loss_Function_by_Minimizing_the_Detrimental_Effect_of_Floating-Point_CVPR_2023_paper.pdf)
* 黑盒
  * [BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning](http://arxiv.org/abs/2303.14773v1)<br>:star:[code](https://github.com/changdaeoh/BlackVIP)
  * [Reinforcement Learning-Based Black-Box Model Inversion Attacks](http://arxiv.org/abs/2304.04625v1)
  * [Minimizing Maximum Model Discrepancy for Transferable Black-Box Targeted Attacks](https://arxiv.org/abs/2212.09035)
* 对抗样本
  * [Demystifying Causal Features on Adversarial Examples and Causal Inoculation for Robust Network by Adversarial Instrumental Variable Regression](https://arxiv.org/abs/2303.01052)
  * [Introducing Competition To Boost the Transferability of Targeted Adversarial Examples Through Clean Feature Mixup](https://openaccess.thecvf.com/content/CVPR2023/papers/Byun_Introducing_Competition_To_Boost_the_Transferability_of_Targeted_Adversarial_Examples_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/dreamflake/CFM)
  * [Towards Transferable Targeted Adversarial Examples](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Towards_Transferable_Targeted_Adversarial_Examples_CVPR_2023_paper.pdf)
  * [Improving the Transferability of Adversarial Samples by Path-Augmented Method](http://arxiv.org/abs/2303.15735v1)
  * [Increasing the Latency of LiDAR-Based Detection Using Adversarial Examples](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_SlowLiDAR_Increasing_the_Latency_of_LiDAR-Based_Detection_Using_Adversarial_Examples_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/WUSTL-CSPL/SlowLiDAR)
* 后门攻击
  * [Single Image Backdoor Inversion via Robust Smoothed Classifiers](https://arxiv.org/pdf/2303.00215.pdf)<br>:star:[code](https://arxiv.org/pdf/2303.00215.pdf)
  * [MEDIC: Remove Model Backdoors via Importance Driven Cloning](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_MEDIC_Remove_Model_Backdoors_via_Importance_Driven_Cloning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/qiulingxu/MEDIC)
  * [Backdoor Defense via Adaptively Splitting Poisoned Dataset](http://arxiv.org/abs/2303.12993v1)<br>:star:[code](https://github.com/KuofengGao/ASD)
  * [Detecting Backdoors in Pre-trained Encoders](http://arxiv.org/abs/2303.15180v1)<br>:star:[code](https://github.com/GiantSeaweed/DECREE)
  * [Color Backdoor: A Robust Poisoning Attack in Color Space](https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Color_Backdoor_A_Robust_Poisoning_Attack_in_Color_Space_CVPR_2023_paper.pdf)
  * [Detecting Backdoors in Pre-Trained Encoders](https://arxiv.org/abs/2303.15180)<br>:star:[code](https://github.com/GiantSeaweed/DECREE)
* 对抗攻击
  * [Adversarial Attack with Raindrops](https://arxiv.org/pdf/2302.14267.pdf)
  * [The Best Defense Is a Good Offense: Adversarial Augmentation Against Adversarial Attacks](https://openaccess.thecvf.com/content/CVPR2023/papers/Frosio_The_Best_Defense_Is_a_Good_Offense_Adversarial_Augmentation_Against_CVPR_2023_paper.pdf)
  * [Transferable Adversarial Attacks on Vision Transformers with Token Gradient Regularization](http://arxiv.org/abs/2303.15754v1)
  * [Robust Single Image Reflection Removal Against Adversarial Attacks](https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Robust_Single_Image_Reflection_Removal_Against_Adversarial_Attacks_CVPR_2023_paper.pdf)
  * [Transferable Adversarial Attacks on Vision Transformers With Token Gradient Regularization](https://arxiv.org/abs/2303.15754)<br>:star:[code](https://github.com/jpzhang1810/TGR)
  * [StyLess: Boosting the Transferability of Adversarial Examples](http://arxiv.org/abs/2304.11579v1)
  * [Re-thinking Model Inversion Attacks Against Deep Neural Networks](http://arxiv.org/abs/2304.01669v1)<br>:star:[code](https://ngoc-nguyen-0.github.io/re-thinking_model_inversion_attacks/)
  * [Defending Against Patch-based Backdoor Attacks on Self-Supervised Learning](http://arxiv.org/abs/2304.01482v1)<br>:star:[code](https://github.com/UCDvision/PatchSearch)
  * [Jedi: Entropy-based Localization and Removal of Adversarial Patches](http://arxiv.org/abs/2304.10029v1)
* 后门防御
  * [Backdoor Defense via Deconfounded Representation Learning](https://arxiv.org/abs/2303.06818)<br>:star:[code](https://github.com/zaixizhang/CBD)
* 对抗训练
  * [The Enemy of My Enemy Is My Friend: Exploring Inverse Adversaries for Improving Adversarial Training](https://arxiv.org/abs/2211.00525)
  * [Randomized Adversarial Training via Taylor Expansion](https://arxiv.org/abs/2303.10653)<br>:star:[code](https://github.com/Alexkael/Randomized-Adversarial-Training)
  * [AGAIN: Adversarial Training With Attribution Span Enlargement and Hybrid Feature Fusion](https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_AGAIN_Adversarial_Training_With_Attribution_Span_Enlargement_and_Hybrid_Feature_CVPR_2023_paper.pdf)

<a name="39"/>

## 39.Continual Learning(持续学习)
* [Batch Model Consolidation: A Multi-Task Model Consolidation Framework](https://openaccess.thecvf.com/content/CVPR2023/papers/Fostiropoulos_Batch_Model_Consolidation_A_Multi-Task_Model_Consolidation_Framework_CVPR_2023_paper.pdf)
* [CODA-Prompt: COntinual Decomposed Attention-Based Prompting for Rehearsal-Free Continual Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Smith_CODA-Prompt_COntinual_Decomposed_Attention-Based_Prompting_for_Rehearsal-Free_Continual_Learning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/GT-RIPL/CODA-Prompt)
* [Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning](https://arxiv.org/abs/2303.09483)<br>:star:[code](https://github.com/kim-sanghwan/ANCL)
* [Computationally Budgeted Continual Learning: What Does Matter?](https://arxiv.org/abs/2303.11165)<br>:star:[code](https://github.com/drimpossible/BudgetCL)
* [Achieving a Better Stability-Plasticity Trade-Off via Auxiliary Networks in Continual Learning](https://arxiv.org/abs/2303.09483)
* [Preserving Linear Separability in Continual Learning by Backward Feature Projection](http://arxiv.org/abs/2303.14595v1)
* [Regularizing Second-Order Influences for Continual Learning](http://arxiv.org/abs/2304.10177v1)<br>:star:[code](https://github.com/feifeiobama/InfluenceCL)
* [Rethinking Gradient Projection Continual Learning: Stability / Plasticity Feature Space Decoupling](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Rethinking_Gradient_Projection_Continual_Learning_Stability__Plasticity_Feature_Space_CVPR_2023_paper.pdf)
* [MetaMix: Towards Corruption-Robust Continual Learning With Temporally Self-Adaptive Data Transformation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MetaMix_Towards_Corruption-Robust_Continual_Learning_With_Temporally_Self-Adaptive_Data_Transformation_CVPR_2023_paper.pdf)
* [Exploring Data Geometry for Continual Learning](http://arxiv.org/abs/2304.03931v1)
* [PCR: Proxy-based Contrastive Replay for Online Class-Incremental Continual Learning](http://arxiv.org/abs/2304.04408v1)
* [Bilateral Memory Consolidation for Continual Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Nie_Bilateral_Memory_Consolidation_for_Continual_Learning_CVPR_2023_paper.pdf)
* [Adaptive Plasticity Improvement for Continual Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Liang_Adaptive_Plasticity_Improvement_for_Continual_Learning_CVPR_2023_paper.pdf)
* [Real-Time Evaluation in Online Continual Learning: A New Hope](https://arxiv.org/abs/2302.01047)
* [PIVOT: Prompting for Video Continual Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Villa_PIVOT_Prompting_for_Video_Continual_Learning_CVPR_2023_paper.pdf)

<a name="38"/>

## 38.Meta-Learning(元学习)
* [Meta-Learning with a Geometry-Adaptive Preconditioner](http://arxiv.org/abs/2304.01552v1)<br>:star:[code](https://github.com/Suhyun777/CVPR23-GAP)元学习
* [Improving Generalization of Meta-Learning With Inverted Regularization at Inner-Level](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Improving_Generalization_of_Meta-Learning_With_Inverted_Regularization_at_Inner-Level_CVPR_2023_paper.pdf)
* [Ground-Truth Free Meta-Learning for Deep Compressive Sampling](https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Ground-Truth_Free_Meta-Learning_for_Deep_Compressive_Sampling_CVPR_2023_paper.pdf)

<a name="37"/>

## 37.Contrastive Learning(对比学习)
* [MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining](https://arxiv.org/abs/2208.12262)<br>:star:[code](https://github.com/LightDXY/MaskCLIP)
* [Twin Contrastive Learning with Noisy Labels](https://arxiv.org/abs/2303.06930)<br>:star:[code](https://github.com/Hzzone/TCL)
* [Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens](http://arxiv.org/abs/2303.14865v1)
* [Best of Both Worlds: Multimodal Contrastive Learning With Tabular and Imaging Data](https://arxiv.org/abs/2303.14080)
* [CLAMP: Prompt-Based Contrastive Learning for Connecting Language and Animal Pose](https://arxiv.org/abs/2206.11752)
* [ContraNeRF: Generalizable Neural Radiance Fields for Synthetic-to-Real Novel View Synthesis via Contrastive Learning](https://arxiv.org/abs/2303.11052)
* [Hyperbolic Contrastive Learning for Visual Representations beyond Objects](https://arxiv.org/abs/2212.00653)<br>:star:[code](https://github.com/shlokk/HCL/tree/main/HCL)
* 非对比学习
  * [Non-Contrastive Learning Meets Language-Image Pre-Training](https://arxiv.org/abs/2210.09304)

<a name="36"/>

## 36.Optical Flow(光流估计)
* [Rethinking Optical Flow from Geometric Matching Consistent Perspective](https://arxiv.org/abs/2303.08384)<br>:star:[code](https://github.com/DQiaole/MatchFlow)
* [DistractFlow: Improving Optical Flow Estimation via Realistic Distractions and Pseudo-Labeling](http://arxiv.org/abs/2303.14078v1)
* [AnyFlow: Arbitrary Scale Optical Flow with Implicit Neural Representation](http://arxiv.org/abs/2303.16493v1)
* [TransFlow: Transformer as Flow Learner](http://arxiv.org/abs/2304.11523v1)
* [Tangentially Elongated Gaussian Belief Propagation for Event-Based Incremental Optical Flow Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Nagata_Tangentially_Elongated_Gaussian_Belief_Propagation_for_Event-Based_Incremental_Optical_Flow_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/DensoITLab/tegbp/)
* [FlowFormer++: Masked Cost Volume Autoencoding for Pretraining Optical Flow Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Shi_FlowFormer_Masked_Cost_Volume_Autoencoding_for_Pretraining_Optical_Flow_Estimation_CVPR_2023_paper.pdf)

<a name="35"/>

## 35.OCR
* 文本识别
  * [Self-Supervised Implicit Glyph Attention for Text Recognition](https://arxiv.org/abs/2203.03382)
* 场景文本检测
  * [Turning a CLIP Model into a Scene Text Detector](https://arxiv.org/pdf/2302.14338.pdf)<br>:star:[code](https://github.com/wenwenyu/TCM)
* 表格结构识别
  * [Improving Table Structure Recognition with Visual-Alignment Sequential Coordinate Modeling](https://arxiv.org/abs/2303.06949)
* 字体生成
  * [CF-Font: Content Fusion for Few-shot Font Generation](http://arxiv.org/abs/2303.14017v1)<br>:star:[code](https://github.com/wangchi95/CF-Font)
  * [Neural Transformation Fields for Arbitrary-Styled Font Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Neural_Transformation_Fields_for_Arbitrary-Styled_Font_Generation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/fubinfb/NTF)
  * [DualVector: Unsupervised Vector Font Synthesis with Dual-Part Representation](http://arxiv.org/abs/2305.10462v1)<br>:star:[code](https://github.com/thuliu-yt16/dualvector)
* 手写文本生成
  * [Disentangling Writer and Character Styles for Handwriting Generation](http://arxiv.org/abs/2303.14736v1)<br>:star:[code](https://github.com/dailenson/SDT)
  * [Handwritten Text Generation from Visual Archetypes](http://arxiv.org/abs/2303.15269v1)
* 矢量字体合成
  * [DeepVecFont-v2: Exploiting Transformers to Synthesize Vector Fonts with Higher Quality](http://arxiv.org/abs/2303.14585v1)<br>:star:[code](https://github.com/yizhiwang96/deepvecfont-v2)
* 生成图形文档
  * [Towards Flexible Multi-modal Document Models](http://arxiv.org/abs/2303.18248v1)<br>:star:[code](https://cyberagentailab.github.io/flex-dm)
* 文本检测
  * [Towards Robust Tampered Text Detection in Document Image: New Dataset and New Solution](https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_Towards_Robust_Tampered_Text_Detection_in_Document_Image_New_Dataset_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/qcf-568/DocTamper)
* 文档处理
  * [Unifying Vision, Text, and Layout for Universal Document Processing](https://arxiv.org/abs/2212.02623)
* Scene Text Spotting
  * [Towards Unified Scene Text Spotting based on Sequence Generation](https://arxiv.org/abs/2304.03435)<br>:star:[code](https://github.com/clovaai/units)
  * [Towards Unified Scene Text Spotting based on Sequence Generation](http://arxiv.org/abs/2304.03435v1)<br>:star:[code](https://github.com/clovaai/units)
  * [DeepSolo: Let Transformer Decoder with Explicit Points Solo for Text Spotting](https://arxiv.org/abs/2211.10772)<br>:star:[code](https://github.com/ViTAE-Transformer/DeepSolo)

<a name="34"/>

## 34.Model Compression/Knowledge Distillation/Pruning(模型压缩/知识蒸馏/剪枝)
* [Network Expansion for Practical Training Acceleration](https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_Network_Expansion_for_Practical_Training_Acceleration_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/huawei-noah/Efficient-Computing/tree/master/TrainingAcceleration/NetworkExpansion)
* [Accelerating Dataset Distillation via Model Augmentation](https://arxiv.org/abs/2212.06152)
* [Run, Don’t Walk: Chasing Higher FLOPS for Faster Neural Networks](https://arxiv.org/abs/2303.03667)<br>:star:[code](https://github.com/JierunChen/FasterNet)
* 量化
  * [Solving Oscillation Problem in Post-Training Quantization Through a Theoretical Perspective](https://arxiv.org/abs/2303.11906)<br>:star:[code](https://github.com/bytedance/MRECG)
  * [Adaptive Data-Free Quantization](https://arxiv.org/abs/2303.06869)<br>:star:[code](https://github.com/hfutqian/AdaDFQ)
  * [NIPQ: Noise Proxy-Based Integrated Pseudo-Quantization](https://openaccess.thecvf.com/content/CVPR2023/papers/Shin_NIPQ_Noise_Proxy-Based_Integrated_Pseudo-Quantization_CVPR_2023_paper.pdf)
  * [Bit-Shrinking: Limiting Instantaneous Sharpness for Improving Post-Training Quantization](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Bit-Shrinking_Limiting_Instantaneous_Sharpness_for_Improving_Post-Training_Quantization_CVPR_2023_paper.pdf)
  * [Genie: Show Me the Data for Quantization](https://arxiv.org/abs/2212.04780)
  * [One-Shot Model for Mixed-Precision Quantization](https://openaccess.thecvf.com/content/CVPR2023/papers/Koryakovskiy_One-Shot_Model_for_Mixed-Precision_Quantization_CVPR_2023_paper.pdf)
  * [Post-training Quantization on Diffusion Models](https://arxiv.org/abs/2211.15736)<br>:star:[code](https://github.com/42Shawn/PTQ4DM)
  * [Q-DETR: An Efficient Low-Bit Quantized Detection Transformer](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Q-DETR_An_Efficient_Low-Bit_Quantized_Detection_Transformer_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/SteveTsui/Q-DETR)
  * [NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers](https://arxiv.org/abs/2211.16056)
  * [PD-Quant: Post-Training Quantization Based on Prediction Difference Metric](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_PD-Quant_Post-Training_Quantization_Based_on_Prediction_Difference_Metric_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/hustvl/PD-Quant)
  * [Boost Vision Transformer with GPU-Friendly Sparsity and Quantization](http://arxiv.org/abs/2305.10727v1)
* 剪枝
  * [CP$^3$: Channel Pruning Plug-in for Point-based Networks](http://arxiv.org/abs/2303.13097v1)
  * [Bias in Pruned Vision Models: In-Depth Analysis and Countermeasures](http://arxiv.org/abs/2304.12622v1)
  * [Global Vision Transformer Pruning With Hessian-Aware Saliency](https://arxiv.org/abs/2110.04869)
  * [X-Pruner: eXplainable Pruning for Vision Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_X-Pruner_eXplainable_Pruning_for_Vision_Transformers_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/vickyyu90/XPruner)
  * [DepGraph: Towards Any Structural Pruning](https://arxiv.org/abs/2301.12900)
  * [Progressive Neighbor Consistency Mining for Correspondence Pruning](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Progressive_Neighbor_Consistency_Mining_for_Correspondence_Pruning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/xinliu29/NCMNet)
* MC
  * [Hard Sample Matters a Lot in Zero-Shot Quantization](http://arxiv.org/abs/2303.13826v1)
* KD
  * [DisWOT: Student Architecture Search for Distillation WithOut Training](http://arxiv.org/abs/2303.15678v1)<br>:star:[code](https://lilujunai.github.io/DisWOT-CVPR2023/)
  * [Supervised Masked Knowledge Distillation for Few-Shot Transformers](https://arxiv.org/abs/2303.15466)<br>:star:[code](https://github.com/HL-hanlin/SMKD)
  * [Generalization Matters: Loss Minima Flattening via Parameter Hybridization for Efficient Online Knowledge Distillation](https://arxiv.org/abs/2303.14666)<br>:star:[code](https://github.com/tianlizhang/OKDPH)
  * [KD-DLGAN: Data Limited Image Generation via Knowledge Distillation](http://arxiv.org/abs/2303.17158v1)
  * [TinyMIM: An Empirical Study of Distilling MIM Pre-Trained Models](https://arxiv.org/abs/2301.01296)(https://github.com/OliverRensu/TinyMIM)
  * [Masked Autoencoders Enable Efficient Knowledge Distillers](https://arxiv.org/abs/2208.12256)<br>:star:[code]<br>:star:[code](https://github.com/UCSC-VLAA/DMAE)
  * [Multi-Mode Online Knowledge Distillation for Self-Supervised Visual Representation Learning](http://arxiv.org/abs/2304.06461v1)
  * [Class Attention Transfer Based Knowledge Distillation](http://arxiv.org/abs/2304.12777v1)<br>:star:[code](https://github.com/GzyAftermath/CAT-KD)
  * [DaFKD: Domain-Aware Federated Knowledge Distillation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_DaFKD_Domain-Aware_Federated_Knowledge_Distillation_CVPR_2023_paper.pdf)
  * [Multi-Level Logit Distillation](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Multi-Level_Logit_Distillation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Jin-Ying/Multi-Level-Logit-Distillation)
  * [A Unified Knowledge Distillation Framework for Deep Directed Graphical Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_A_Unified_Knowledge_Distillation_Framework_for_Deep_Directed_Graphical_Models_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/YizhuoChen99/KD4DGM-CVPR)
  * [Enhanced Multimodal Representation Learning with Cross-modal KD](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Enhanced_Multimodal_Representation_Learning_With_Cross-Modal_KD_CVPR_2023_paper.pdf)
  * [Constructing Deep Spiking Neural Networks From Artificial Neural Networks With Knowledge Distillation](http://arxiv.org/abs/2304.05627)
  * 对抗性蒸馏
    * [Boosting Accuracy and Robustness of Student Models via Adaptive Adversarial Distillation](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Boosting_Accuracy_and_Robustness_of_Student_Models_via_Adaptive_Adversarial_CVPR_2023_paper.pdf) 
* 轻量级网络
  * [FeatureBooster: Boosting Feature Descriptors with a Lightweight Neural Network](https://arxiv.org/abs/2211.15069)<br>:star:[code](http://github.com/SJTU-ViSYS/FeatureBooster)
* 去量化
  * [ABCD: Arbitrary Bitwise Coefficient for De-Quantization](https://openaccess.thecvf.com/content/CVPR2023/papers/Han_ABCD_Arbitrary_Bitwise_Coefficient_for_De-Quantization_CVPR_2023_paper.pdf)

<a name="33"/>

## 33.Human-Object Interaction(人物交互)
* [Affordance Diffusion: Synthesizing Hand-Object Interactions](http://arxiv.org/abs/2303.12538)
* [HOICLIP: Efficient Knowledge Transfer for HOI Detection With Vision-Language Models](https://arxiv.org/abs/2303.15786)<br>:star:[code](https://github.com/Artanic30/HOICLIP)
* [ViPLO: Vision Transformer Based Pose-Conditioned Self-Loop Graph for Human-Object Interaction Detection](https://arxiv.org/abs/2304.08114)<br>:star:[code](https://github.com/Jeeseung-Park/ViPLO)
* [Open-Category Human-Object Interaction Pre-Training via Language Modeling Framework](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Open-Category_Human-Object_Interaction_Pre-Training_via_Language_Modeling_Framework_CVPR_2023_paper.pdf)
* [Detecting Human-Object Contact in Images](https://arxiv.org/abs/2303.03373)<br>:house:[project](https://hot.is.tue.mpg.de/)
* [Category Query Learning for Human-Object Interaction Classification](http://arxiv.org/abs/2303.14005v1)
* [Gazeformer: Scalable, Effective and Fast Prediction of Goal-Directed Human Attention](http://arxiv.org/abs/2303.15274v1)
* [Relational Context Learning for Human-Object Interaction Detection](http://arxiv.org/abs/2304.04997v1)
* [HOICLIP: Efficient Knowledge Transfer for HOI Detection with Vision-Language Models](http://arxiv.org/abs/2303.15786v1)<br>:star:[code](https://github.com/Artanic30/HOICLIP)
* [ViPLO: Vision Transformer based Pose-Conditioned Self-Loop Graph for Human-Object Interaction Detection](http://arxiv.org/abs/2304.08114v1)<br>:star:[code](https://github.com/Jeeseung-Park/ViPLO)
* [Visibility Aware Human-Object Interaction Tracking from Single RGB Camera](http://arxiv.org/abs/2303.16479v1)<br>:house:[project](https://virtualhumans.mpi-inf.mpg.de/VisTracker)
* [Instant-NVR: Instant Neural Volumetric Rendering for Human-object Interactions from Monocular RGBD Stream](http://arxiv.org/abs/2304.03184v1)
* [A Neural Modeling Pipeline on Multi-View Human-Object Interactions](https://arxiv.org/abs/2212.07626)
* 双手交互
  * [Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes](https://arxiv.org/pdf/2302.14348.pdf)<br>:star:[code](https://github.com/jyunlee/Im2Hands)
* 手物交互
  * [Visual-Tactile Sensing for In-Hand Object Reconstruction](http://arxiv.org/abs/2303.14498v1)<br>:house:[project](https://sites.google.com/view/vtaco/)
  * [CAMS: CAnonicalized Manipulation Spaces for Category-Level Functional Hand-Object Manipulation Synthesis](http://arxiv.org/abs/2303.15469v1)<br>:star:[code](https://cams-hoi.github.io/)
  * [Transformer-Based Unified Recognition of Two Hands Manipulating Objects](https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_Transformer-Based_Unified_Recognition_of_Two_Hands_Manipulating_Objects_CVPR_2023_paper.pdf)

<a name="32"/>

## 32.Data Augmentation(数据增强)
* [Full or Weak annotations? An adaptive strategy for budget-constrained annotation campaigns](https://arxiv.org/abs/2303.11678)
* [SLACK: Stable Learning of Augmentations With Cold-Start and KL Regularization](https://openaccess.thecvf.com/content/CVPR2023/papers/Marrie_SLACK_Stable_Learning_of_Augmentations_With_Cold-Start_and_KL_Regularization_CVPR_2023_paper.pdf)<br>:star:[code](https://europe.naverlabs.com/slack)
* 关键点定位
  * [Few-shot Geometry-Aware Keypoint Localization](http://arxiv.org/abs/2303.17216v1)<br>:star:[code](https://xingzhehe.github.io/FewShot3DKP/)

<a name="31"/>

## 31.Vision-Language(视觉语言)
* [Learning Visual Representations via Language-Guided Sampling](http://arxiv.org/abs/2302.12248)
* [LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models](http://arxiv.org/abs/2210.01115)
* [Scaling Language-Image Pre-Training via Masking](https://arxiv.org/abs/2212.00794)
* [MAP: Multimodal Uncertainty-Aware Vision-Language Pre-Training Model](https://arxiv.org/abs/2210.05335)
* [Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles](https://arxiv.org/abs/2211.16504)<br>:star:[code](https://github.com/pleaseconnectwifi/DANCE)
* [Few-Shot Learning With Visual Distribution Calibration and Cross-Modal Distribution Alignment](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Few-Shot_Learning_With_Visual_Distribution_Calibration_and_Cross-Modal_Distribution_Alignment_CVPR_2023_paper.pdf)<br>:star:[code](https://gitee.com/mindspore/models/tree/master/research/cv/SADA)
* [ConStruct-VL: Data-Free Continual Structured VL Concepts Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Smith_ConStruct-VL_Data-Free_Continual_Structured_VL_Concepts_Learning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/jamessealesmith/ConStruct-VL)
* [Teaching Structured Vision & Language Concepts to Vision & Language Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Doveh_Teaching_Structured_Vision__Language_Concepts_to_Vision__Language_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/SivanDoveh/TSVLC)
* [Leveraging per Image-Token Consistency for Vision-Language Pre-Training](https://arxiv.org/abs/2211.15398)
* [Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Image_as_a_Foreign_Language_BEiT_Pretraining_for_Vision_and_CVPR_2023_paper.pdf)<br>:house:[project](https://aka.ms/beit-3)
* [CREPE: Can Vision-Language Foundation Models Reason Compositionally?](https://arxiv.org/abs/2212.07796)
* [Open-vocabulary Attribute Detection](https://arxiv.org/abs/2211.12914)<br>:house:[project](https://ovad-benchmark.github.io/)
* [Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training](https://arxiv.org/abs/2301.02280)<br>:star:[code](https://github.com/facebookresearch/diht)
* [FashionSAP: Symbols and Attributes Prompt for Fine-Grained Fashion Vision-Language Pre-Training](https://arxiv.org/abs/2304.05051)
* [Exploring the Effect of Primitives for Compositional Generalization in Vision-and-Language](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Exploring_the_Effect_of_Primitives_for_Compositional_Generalization_in_Vision-and-Language_CVPR_2023_paper.pdf)
* [Task Residual for Tuning Vision-Language Models](https://arxiv.org/abs/2211.10277)<br>:star:[code](https://github.com/geekyutao/TaskRes)
* [Masked Autoencoding Does Not Help Natural Language Supervision at Scale](https://arxiv.org/abs/2301.07836)
* [Open-Set Fine-Grained Retrieval via Prompting Vision-Language Evaluator](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Open-Set_Fine-Grained_Retrieval_via_Prompting_Vision-Language_Evaluator_CVPR_2023_paper.pdf)
* [Visual-Language Prompt Tuning With Knowledge-Guided Context Optimization](https://arxiv.org/abs/2303.13283)
* [Position-Guided Text Prompt for Vision-Language Pre-Training](https://arxiv.org/abs/2212.09737)<br>:star:[code](https://github.com/sail-sg/ptp)
* [RA-CLIP: Retrieval Augmented Contrastive Language-Image Pre-Training](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_RA-CLIP_Retrieval_Augmented_Contrastive_Language-Image_Pre-Training_CVPR_2023_paper.pdf)
* [FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks](https://arxiv.org/abs/2303.02483)<br>:star:[code](https://github.com/BrandonHanx/FAME-ViL)
* [Seeing What You Miss: Vision-Language Pre-Training With Semantic Completion Learning](https://arxiv.org/abs/2211.13437)
* [You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model](https://arxiv.org/abs/2211.11152)
* [DeAR: Debiasing Vision-Language Models with Additive Residuals](https://arxiv.org/abs/2303.10431)
* [Understanding and Constructing Latent Modality Structures in Multi-modal Representation Learning](https://arxiv.org/abs/2303.05952)
* [Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding](http://arxiv.org/abs/2303.12513v1)<br>:star:[code](https://isbertblind.github.io/)
* [VILA: Learning Image Aesthetics from User Comments with Vision-Language Pretraining](http://arxiv.org/abs/2303.14302v1)
* [MAGVLT: Masked Generative Vision-and-Language Transformer](http://arxiv.org/abs/2303.12208v1)
* [Visual-Language Prompt Tuning with Knowledge-guided Context Optimization](http://arxiv.org/abs/2303.13283v1)
* [Top-Down Visual Attention from Analysis by Synthesis](http://arxiv.org/abs/2303.13043v1)<br>:house:[project](https://sites.google.com/view/absvit)
* [Accelerating Vision-Language Pretraining with Free Language Modeling](http://arxiv.org/abs/2303.14038v1)<br>:star:[code](https://github.com/TencentARC/FLM)
* [Multi-Modal Representation Learning with Text-Driven Soft Masks](http://arxiv.org/abs/2304.00719v1)
* [Fine-tuned CLIP models are efficient video learners](https://arxiv.org/abs/2212.03640)<br>:star:[code](https://github.com/muzairkhattak/ViFi-CLIP)
* [MaPLe: Multi-modal Prompt Learning](https://arxiv.org/abs/2210.03117)<br>:star:[code](https://github.com/muzairkhattak/multimodal-prompt-learning)
* [Learning to Name Classes for Vision and Language Models](http://arxiv.org/abs/2304.01830v1)
* [Dynamic Inference With Grounding Based Vision and Language Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Uzkent_Dynamic_Inference_With_Grounding_Based_Vision_and_Language_Models_CVPR_2023_paper.pdf)
* [Connecting Vision and Language with Video Localized Narratives](https://arxiv.org/abs/2302.11217)<br>:house:[project](https://google.github.io/video-localized-narratives/)
* [Bidirectional Cross-Modal Knowledge Exploration for Video Recognition With Pre-Trained Vision-Language Models](https://arxiv.org/abs/2301.00182)<br>:star:[code](https://github.com/whwu95/BIKE)
* [Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks](https://arxiv.org/abs/2211.09808)<br>:star:[code](https://github.com/fundamentalvision/Uni-Perceiver)
* [VILA: Learning Image Aesthetics From User Comments With Vision-Language Pretraining](https://arxiv.org/abs/2303.14302)<br>:star:[code](https://github.com/google-research/google-research/tree/master/vila)
* VLN
  * [Meta-Explore: Exploratory Hierarchical Vision-and-Language Navigation Using Scene Object Spectrum Grounding](https://arxiv.org/abs/2303.04077)<br>:house:[project](https://rllab-snu.github.io/projects/Meta-Explore/doc.html)
  * [Lana: A Language-Capable Navigator for Instruction Following and Generation](https://arxiv.org/abs/2303.08409)<br>:star:[code](https://github.com/wxh1996/LANA-VLN)
  * [LANA: A Language-Capable Navigator for Instruction Following and Generation](https://arxiv.org/abs/2303.08409)
  * [KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation](http://arxiv.org/abs/2303.15796v1)<br>:star:[code](https://github.com/XiangyangLi20/KERM)
  * [Improving Vision-and-Language Navigation by Generating Future-View Image Semantics](http://arxiv.org/abs/2304.04907v1)<br>:star:[code](https://jialuli-luka.github.io/VLN-SIG)
  * [Iterative Vision-and-Language Navigation](https://arxiv.org/abs/2210.03087)
  * [Behavioral Analysis of Vision-and-Language Navigation Agents](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Behavioral_Analysis_of_Vision-and-Language_Navigation_Agents_CVPR_2023_paper.pdf)
  * [Adaptive Zone-Aware Hierarchical Planner for Vision-Language Navigation](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Adaptive_Zone-Aware_Hierarchical_Planner_for_Vision-Language_Navigation_CVPR_2023_paper.pdf)
  * [GeoVLN: Learning Geometry-Enhanced Visual Representation With Slot Attention for Vision-and-Language Navigation](https://openaccess.thecvf.com/content/CVPR2023/papers/Huo_GeoVLN_Learning_Geometry-Enhanced_Visual_Representation_With_Slot_Attention_for_Vision-and-Language_CVPR_2023_paper.pdf)
  * [A New Path: Scaling Vision-and-Language Navigation With Synthetic Instructions and Imitation Learning](https://arxiv.org/abs/2210.03112)
  * [Layout-Based Causal Inference for Object Navigation](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Layout-Based_Causal_Inference_for_Object_Navigation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/sx-zhang/Layout-based-sTDE.git)
  * [KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation](http://arxiv.org/abs/2303.15796)
* 视频语言
  * [Test of Time: Instilling Video-Language Models with a Sense of Time](https://arxiv.org/abs/2301.02074)<br>:house:[project](https://bpiyush.github.io/testoftime-website/index.html)
  * [All in One: Exploring Unified Video-Language Pre-Training](https://arxiv.org/abs/2203.07303)<br>:star:[code](https://github.com/showlab/all-in-one)
  * [HierVL: Learning Hierarchical Video-Language Embeddings](https://arxiv.org/abs/2301.02311)
  * [An Empirical Study of End-to-End Video-Language Transformers With Masked Visual Modeling](https://arxiv.org/abs/2209.01540)<br>:star:[code](https://github.com/tsujuifu/pytorch_empirical-mvm)
  * [Clover: Towards A Unified Video-Language Alignment and Fusion Model](https://arxiv.org/abs/2207.07885)<br>:star:[code](https://github.com/LeeYN-43/Clover)<br>Clover 视频-文本预训练模型在 DiDeMo、MSRVTT 和 LSMDC 三个文本-视频检索任务上取得了 zero-shot 及 finetune performance 的最佳表现；在 8 个主流的视频问答 benchmark 上也达到了新的 state-of-the-art。
  * [VindLU: A Recipe for Effective Video-and-Language Pretraining](https://arxiv.org/abs/2212.05051)<br>:star:[code](https://github.com/klauscc/VindLU)
* LLM
  * [Learning Video Representations From Large Language Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Learning_Video_Representations_From_Large_Language_Models_CVPR_2023_paper.pdf)
* visual grounding
  * [EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding](https://arxiv.org/abs/2209.14941)<br>:star:[code](https://github.com/yanmin-wu/EDA)


<a name="30"/>

## 30.Visual Answer Questions(视觉问答)
* VQA
  * [SimVQA: Exploring Simulated Environments for Visual Question Answering](https://arxiv.org/abs/2203.17219)<br>:house:[project](https://simvqa.github.io)
  * [Logical Implications for Visual Question Answering Consistency](https://openaccess.thecvf.com/content/CVPR2023/papers/Tascon-Morales_Logical_Implications_for_Visual_Question_Answering_Consistency_CVPR_2023_paper.pdf)
  * [S3C: Semi-Supervised VQA Natural Language Explanation via Self-Critical Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Suo_S3C_Semi-Supervised_VQA_Natural_Language_Explanation_via_Self-Critical_Learning_CVPR_2023_paper.pdf)
  * [RMLVQA: A Margin Loss Approach for Visual Question Answering With Language Biases](https://openaccess.thecvf.com/content/CVPR2023/papers/Basu_RMLVQA_A_Margin_Loss_Approach_for_Visual_Question_Answering_With_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/val-iisc/RMLVQA)
  * [VQACL: A Novel Visual Question Answering Continual Learning Setting](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_VQACL_A_Novel_Visual_Question_Answering_Continual_Learning_Setting_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/zhangxi1997/VQACL)
  * [Q: How To Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images!](https://openaccess.thecvf.com/content/CVPR2023/papers/Khan_Q_How_To_Specialize_Large_Vision-Language_Models_to_Data-Scarce_VQA_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/codezakh/SelTDA)
  * [Improving Selective Visual Question Answering by Learning From Your Peers](https://openaccess.thecvf.com/content/CVPR2023/papers/Dancette_Improving_Selective_Visual_Question_Answering_by_Learning_From_Your_Peers_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/selective-vqa_ood)
  * [MixPHM: Redundancy-Aware Parameter-Efficient Tuning for Low-Resource Visual Question Answering](https://arxiv.org/pdf/2303.01239.pdf)
  * [Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2303.01903)<br>:star:[code](https://github.com/MILVLG/prophet)
  * [MD-VQA: Multi-Dimensional Quality Assessment for UGC Live Videos](http://arxiv.org/abs/2303.14933v1)
  * [Divide and Conquer: Answering Questions with Object Factorization and Compositional Reasoning](https://arxiv.org/abs/2303.10482)<br>:star:[code](https://github.com/szzexpoi/POEM)
  * [Prompting Large Language Models With Answer Heuristics for Knowledge-Based Visual Question Answering](https://arxiv.org/abs/2303.01903)<br>:star:[code](https://github.com/MILVLG/prophet)
   * [Generative Bias for Robust Visual Question Answering](https://arxiv.org/abs/2208.00690)
* Video-QA
  * [Learning Situation Hyper-Graphs for Video Question Answering](https://arxiv.org/abs/2304.08682)
  * [Discovering the Real Association: Multimodal Causal Reasoning in Video Question Answering](https://openaccess.thecvf.com/content/CVPR2023/papers/Zang_Discovering_the_Real_Association_Multimodal_Causal_Reasoning_in_Video_Question_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Chuanqi-Zang/Discovering-the-Real-Association)

<a name="29"/>

## 29.SLAM/Augmented Reality/Virtual Reality/Robotics(增强/虚拟现实/机器人)
* 机器人
  * [PartManip: Learning Cross-Category Generalizable Part Manipulation Policy from Point Cloud Observations](http://arxiv.org/abs/2303.16958v1)
  * [Markerless Camera-to-Robot Pose Estimation via Self-supervised Sim-to-Real Transfer](https://arxiv.org/abs/2302.14332)
  * [Robot Structure Prior Guided Temporal Attention for Camera-to-Robot Pose Estimation From Image Sequence](https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_Robot_Structure_Prior_Guided_Temporal_Attention_for_Camera-to-Robot_Pose_Estimation_CVPR_2023_paper.pdf)<br>:house:[project](https://sites.google.com/view/sgtapose)
  * [Phone2Proc: Bringing Robust Robots Into Our Chaotic World](https://arxiv.org/abs/2212.04819)<br>:house:[project](https://allenai.org/project/phone2proc)
  * [DexArt: Benchmarking Generalizable Dexterous Manipulation with Articulated Objects](http://arxiv.org/abs/2305.05706v1)<br>:house:[project](https://www.chenbao.tech/dexart/)
  * [Learning Human-to-Robot Handovers from Point Clouds](http://arxiv.org/abs/2303.17592v1)<br>:star:[code](https://handover-sim2real.github.io)
  * [Neural Volumetric Memory for Visual Locomotion Control](http://arxiv.org/abs/2304.01201v1)<br>:star:[code](https://rchalyang.github.io/NVM)
  * [Affordances from Human Videos as a Versatile Representation for Robotics](http://arxiv.org/abs/2304.08488v1)<br>:star:[code](https://robo-affordances.github.io/)
  * [NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models](http://arxiv.org/abs/2304.09787v1)机器人
  * 机器手抓取
    * [UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy](https://arxiv.org/abs/2303.00938)<br>:house:[project](https://pku-epic.github.io/UniDexGrasp/)
    * [UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum and Iterative Generalist-Specialist Learning](http://arxiv.org/abs/2304.00464v1)
    * [Target-Referenced Reactive Grasping for Dynamic Objects](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Target-Referenced_Reactive_Grasping_for_Dynamic_Objects_CVPR_2023_paper.pdf)<br>:house:[project](https://graspnet.net/reactive)
  * Visual Navigation(视觉导航)
    * [Renderable Neural Radiance Map for Visual Navigation](https://arxiv.org/pdf/2303.00304.pdf)
    * [Object-Goal Visual Navigation via Effective Exploration of Relations Among Historical Navigation States](https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Object-Goal_Visual_Navigation_via_Effective_Exploration_of_Relations_Among_Historical_CVPR_2023_paper.pdf)
* SLAM
  * [Efficient Map Sparsification Based on 2D and 3D Discretized Grids](https://arxiv.org/abs/2303.10882)<br>:star:[code](https://github.com/fishmarch/SLAM_Map_Compression)
  * [Co-SLAM: Joint Coordinate and Sparse Parametric Encodings for Neural Real-Time SLAM](http://arxiv.org/abs/2304.14377v1)<br>:star:[code](https://hengyiwang.github.io/projects/CoSLAM)
  * [ESLAM: Efficient Dense SLAM System Based on Hybrid Representation of Signed Distance Fields](https://arxiv.org/abs/2211.11704)<br>:house:[project](https://www.idiap.ch/paper/eslam/)
  * [ObjectMatch: Robust Registration Using Canonical Object Correspondences](https://openaccess.thecvf.com/content/CVPR2023/papers/Gumeli_ObjectMatch_Robust_Registration_Using_Canonical_Object_Correspondences_CVPR_2023_paper.pdf)<br>:house:[project](https://cangumeli.github.io/ObjectMatch/)
  * [vMAP: Vectorised Object Mapping for Neural Field SLAM](https://arxiv.org/abs/2302.01838)<br>:house:[project](https://kxhit.github.io/vMAP)
* 虚拟试穿
  * [GP-VTON: Towards General Purpose Virtual Try-on via Collaborative Local-Flow Global-Parsing Learning](https://arxiv.org/abs/2303.13756)<br>:star:[code](https://github.com/xiezhy6/GP-VTON)
  * [TryOnDiffusion: A Tale of Two UNets](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_TryOnDiffusion_A_Tale_of_Two_UNets_CVPR_2023_paper.pdf)
  * [Linking Garment With Person via Semantically Associated Landmarks for Virtual Try-On](https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_Linking_Garment_With_Person_via_Semantically_Associated_Landmarks_for_Virtual_CVPR_2023_paper.pdf)<br>:house:[project](https://modelscope.cn/datasets/damo/SAL-HG/summary)
  * [Synthesizing Photorealistic Virtual Humans Through Cross-Modal Disentanglement](https://arxiv.org/abs/2209.01320)
* AR/VR
  * [Affordance Grounding from Demonstration Video to Target Image](http://arxiv.org/abs/2303.14644v1)<br>:star:[code](https://github.com/showlab/afformer)
  * [GarmentTracking: Category-Level Garment Pose Tracking](https://arxiv.org/abs/2303.13913)<br>:house:[project](https://garment-tracking.robotflow.ai/)
  * [Object Pop-Up: Can We Infer 3D Objects and Their Poses From Human Interactions Alone?](https://openaccess.thecvf.com/content/CVPR2023/papers/Petrov_Object_Pop-Up_Can_We_Infer_3D_Objects_and_Their_Poses_CVPR_2023_paper.pdf)
  * [Learning to Zoom and Unzoom](http://arxiv.org/abs/2303.15390v1)<br>:star:[code](https://tchittesh.github.io/lzu/)
  * [Auto-CARD: Efficient and Robust Codec Avatar Driving for Real-time Mobile Telepresence](http://arxiv.org/abs/2304.11835v1)
  * [Avatars Grow Legs: Generating Smooth Human Motion from Sparse Tracking Inputs with Diffusion Model](http://arxiv.org/abs/2304.08577v1)<br>:star:[code](https://dulucas.github.io/agrol/)VR/AR
  * [Auto-CARD: Efficient and Robust Codec Avatar Driving for Real-Time Mobile Telepresence](https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Auto-CARD_Efficient_and_Robust_Codec_Avatar_Driving_for_Real-Time_Mobile_CVPR_2023_paper.pdf)
  * [Affordance Grounding From Demonstration Video To Target Image](https://arxiv.org/abs/2303.14644)<br>:star:[code](https://github.com/showlab/afformer)
  * [Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video](https://arxiv.org/abs/2211.12782)<br>:house:[project](https://seanchenxy.github.io/HandAvatarWeb)
* 混合现实
  * [MixSim: A Hierarchical Framework for Mixed Reality Traffic Simulation](https://openaccess.thecvf.com/content/CVPR2023/papers/Suo_MixSim_A_Hierarchical_Framework_for_Mixed_Reality_Traffic_Simulation_CVPR_2023_paper.pdf)<br>:house:[project](https://waabi.ai/research/mixsim/)
* Visual Localization(视觉定位)
  * [OrienterNet: Visual Localization in 2D Public Maps with Neural Matching](http://arxiv.org/abs/2304.02009v1)
  * [Visual Localization using Imperfect 3D Models from the Internet](http://arxiv.org/abs/2304.05947v1)
  * [SFD2: Semantic-Guided Feature Detection and Description](https://arxiv.org/abs/2304.14845)<br>:star:[code](https://github.com/feixue94/sfd2)
  * [SegLoc: Learning Segmentation-Based Representations for Privacy-Preserving Visual Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Pietrantoni_SegLoc_Learning_Segmentation-Based_Representations_for_Privacy-Preserving_Visual_Localization_CVPR_2023_paper.pdf)
  * [Long-term Visual Localization with Mobile Sensors](https://arxiv.org/abs/2304.07691)
  * [Paired-Point Lifting for Enhanced Privacy-Preserving Visual Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Paired-Point_Lifting_for_Enhanced_Privacy-Preserving_Visual_Localization_CVPR_2023_paper.pdf)
* VPR(Visual Place Recognition)
  * [StructVPR: Distill Structural Knowledge With Weighting Samples for Visual Place Recognition](https://arxiv.org/abs/2212.00937)
  * [Data-efficient Large Scale Place Recognition with Graded Similarity Supervision](https://arxiv.org/abs/2303.11739)<br>:star:[code](https://github.com/marialeyvallina/generalized_contrastive_loss)
 

<a name="28"/>

## 28.Style Transfer(风格迁移)
* [CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer](http://arxiv.org/abs/2303.17867v1)
* [StyleGAN Salon: Multi-View Latent Optimization for Pose-Invariant Hairstyle Transfer](http://arxiv.org/abs/2304.02744v1)<br>:star:[code](https://stylegan-salon.github.io/)
* [Modernizing Old Photos Using Multiple References via Photorealistic Style Transfer](http://arxiv.org/abs/2304.04461v1)<br>:star:[code](https://kaist-viclab.github.io/old-photo-modernization)
* [Master: Meta Style Transformer for Controllable Zero-Shot and Few-Shot Artistic Style Transfer](http://arxiv.org/abs/2304.11818v1)
* [Neural Preset for Color Style Transfer](https://arxiv.org/abs/2303.13511)<br>:house:[project](https://zhkkke.github.io/NeuralPreset)
* [Learning Dynamic Style Kernels for Artistic Style Transfer](https://arxiv.org/abs/2304.00414)
* [Inversion-Based Style Transfer With Diffusion Models](https://arxiv.org/abs/2211.13203)<br>:star:[code](https://github.com/zyxElsa/InST)
* [QuantArt: Quantizing Image Style Transfer Towards High Visual Fidelity](https://arxiv.org/abs/2212.10431)<br>:star:[code](https://github.com/siyuhuang/QuantArt)

<a name="27"/>

## 27.Pose Estimation(物体姿势估计)
* 物体姿势估计
  * [Object Pose Estimation with Statistical Guarantees: Conformal Keypoint Detection and Geometric Uncertainty Propagation](http://arxiv.org/abs/2303.12246v1)
  * [SMOC-Net: Leveraging Camera Pose for Self-Supervised Monocular Object Pose Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_SMOC-Net_Leveraging_Camera_Pose_for_Self-Supervised_Monocular_Object_Pose_Estimation_CVPR_2023_paper.pdf)
  * [HS-Pose: Hybrid Scope Feature Extraction for Category-level Object Pose Estimation](http://arxiv.org/abs/2303.15743v1)
  * [TTA-COPE: Test-Time Adaptation for Category-Level Object Pose Estimation](http://arxiv.org/abs/2303.16730v1)<br>:house:[project](https://taeyeop.com/ttacope)
  * [IMP: Iterative Matching and Pose Estimation with Adaptive Pooling](https://arxiv.org/abs/2304.14837)<br>:star:[code](https://github.com/feixue94/imp-release)
* 6D
  * [Rigidity-Aware Detection for 6D Object Pose Estimation](http://arxiv.org/abs/2303.12396v1)
  * [Neural Texture Learning for Self-Supervised 6D Object Pose Estimation](https://arxiv.org/abs/2212.12902)
  * [Shape-Constraint Recurrent Flow for 6D Object Pose Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Hai_Shape-Constraint_Recurrent_Flow_for_6D_Object_Pose_Estimation_CVPR_2023_paper.pdf)
  * [Knowledge Distillation for 6D Pose Estimation by Aligning Distributions of Local Predictions](https://arxiv.org/abs/2205.14971)
  * [HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling](https://openaccess.thecvf.com/content/CVPR2023/papers/Attal_HyperReel_High-Fidelity_6-DoF_Video_With_Ray-Conditioned_Sampling_CVPR_2023_paper.pdf)<br>:house:[project](https://hyperreel.github.io/)
* 4D
  * [Transfer4D: A Framework for Frugal Motion Capture and Deformation Transfer](https://openaccess.thecvf.com/content/CVPR2023/papers/Maheshwari_Transfer4D_A_Framework_for_Frugal_Motion_Capture_and_Deformation_Transfer_CVPR_2023_paper.pdf)
* 动物姿态估计
  * [ScarceNet: Animal Pose Estimation with Scarce Annotations](http://arxiv.org/abs/2303.15023v1)
  * [Matching Is Not Enough: A Two-Stage Framework for Category-Agnostic Pose Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Shi_Matching_Is_Not_Enough_A_Two-Stage_Framework_for_Category-Agnostic_Pose_CVPR_2023_paper.pdf)<br>:star:[code](github.com/flyinglynx/CapeFormer)
  * [BITE: Beyond Priors for Improved Three-D Dog Pose Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Ruegg_BITE_Beyond_Priors_for_Improved_Three-D_Dog_Pose_Estimation_CVPR_2023_paper.pdf)<br>:house:[project](https://bite.is.tue.mpg.de/)

<a name="26"/>

## 26.GCN/GNN
* GNN
  * [Turning Strengths Into Weaknesses: A Certified Robustness Inspired Attack Framework Against Graph Neural Networks](https://arxiv.org/abs/2303.06199)
  * [From Node Interaction To Hop Interaction: New Effective and Scalable Graph Learning Paradigm](https://arxiv.org/abs/2211.11761)<br>:star:[code](https://github.com/JC-202/HopGNN)

<a name="25"/>

## 25.Fine-Grained/Image Classification(细粒度/图像分类)
* [I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification](https://arxiv.org/abs/2212.02291)
* [Soft Augmentation for Image Classification](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Soft_Augmentation_for_Image_Classification_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/youngleox/soft_augmentation)
* [Explaining Image Classifiers With Multiscale Directional Image Representation](https://openaccess.thecvf.com/content/CVPR2023/papers/Kolek_Explaining_Image_Classifiers_With_Multiscale_Directional_Image_Representation_CVPR_2023_paper.pdf)
* [Equiangular Basis Vectors](https://arxiv.org/abs/2303.11637)<br>:star:[code](https://github.com/NJUST-VIPGroup/Equiangular-Basis-Vectors)
* [Prefix Conditioning Unifies Language and Label Supervision](https://arxiv.org/abs/2206.01125)
* [Improving Image Recognition by Retrieving from Web-Scale Image-Text Data](http://arxiv.org/abs/2304.05173v1)
* [Boosting Verified Training for Robust Image Classifications via Abstraction](https://arxiv.org/abs/2303.11552)<br>:star:[code](https://github.com/zhangzhaodi233/ABSCERT.git)
* [Semantic Prompt for Few-Shot Image Recognition](http://arxiv.org/abs/2303.14123v1)
* [Regularization of polynomial networks for image recognition](http://arxiv.org/abs/2303.13896v1)<br>:star:[code](https://github.com/grigorisg9gr/regularized_polynomials)
* [Active Finetuning: Exploiting Annotation Budget in the Pretraining-Finetuning Paradigm](http://arxiv.org/abs/2303.14382v1)<br>:star:[code](https://github.com/yichen928/ActiveFT)
* [Dynamic Conceptional Contrastive Learning for Generalized Category Discovery](http://arxiv.org/abs/2303.17393v1)<br>:star:[code](https://github.com/TPCD/DCCL)
* [Learning Bottleneck Concepts in Image Classification](http://arxiv.org/abs/2304.10131v1)<br>:house:[project](https://botcl.liangzhili.com/)<br>:star:[code](https://github.com/wbw520/BotCL)
* [Learning Partial Correlation based Deep Visual Representation for Image Classification](http://arxiv.org/abs/2304.11597v1)
* [PIP-Net: Patch-Based Intuitive Prototypes for Interpretable Image Classification](https://openaccess.thecvf.com/content/CVPR2023/papers/Nauta_PIP-Net_Patch-Based_Intuitive_Prototypes_for_Interpretable_Image_Classification_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/M-Nauta/PIPNet)
* [Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification](https://arxiv.org/abs/2211.11158)
* 小样本分类
  * [Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners](https://arxiv.org/abs/2303.02151)<br>:star:[code](https://github.com/ZrrSkywalker/CaFo)
  * [Hubs and Hyperspheres: Reducing Hubness and Improving Transductive Few-shot Learning with Hyperspherical Embeddings](https://arxiv.org/abs/2303.09352)<br>:star:[code](https://github.com/uitml/noHub)
  * [Distilling Self-Supervised Vision Transformers for Weakly-Supervised Few-Shot Classification & Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Distilling_Self-Supervised_Vision_Transformers_for_Weakly-Supervised_Few-Shot_Classification__Segmentation_CVPR_2023_paper.pdf)
* 细粒度
  * [Learning Common Rationale to Improve Self-Supervised Representation for Fine-Grained Visual Recognition Problems](https://arxiv.org/abs/2303.01669)<br>:star:[code](https://github.com/GANPerf/LCR)
  * [Fine-Grained Classification with Noisy Labels](https://arxiv.org/abs/2303.02404)
  * [An Erudite Fine-Grained Visual Classification Model](https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_An_Erudite_Fine-Grained_Visual_Classification_Model_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/PRIS-CV/An-Erudite-FGVC-Model)
  * [Weakly Supervised Posture Mining for Fine-Grained Classification](https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Weakly_Supervised_Posture_Mining_for_Fine-Grained_Classification_CVPR_2023_paper.pdf)
  * [Learning Attribute and Class-Specific Representation Duet for Fine-Grained Fashion Analysis](https://openaccess.thecvf.com/content/CVPR2023/papers/Jiao_Learning_Attribute_and_Class-Specific_Representation_Duet_for_Fine-Grained_Fashion_Analysis_CVPR_2023_paper.pdf)
* 视觉识别
  * [Adapting Shortcut With Normalizing Flow: An Efficient Tuning Framework for Visual Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Adapting_Shortcut_With_Normalizing_Flow_An_Efficient_Tuning_Framework_for_CVPR_2023_paper.pdf)
* 长尾分类
  * [Curvature-Balanced Feature Manifold Learning for Long-Tailed Classification](http://arxiv.org/abs/2303.12307v1)
* 长尾视觉识别
  * [SuperDisco: Super-Class Discovery Improves Visual Recognition for the Long-Tail](http://arxiv.org/abs/2304.00101v1)
  * [Balanced Product of Calibrated Experts for Long-Tailed Recognition](https://arxiv.org/abs/2206.05260)<br>:star:[code](https://github.com/emasa/BalPoE-CalibratedLT)
  * [FCC: Feature Clusters Compression for Long-Tailed Visual Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_FCC_Feature_Clusters_Compression_for_Long-Tailed_Visual_Recognition_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/lijian16/FCC)
  * [Long-tailed Visual Recognition via Gaussian Clouded Logit Adjustment](http://arxiv.org/abs/2305.11733v1)<br>:star:[code](https://github.com/Keke921/GCLLoss)
  * [Global and Local Mixture Consistency Cumulative Learning for Long-tailed Visual Recognitions](http://arxiv.org/abs/2305.08661v1)<br>:star:[code](https://github.com/ynu-yangpeng/GLMC)
  * [Long-Tailed Visual Recognition via Self-Heterogeneous Integration with Knowledge Excavation](http://arxiv.org/abs/2304.01279v1)
  * [Class-Conditional Sharpness-Aware Minimization for Deep Long-Tailed Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Class-Conditional_Sharpness-Aware_Minimization_for_Deep_Long-Tailed_Recognition_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/zzpustc/CC-SAM)
  * [No One Left Behind: Improving the Worst Categories in Long-Tailed Learning](https://arxiv.org/abs/2303.03630)
* 多标签分类
  * [Bridging the Gap between Model Explanations in Partially Annotated Multi-label Classification](http://arxiv.org/abs/2304.01804v1)<br>:star:[code](https://github.com/youngwk/BridgeGapExplanationPAMC)
* 多标签识别
  * [Exploring Structured Semantic Prior for Multi Label Recognition With Incomplete Labels](https://arxiv.org/abs/2303.13223)<br>:star:[code](https://github.com/jameslahm/SCPNet)    
  * [Texts as Images in Prompt Tuning for Multi-Label Image Recognition](https://arxiv.org/abs/2211.12739)<br>:star:[code](https://github.com/guozix/TaI-DPT)
* 多视觉分类
  * [Exploring and Exploiting Uncertainty for Incomplete Multi-View Classification](https://arxiv.org/abs/2304.05165)
* Superclass Learning(超类学习)
  * [Superclass Learning with Representation Enhancement](https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Superclass_Learning_With_Representation_Enhancement_CVPR_2023_paper.pdf)
* 材料分类
  * [Thermal Spread Functions (TSF): Physics-Guided Material Classification](https://arxiv.org/abs/2304.00696)

<a name="24"/>

## 24.Super-Resolution(超分辨率)
* [CiaoSR: Continuous Implicit Attention-in-Attention Network for Arbitrary-Scale Image Super-Resolution](http://arxiv.org/abs/2212.04362)
* [Zero-Shot Dual-Lens Super-Resolution](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Zero-Shot_Dual-Lens_Super-Resolution_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/XrKang/ZeDuSR)
* [Non-Line-of-Sight Imaging With Signal Superresolution Network](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Non-Line-of-Sight_Imaging_With_Signal_Superresolution_Network_CVPR_2023_paper.pdf)
* [Kernel Aware Resampler](https://openaccess.thecvf.com/content/CVPR2023/papers/Bernasconi_Kernel_Aware_Resampler_CVPR_2023_paper.pdf)
* [RobustNeRF: Ignoring Distractors With Robust Losses](https://openaccess.thecvf.com/content/CVPR2023/papers/Gou_Rethinking_Image_Super_Resolution_From_Long-Tailed_Distribution_Learning_Perspective_CVPR_2023_paper.pdf)
* 光场超分辨率
  * [CutMIB: Boosting Light Field Super-Resolution via Multi-View Image Blending](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_CutMIB_Boosting_Light_Field_Super-Resolution_via_Multi-View_Image_Blending_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/zeyuxiao1997/CutMIB)
* ISR
  * [OPE-SR: Orthogonal Position Encoding for Designing a Parameter-free Upsampling Module in Arbitrary-scale Image Super-Resolution](https://arxiv.org/pdf/2303.01091.pdf)
  * [Activating More Pixels in Image Super-Resolution Transformer](https://arxiv.org/abs/2205.04437)<br>:star:[code](https://github.com/XPixelGroup/HAT)
  * [Equivalent Transformation and Dual Stream Network Construction for Mobile Image Super-Resolution](https://openaccess.thecvf.com/content/CVPR2023/papers/Chao_Equivalent_Transformation_and_Dual_Stream_Network_Construction_for_Mobile_Image_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ECNUSR/ETDS)
  * [Learning Generative Structure Prior for Blind Text Image Super-Resolution](http://arxiv.org/abs/2303.14726)
  * [Human Guided Ground-Truth Generation for Realistic Image Super-Resolution](https://arxiv.org/abs/2303.13069)<br>:star:[code](https://github.com/ChrisDud0257/HGGT)
  * [OSRT: Omnidirectional Image Super-Resolution With Distortion-Aware Transformer](https://arxiv.org/abs/2302.03453)<br>:star:[code](https://github.com/Fanghua-Yu/OSRT)
  * [CABM: Content-Aware Bit Mapping for Single Image Super-Resolution Network with Large Input](http://arxiv.org/abs/2304.06454v1)
  * [Memory-Friendly Scalable Super-Resolution via Rewinding Lottery Ticket Hypothesis](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Memory-Friendly_Scalable_Super-Resolution_via_Rewinding_Lottery_Ticket_Hypothesis_CVPR_2023_paper.pdf)
  * [B-Spline Texture Coefficients Estimator for Screen Content Image Super-Resolution](https://openaccess.thecvf.com/content/CVPR2023/papers/Pak_B-Spline_Texture_Coefficients_Estimator_for_Screen_Content_Image_Super-Resolution_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ByeongHyunPak/btc)
  * [Rethinking Image Super Resolution From Long-Tailed Distribution Learning Perspective](https://openaccess.thecvf.com/content/CVPR2023/papers/Gou_Rethinking_Image_Super_Resolution_From_Long-Tailed _Distribution_Learning_Perspective_CVPR_2023_paper.pdf)
  * [Correspondence Transformers With Asymmetric Feature Learning and Matching Flow Super-Resolution](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Correspondence_Transformers_With_Asymmetric_Feature_Learning_and_Matching_Flow_Super-Resolution_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/YXSUNMADMAX/ACTR)
  * [Toward Accurate Post-Training Quantization for Image Super Resolution](https://openaccess.thecvf.com/content/CVPR2023/papers/Tu_Toward_Accurate_Post-Training_Quantization_for_Image_Super_Resolution_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/huawei-noah/Efficient-Computing/tree/master/Quantization/PTQ4SR)
  * [Image Super-Resolution Using T-Tetromino Pixels](https://openaccess.thecvf.com/content/CVPR2023/papers/Grosche_Image_Super-Resolution_Using_T-Tetromino_Pixels_CVPR_2023_paper.pdf)
  * [Spectral Bayesian Uncertainty for Image Super-Resolution](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Spectral_Bayesian_Uncertainty_for_Image_Super-Resolution_CVPR_2023_paper.pdf)
  * [Super-Resolution Neural Operator](https://arxiv.org/pdf/2303.02584.pdf)<br>:star:[code](https://github.com/2y7c3/Super-Resolution-Neural-Operator)
  * [Local Implicit Normalizing Flow for Arbitrary-Scale Image Super-Resolution](https://arxiv.org/abs/2303.05156)
  * [Better "CMOS" Produces Clearer Images: Learning Space-Variant Blur Estimation for Blind Image Super-Resolution](http://arxiv.org/abs/2304.03542v1)
  * [Human Guided Ground-truth Generation for Realistic Image Super-resolution](http://arxiv.org/abs/2303.13069v1)<br>:star:[code](https://github.com/ChrisDud0257/HGGT)
  * [Implicit Diffusion Models for Continuous Super-Resolution](http://arxiv.org/abs/2303.16491v1)
  * [Better "CMOS" Produces Clearer Images: Learning Space-Variant Blur Estimation for Blind Image Super-Resolution](https://arxiv.org/abs/2304.03542)
  * [Guided Depth Super-Resolution by Deep Anisotropic Diffusion](https://arxiv.org/abs/2211.11592)<br>:star:[code](https://github.com/prs-eth/Diffusion-Super-Resolution)
  * [Omni Aggregation Networks for Lightweight Image Super-Resolution](http://arxiv.org/abs/2304.10244v1)<br>:star:[code](https://github.com/Francis0625/Omni-SR)
* VSR
  * [Towards High-Quality and Efficient Video Super-Resolution via Spatial-Temporal Data Overfitting](https://arxiv.org/abs/2303.08331)<br>:star:[code](https://github.com/coulsonlee/STDO-CVPR2023.git)
  * [Compression-Aware Video Super-Resolution](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Compression-Aware_Video_Super-Resolution_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/aprBlue/CAVSR)
  * [Structured Sparsity Learning for Efficient Video Super-Resolution](https://arxiv.org/abs/2206.07687)<br>:star:[code](https://github.com/Zj-BinXia/SSL)
  * [Consistent Direct Time-of-Flight Video Depth Super-Resolution](https://arxiv.org/abs/2211.08658)<br>:star:[code](https://github.com/facebookresearch/DVSR/)
* 文本图像超分辨率
  * [Learning Generative Structure Prior for Blind Text Image Super-resolution](http://arxiv.org/abs/2303.14726v1)<br>:star:[code](https://github.com/csxmli2016/MARCONet)
* Image Resampling(图像重采样)
  * [Learning Steerable Function for Efficient Image Resampling](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Learning_Steerable_Function_for_Efficient_Image_Resampling_CVPR_2023_paper.pdf)


<a name="23"/>

## 23.Image Retrieval(图像检索)
* [Towards a Smaller Student: Capacity Dynamic Distillation for Efficient Image Retrieval](https://arxiv.org/abs/2303.09230)
* [Asymmetric Feature Fusion for Image Retrieval](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Asymmetric_Feature_Fusion_for_Image_Retrieval_CVPR_2023_paper.pdf)
* [Improving Image Recognition by Retrieving From Web-Scale Image-Text Data](https://arxiv.org/abs/2304.05173)
* [Boundary-aware Backward-Compatible Representation via Adversarial Learning in Image Retrieval](http://arxiv.org/abs/2305.02610v1)<br>:star:[code](https://github.com/Ashespt/AdvBCT)
* [Revisiting Self-Similarity: Structural Embedding for Image Retrieval](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Revisiting_Self-Similarity_Structural_Embedding_for_Image_Retrieval_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/sungonce/SENet)
* [Train/Test-Time Adaptation With Retrieval](https://arxiv.org/abs/2303.14333)
* [Pic2Word: Mapping Pictures to Words for Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2302.03084)<br>:star:[code](https://github.com/google-research/composed_image_retrieval)
* 基于草图的图像检索
  * [Data-Free Sketch-Based Image Retrieval](https://arxiv.org/abs/2303.07775)<br>:star:[code](https://github.com/abhrac/data-free-sbir)
  * [CLIP for All Things Zero-Shot Sketch-Based Image Retrieval, Fine-Grained or Not](http://arxiv.org/abs/2303.13440v1)
  * [Exploiting Unlabelled Photos for Stronger Fine-Grained SBIR](http://arxiv.org/abs/2303.13779v1)<br>:star:[code](https://aneeshan95.github.io/Sketch_PVT/)
  * [Zero-Shot Everything Sketch-Based Image Retrieval, and in Explainable Style](http://arxiv.org/abs/2303.14348v1)<br>:star:[code](https://github.com/buptLinfy/ZSE-SBIR)
* 视频-文本检索
  * [Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning](http://arxiv.org/abs/2303.14369v1)<br>:star:[code](https://jpthu17.github.io/HBI/)
  * [Dual Alignment Unsupervised Domain Adaptation for Video-Text Retrieval](https://openaccess.thecvf.com/content/CVPR2023/papers/Hao_Dual_Alignment_Unsupervised_Domain_Adaptation_for_Video-Text_Retrieval_CVPR_2023_paper.pdf)
* 视频-文本
  * [SViTT: Temporal Learning of Sparse Video-Text Transformers](http://arxiv.org/abs/2304.08809v1)<br>:house:[project](http://svcl.ucsd.edu/projects/svitt)视频文本检索和问答
* 多模态检索
  * [ImageBind: One Embedding Space To Bind Them All](http://arxiv.org/abs/2305.05665v1)<br>:house:[project](https://imagebind.metademolab.com/)<br>:star:[code](https://github.com/facebookresearch/ImageBind)
* 跨模态检索
  * [VoP: Text-Video Co-Operative Prompt Tuning for Cross-Modal Retrieval](https://arxiv.org/abs/2211.12764)<br>:star:[code](https://github.com/bighuang624/VoP)
  * [RONO: Robust Discriminative Learning With Noisy Labels for 2D-3D Cross-Modal Retrieval](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_RONO_Robust_Discriminative_Learning_With_Noisy_Labels_for_2D-3D_Cross-Modal_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/penghu-cs/RONO)
* 文本-图像匹配
  * [Learning Semantic Relationship among Instances for Image-Text Matching](https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Learning_Semantic_Relationship_Among_Instances_for_Image-Text_Matching_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/CrossmodalGroup/HREM)
  * [Fine-Grained Image-Text Matching by Cross-Modal Hard Aligning Network](https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_Fine-Grained_Image-Text_Matching_by_Cross-Modal_Hard_Aligning_Network_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ppanzx/CHAN)
* 图像文本检索
  * [Multilateral Semantic Relations Modeling for Image Text Retrieval](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Multilateral_Semantic_Relations_Modeling_for_Image_Text_Retrieval_CVPR_2023_paper.pdf)
* 文本-视频检索
  * [Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?](https://arxiv.org/abs/2301.00184)<br>:star:[code](https://github.com/whwu95/Cap4Video)
* 视频语言检索
  * [CLIPPING: Distilling CLIP-Based Models With a Student Base for Video-Language Retrieval](https://openaccess.thecvf.com/content/CVPR2023/papers/Pei_CLIPPING_Distilling_CLIP-Based_Models_With_a_Student_Base_for_Video-Language_CVPR_2023_paper.pdf)

<a name="22"/>

## 22.Image Synthesis/Generation(图像合成)
* [LayoutDiffusion: Controllable Diffusion Model for Layout-to-Image Generation](https://arxiv.org/abs/2303.17189)<br>:star:[code](https://github.com/ZGCTroy/LayoutDiffusion)
* [Zero-shot Generative Model Adaptation via Image-specific Prompt Learning](http://arxiv.org/abs/2304.03119v1)<br>:star:[code](https://github.com/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation)
* [TopNet: Transformer-based Object Placement Network for Image Compositing](https://arxiv.org/abs/2304.03372)
* 基于草图生成
  * [Picture that Sketch: Photorealistic Image Generation from Abstract Sketches](https://arxiv.org/abs/2303.11162)<br>:house:[project](https://subhadeepkoley.github.io/PictureThatSketch)
* 图像-视频合成
  * [Conditional Image-to-Video Generation with Latent Flow Diffusion Models](http://arxiv.org/abs/2303.13744v1)<br>:star:[code](https://github.com/nihaomiao/CVPR23_LFDM)
* 海报生成
  * [Unsupervised Domain Adaption with Pixel-level Discriminator for Image-aware Layout Generation](http://arxiv.org/abs/2303.14377v1)
* 文本-图像合成
  * [Variational Distribution Learning for Unsupervised Text-to-Image Generation](http://arxiv.org/abs/2303.16105v1)
  * [ReCo: Region-Controlled Text-to-Image Generation](https://arxiv.org/abs/2211.15518)
  * [Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation](http://arxiv.org/abs/2304.01816v1)
  * [Multi-Concept Customization of Text-to-Image Diffusion](https://arxiv.org/abs/2212.04488)<br>:house:[project](https://www.cs.cmu.edu/~custom-diffusion/dataset.html)
  * [Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models](https://arxiv.org/abs/2212.14704)<br>:house:[project](https://bluestyle97.github.io/dream3d/)
  * [Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models](https://arxiv.org/abs/2212.08698)<br>:star:[code](https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement)
  * [GLIGEN: Open-Set Grounded Text-to-Image Generation](https://arxiv.org/abs/2301.07093)
  * [RIATIG: Reliable and Imperceptible Adversarial Text-to-Image Generation With Natural Prompts](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_RIATIG_Reliable_and_Imperceptible_Adversarial_Text-to-Image_Generation_With_Natural_Prompts_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/WUSTL-CSPL/RIATIG)
  * [GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis](https://arxiv.org/abs/2301.12959)<br>:star:[code](https://github.com/tobran/GALIP)
  * [Shifted Diffusion for Text-to-image Generation](https://arxiv.org/abs/2211.15388)<br>:star:[code](https://github.com/drboog/Shifted_Diffusion)
  * [Conditional Text Image Generation With Diffusion Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Conditional_Text_Image_Generation_With_Diffusion_Models_CVPR_2023_paper.pdf)
  * [Scaling Up GANs for Text-to-Image Synthesis](https://arxiv.org/abs/2303.05511)<br>:house:[project](https://mingukkang.github.io/GigaGAN/)
* prompting
  * [Diversity-Aware Meta Visual Prompting](https://arxiv.org/abs/2303.08138)<br>:star:[code](https://github.com/shikiw/DAM-VP)
* 图像生成
  * [LayoutDM: Discrete Diffusion Model for Controllable Layout Generation](https://arxiv.org/abs/2303.08137)<br>:house:[project](https://cyberagentailab.github.io/layout-dm/)
  * [Unsupervised Domain Adaption With Pixel-Level Discriminator for Image-Aware Layout Generation](http://arxiv.org/abs/2303.14377)
  * [SpaText: Spatio-Textual Representation for Controllable Image Generation](https://arxiv.org/abs/2211.14305)<br>:house:[project](https://omriavrahami.com/spatext)
  * [MaskSketch: Unpaired Structure-Guided Masked Image Generation](https://arxiv.org/abs/2302.05496)
  * [Where Is My Spot? Few-Shot Image Generation via Latent Subspace Optimization](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Where_Is_My_Spot_Few-Shot_Image_Generation_via_Latent_Subspace_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/chansey0529/LSO)
  * [Not All Image Regions Matter: Masked Vector Quantization for Autoregressive Image Generation](https://arxiv.org/abs/2305.13607)<br>:star:[code](https://github.com/CrossmodalGroup/MaskedVectorQuantization)
  * [Controllable Mesh Generation Through Sparse Latent Point Diffusion Models](https://arxiv.org/abs/2303.07938)<br>:house:[project](https://slide-3d.github.io/) 
  * [NoisyTwins: Class-Consistent and Diverse Image Generation Through StyleGANs](https://arxiv.org/abs/2304.05866)<br>:house:[project](https://rangwani-harsh.github.io/NoisyTwins/)
  * [Exploring Incompatible Knowledge Transfer in Few-shot Image Generation](http://arxiv.org/abs/2304.07574v1)
  * [Wavelet Diffusion Models Are Fast and Scalable Image Generators](https://arxiv.org/abs/2211.16152)<br>:star:[code](https://github.com/VinAIResearch/WaveDiff.git)
  * [Picture That Sketch: Photorealistic Image Generation From Abstract Sketches](https://arxiv.org/abs/2303.11162)<br>:house:[project](https://subhadeepkoley.github.io/PictureThatSketch)
  * [DiffCollage: Parallel Generation of Large Content with Diffusion Models](http://arxiv.org/abs/2303.17076v1)<br>:house:[project](https://research.nvidia.com/labs/dir/diffcollage) 
  * [Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization](http://arxiv.org/abs/2305.11718v1)<br>:star:[code](https://github.com/CrossmodalGroup/DynamicVectorQuantization)
  * [LayoutDiffusion: Controllable Diffusion Model for Layout-to-image Generation](http://arxiv.org/abs/2303.17189v1)<br>:star:[code](https://github.com/ZGCTroy/LayoutDiffusion)
  * [Domain Expansion of Image Generators](https://arxiv.org/abs/2301.05225)<br>:house:[project](https://yotamnitzan.github.io/domain-expansion/)
* 视频生成
  * [Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models](http://arxiv.org/abs/2304.08818v1)<br>:house:[project](https://research.nvidia.com/labs/toronto-ai/VideoLDM/)  
  * 文本驱动的视频合成
    * [Tell Me What Happened: Unifying Text-Guided Video Completion via Multimodal Masked Video Generation](https://arxiv.org/abs/2211.12824)
* Image Synthesis(图像合成)
  * [Learning 3D-aware Image Synthesis with Unknown Pose Distribution](https://arxiv.org/abs/2301.07702)<br>:house:[project](https://vivianszf.github.io/pof3d/)
  * [3D-Aware Conditional Image Synthesis](http://arxiv.org/abs/2302.08509)
  * [SceneComposer: Any-Level Semantic Image Synthesis](https://arxiv.org/abs/2211.11742)<br>:house:[project](https://zengxianyu.github.io/scenec/)
  * [RWSC-Fusion: Region-Wise Style-Controlled Fusion Network for the Prohibited X-Ray Security Image Synthesis](https://openaccess.thecvf.com/content/CVPR2023/papers/Duan_RWSC-Fusion_Region-Wise_Style-Controlled_Fusion_Network_for_the_Prohibited_X-Ray_Security_CVPR_2023_paper.pdf)
  * [Exploring Intra-Class Variation Factors With Learnable Cluster Prompts for Semi-Supervised Image Synthesis](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Exploring_Intra-Class_Variation_Factors_With_Learnable_Cluster_Prompts_for_Semi-Supervised_CVPR_2023_paper.pdf)
  * [Quantitative Manipulation of Custom Attributes on 3D-Aware Image Synthesis](https://openaccess.thecvf.com/content/CVPR2023/papers/Do_Quantitative_Manipulation_of_Custom_Attributes_on_3D-Aware_Image_Synthesis_CVPR_2023_paper.pdf)
  * [Inferring and Leveraging Parts From Object Shape for Improving Semantic Image Synthesis](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Inferring_and_Leveraging_Parts_From_Object_Shape_for_Improving_Semantic_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/csyxwei/iPOSE)
  * [MAGE: MAsked Generative Encoder To Unify Representation Learning and Image Synthesis](https://arxiv.org/abs/2211.09117)<br>:star:[code](https://github.com/LTH14/mage)
  * [Person Image Synthesis via Denoising Diffusion Model](https://arxiv.org/abs/2211.12500)
  * [Freestyle Layout-to-Image Synthesis](http://arxiv.org/abs/2303.14412v1)<br>:star:[code](https://github.com/essunny310/FreestyleNet)
  * [Few-shot Semantic Image Synthesis with Class Affinity Transfer](http://arxiv.org/abs/2304.02321v1)图像合成
  * [Regularized Vector Quantization for Tokenized Image Synthesis](https://arxiv.org/abs/2303.06424)
  * [High-Fidelity Guided Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2211.17084)<br>:house:[project](https://1jsingh.github.io/gradop)
  * [PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing](https://openaccess.thecvf.com/content/CVPR2023/papers/Sheng_PixHt-Lab_Pixel_Height_Based_Light_Effect_Generation_for_Image_Compositing_CVPR_2023_paper.pdf)<br>:house:[project](https://shengcn.github.io/PixHtLab/)
* 文本-运动生成
  * [Being Comes From Not-Being: Open-Vocabulary Text-to-Motion Generation With Wordless Training](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Being_Comes_From_Not-Being_Open-Vocabulary_Text-to-Motion_Generation_With_Wordless_Training_CVPR_2023_paper.pdf)<br>:house:[project](https://github.com/junfanlin/)
* 纹理合成
  * [Neural Texture Synthesis With Guided Correspondence](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Neural_Texture_Synthesis_With_Guided_Correspondence_CVPR_2023_paper.pdf)

<a name="21"/>

## 21.UAV/Remote Sensing/Satellite Image(无人机/遥感/卫星图像)
* [TopDiG: Class-Agnostic Topological Directional Graph Extraction From Remote Sensing Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_TopDiG_Class-Agnostic_Topological_Directional_Graph_Extraction_From_Remote_Sensing_Images_CVPR_2023_paper.pdf)
* [Change-Aware Sampling and Contrastive Learning for Satellite Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Mall_Change-Aware_Sampling_and_Contrastive_Learning_for_Satellite_Images_CVPR_2023_paper.pdf)
* [MethaneMapper: Spectral Absorption aware Hyperspectral Transformer for Methane Detection](http://arxiv.org/abs/2304.02767v1)
* [ViTs for SITS: Vision Transformers for Satellite Image Time Series](https://arxiv.org/abs/2301.04944)
* [Adaptive Sparse Convolutional Networks With Global Context Enhancement for Faster Object Detection on Drone Images](https://arxiv.org/abs/2303.14488)<br>:star:[code](https://github.com/Cuogeihong/CEASC)
* 图像检测
  * [Adaptive Sparse Convolutional Networks with Global Context Enhancement for Faster Object Detection on Drone Images](http://arxiv.org/abs/2303.14488v1)<br>:star:[code](https://github.com/Cuogeihong/CEASC)
* 跟踪
  * [Resource-Efficient RGBD Aerial Tracking](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Resource-Efficient_RGBD_Aerial_Tracking_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/yjybuaa/RGBDAerialTracking)
* 雷达定位
  * [SGLoc: Scene Geometry Encoding for Outdoor LiDAR Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SGLoc_Scene_Geometry_Encoding_for_Outdoor_LiDAR_Localization_CVPR_2023_paper.pdf)
* 无人机目标检测
  * [Generalized UAV Object Detection via Frequency Domain Disentanglement](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Generalized_UAV_Object_Detection_via_Frequency_Domain_Disentanglement_CVPR_2023_paper.pdf)

<a name="20"/>

## 20.Autonomous vehicles(自动驾驶)
* 自动驾驶
  * [UniSim: A Neural Closed-Loop Sensor Simulator](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_UniSim_A_Neural_Closed-Loop_Sensor_Simulator_CVPR_2023_paper.pdf)<br>:house:[project](https://waabi.ai/research/unisim/)
  * [Temporal Consistent 3D LiDAR Representation Learning for Semantic Perception in Autonomous Driving](https://openaccess.thecvf.com/content/CVPR2023/papers/Nunes_Temporal_Consistent_3D_LiDAR_Representation_Learning_for_Semantic_Perception_in_CVPR_2023_paper.pdf)
  * [TBP-Former: Learning Temporal Bird's-Eye-View Pyramid for Joint Perception and Prediction in Vision-Centric Autonomous Driving](https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_TBP-Former_Learning_Temporal_Birds-Eye-View_Pyramid_for_Joint_Perception_and_Prediction_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/MediaBrain-SJTU/TBP-Former)
  * [Weakly Supervised Class-Agnostic Motion Prediction for Autonomous Driving](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Weakly_Supervised_Class-Agnostic_Motion_Prediction_for_Autonomous_Driving_CVPR_2023_paper.pdf)
  * [Learning and Aggregating Lane Graphs for Urban Automated Driving](https://openaccess.thecvf.com/content/CVPR2023/papers/Buchner_Learning_and_Aggregating_Lane_Graphs_for_Urban_Automated_Driving_CVPR_2023_paper.pdf)<br>:star:[code](http://urbanlanegraph.cs.uni-freiburg.de/)
  * [RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving](https://arxiv.org/abs/2301.10222)<br>:star:[code](https://github.com/valeoai/rangevit)
  * [Azimuth Super-Resolution for FMCW Radar in Autonomous Driving](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Azimuth_Super-Resolution_for_FMCW_Radar_in_Autonomous_Driving_CVPR_2023_paper.pdf)
  * [Unsupervised 3D Point Cloud Representation Learning by Triangle Constrained Contrast for Autonomous Driving](https://openaccess.thecvf.com/content/CVPR2023/papers/Pang_Unsupervised_3D_Point_Cloud_Representation_Learning_by_Triangle_Constrained_Contrast_CVPR_2023_paper.pdf)<br>:house:[project](https://bopang1996.github.io/)
  * [Localized Semantic Feature Mixers for Efficient Pedestrian Detection in Autonomous Driving](https://openaccess.thecvf.com/content/CVPR2023/papers/Khan_Localized_Semantic_Feature_Mixers_for_Efficient_Pedestrian_Detection_in_Autonomous_CVPR_2023_paper.pdf)
  * [DeepMapping2: Self-Supervised Large-Scale LiDAR Map Optimization](https://arxiv.org/abs/2212.06331)
  * [Visual Exemplar Driven Task-Prompting for Unified Perception in Autonomous Driving](https://arxiv.org/abs/2303.01788)
  * [ReasonNet: End-to-End Driving with Temporal and Global Reasoning](http://arxiv.org/abs/2305.10507v1)
  * [LiDAR2Map: In Defense of LiDAR-Based Semantic Map Construction Using Online Camera Distillation](http://arxiv.org/abs/2304.11379v1)<br>:star:[code](https://github.com/songw-zju/LiDAR2Map)
  * [Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2302.07817)<br>:star:[code](https://github.com/wzzheng/TPVFormer)
  * [Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving](https://openaccess.thecvf.com/content/CVPR2023/papers/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.pdf)
* [MSeg3D: Multi-Modal 3D Semantic Segmentation for Autonomous Driving](https://arxiv.org/abs/2303.08600)<br>:star:[code](https://github.com/jialeli1/lidarseg3d)
  * [Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving](http://arxiv.org/abs/2305.06242v1)
  * [GINA-3D: Learning to Generate Implicit Neural Assets in the Wild](http://arxiv.org/abs/2304.02163v1)自动驾驶
  * [Neural Map Prior for Autonomous Driving](http://arxiv.org/abs/2304.08481v1)
* 轨迹预测
  * [IPCC-TP: Utilizing Incremental Pearson Correlation Coefficient for Joint Multi-Agent Trajectory Prediction](https://arxiv.org/pdf/2303.00575.pdf)
  * [ViP3D: End-to-End Visual Trajectory Prediction via 3D Agent Queries](https://arxiv.org/abs/2208.01582)
  * [Query-Centric Trajectory Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Query-Centric_Trajectory_Prediction_CVPR_2023_paper.pdf)
  * [Leapfrog Diffusion Model for Stochastic Trajectory Prediction](https://arxiv.org/abs/2303.10895)<br>:star:[code](https://github.com/MediaBrain-SJTU/LED)
  * [Uncovering the Missing Pattern: Unified Framework Towards Trajectory Imputation and Prediction](http://arxiv.org/abs/2303.16005v1)<br>:star:[code](https://github.com/colorfulfuture/GC-VRNN)
  * [FEND: A Future Enhanced Distribution-Aware Contrastive Learning Framework for Long-tail Trajectory Prediction](http://arxiv.org/abs/2303.16574v1)
  * [Unsupervised Sampling Promoting for Stochastic Human Trajectory Prediction](http://arxiv.org/abs/2304.04298v1)
  * [Stimulus Verification Is a Universal and Effective Sampler in Multi-Modal Human Trajectory Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Stimulus_Verification_Is_a_Universal_and_Effective_Sampler_in_Multi-Modal_CVPR_2023_paper.pdf)
* Place Recognition
  * [Data-efficient Large Scale Place Recognition with Graded Similarity Supervision](https://arxiv.org/abs/2303.11739)<br>:star:[code](https://github.com/marialeyvallina/generalized_contrastive_loss)
  * [R2Former: Unified Retrieval and Reranking Transformer for Place Recognition](https://arxiv.org/abs/2304.03410)<br>:star:[code](https://github.com/Jeff-Zilence/R2Former)
* 车道线检测
  * [BEV-LaneDet: An Efficient 3D Lane Detection Based on Virtual Camera via Key-Points](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_BEV-LaneDet_An_Efficient_3D_Lane_Detection_Based_on_Virtual_Camera_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/gigo-team/bev_lane_det)  
  * [Anchor3DLane: Learning To Regress 3D Anchors for Monocular 3D Lane Detection](https://arxiv.org/abs/2301.02371)<br>:star:[code](https://github.com/tusen-ai/Anchor3DLane)
* 鸟瞰识别
  * [BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_BEVFormer_v2_Adapting_Modern_Image_Backbones_to_Birds-Eye-View_Recognition_via_CVPR_2023_paper.pdf)
  * [SkyEye: Self-Supervised Bird's-Eye-View Semantic Mapping Using Monocular Frontal View Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Gosala_SkyEye_Self-Supervised_Birds-Eye-View_Semantic_Mapping_Using_Monocular_Frontal_View_Images_CVPR_2023_paper.pdf)<br>:house:[project](http://skyeye.cs.uni-freiburg.de/)

<a name="19"/>

## 19.Neural Architecture Search(神经架构搜索)
* [PA&DA: Jointly Sampling PAth and DAta for Consistent NAS](https://arxiv.org/pdf/2302.14772.pdf)<br>:star:[code](https://github.com/ShunLu91/PA-DA)
* [Differentiable Architecture Search With Random Features](https://arxiv.org/abs/2208.08835)
* [Adversarially Robust Neural Architecture Search for Graph Neural Networks](https://arxiv.org/abs/2304.04168)
* [MDL-NAS: A Joint Multi-Domain Learning Framework for Vision Transformer](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MDL-NAS_A_Joint_Multi-Domain_Learning_Framework_for_Vision_Transformer_CVPR_2023_paper.pdf)
* [HOTNAS: Hierarchical Optimal Transport for Neural Architecture Search](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_HOTNAS_Hierarchical_Optimal_Transport_for_Neural_Architecture_Search_CVPR_2023_paper.pdf)

<a name="18"/>

## 18.Person Re-Identification(人员重识别)
* [Event-Guided Person Re-Identification via Sparse-Dense Complementary Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Event-Guided_Person_Re-Identification_via_Sparse-Dense_Complementary_Learning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Chengzhi-Cao/SDCL)
* [Patch-Wise High-Frequency Augmentation for Transformer-Based Person Re-Identification](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_PHA_Patch-Wise_High-Frequency_Augmentation_for_Transformer-Based_Person_Re-Identification_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/zhangguiwei610/PHA)
* [TranSG: Transformer-Based Skeleton Graph Prototype Contrastive Learning with Structure-Trajectory Prompted Reconstruction for Person Re-Identification](https://arxiv.org/abs/2303.06819)<br>:star:[code](https://github.com/Kali-Hac/TranSG)
* 人员检索
  * [Cross-Modal Implicit Relation Reasoning and Aligning for Text-to-Image Person Retrieval](http://arxiv.org/abs/2303.12501v1)<br>:star:[code](https://github.com/anosorae/IRRA)
* 可见光-红外人员重识别(VIReID)
  * [Diverse Embedding Expansion Network and Low-Light Cross-Modality Benchmark for Visible-Infrared Person Re-identification](http://arxiv.org/abs/2303.14481v1)<br>:star:[code](https://github.com/ZYK100/LLCM)
  * [Shape-Erased Feature Learning for Visible-Infrared Person Re-Identification](http://arxiv.org/abs/2304.04205v1)
  * [PartMix: Regularization Strategy to Learn Part Discovery for Visible-Infrared Person Re-identification](http://arxiv.org/abs/2304.01537v1)可见光-红外人员重识别(VI-ReID)
* G-ReID
  * [Similarity Metric Learning for RGB-Infrared Group Re-Identification](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_Similarity_Metric_Learning_for_RGB-Infrared_Group_Re-Identification_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/WhollyOat/CM-Group)
* 行人检测
  * [VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision](http://arxiv.org/abs/2304.03135v1)<br>:star:[code](https://github.com/lmy98129/VLPD)
  * [Optimal Proposal Learning for Deployable End-to-End Pedestrian Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Optimal_Proposal_Learning_for_Deployable_End-to-End_Pedestrian_Detection_CVPR_2023_paper.pdf)
* 人群计数
  * [CrowdCLIP: Unsupervised Crowd Counting via Vision-Language Model](http://arxiv.org/abs/2304.04231v1)<br>:star:[code](https://github.com/dk-liang/CrowdCLIP)
  * [Boosting Detection in Crowd Analysis via Underutilized Output Features](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Boosting_Detection_in_Crowd_Analysis_via_Underutilized_Output_Features_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/wskingdom/Crowd-Hat)
  * [Optimal Transport Minimization: Crowd Localization on Density Maps for Semi-Supervised Counting](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Optimal_Transport_Minimization_Crowd_Localization_on_Density_Maps_for_Semi-Supervised_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Elin24/OT-M)
* 步态识别
  * [Dynamic Aggregated Network for Gait Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_Dynamic_Aggregated_Network_for_Gait_Recognition_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/XKMar/FastGait)
  * [LidarGait: Benchmarking 3D Gait Recognition With Point Clouds](https://arxiv.org/abs/2211.10598)<br>:house:[project](https://lidargait.github.io/)
  * [GaitGCI: Generative Counterfactual Intervention for Gait Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Dou_GaitGCI_Generative_Counterfactual_Intervention_for_Gait_Recognition_CVPR_2023_paper.pdf)

<a name="17"/>

## 17.Medical Image(医学影像)
* [Geometric Visual Similarity Learning in 3D Medical Image Self-Supervised Pre-Training](http://arxiv.org/abs/2303.00874)
* [Interventional Bag Multi-Instance Learning on Whole-Slide Pathological Images](https://arxiv.org/abs/2303.06873)<br>:star:[code](https://github.com/HHHedo/IBMIL)
* [Causally-Aware Intraoperative Imputation for Overall Survival Time Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Causally-Aware_Intraoperative_Imputation_for_Overall_Survival_Time_Prediction_CVPR_2023_paper.pdf)
* [Flexible-Cm GAN: Towards Precise 3D Dose Prediction in Radiotherapy](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Flexible-Cm_GAN_Towards_Precise_3D_Dose_Prediction_in_Radiotherapy_CVPR_2023_paper.pdf)
* [Towards Trustable Skin Cancer Diagnosis via Rewriting Model’s Decision](https://arxiv.org/pdf/2303.00885.pdf)
* [Hierarchical discriminative learning improves visual representations of biomedical microscopy](https://arxiv.org/abs/2303.01605)<br>:house:[project](https://hidisc.mlins.org/)
* [Topology-Guided Multi-Class Cell Context Generation for Digital Pathology](http://arxiv.org/abs/2304.02255v1)
* [Image Quality-aware Diagnosis via Meta-knowledge Co-embedding](http://arxiv.org/abs/2303.15038v1)
* [METransformer: Radiology Report Generation by Transformer with Multiple Learnable Expert Tokens](http://arxiv.org/abs/2304.02211v1)医学诊断
* 3D医学
  * [Geometric Visual Similarity Learning in 3D Medical Image Self-supervised Pre-training](https://arxiv.org/pdf/2303.00874.pdf)<br>:star:[code](https://github.com/YutingHe-list/GVSL)
* 图像配准
  * [Indescribable Multi-modal Spatial Evaluator](https://arxiv.org/pdf/2303.00369.pdf)<br>:star:[code](https://github.com/Kid-Liet/IMSE)
* 图像分类
  * [ask-specific Fine-tuning via Variational Information Bottleneck for Weakly-supervised Pathology Whole Slide Image Classification](https://arxiv.org/abs/2303.08446)<br>:star:[code](https://github.com/invoker-LL/WSI-finetuning)
  * [RankMix: Data Augmentation for Weakly Supervised Learning of Classifying Whole Slide Images With Diverse Sizes and Imbalanced Categories](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_RankMix_Data_Augmentation_for_Weakly_Supervised_Learning_of_Classifying_Whole_CVPR_2023_paper.pdf)
  * [Grounding Counterfactual Explanation of Image Classifiers to Textual Concept Space](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Grounding_Counterfactual_Explanation_of_Image_Classifiers_to_Textual_Concept_Space_CVPR_2023_paper.pdf)
  * [PEFAT: Boosting Semi-Supervised Medical Image Classification via Pseudo-Loss Estimation and Feature Adversarial Training](https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_PEFAT_Boosting_Semi-Supervised_Medical_Image_Classification_via_Pseudo-Loss_Estimation_and_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/maxwell0027/PEFAT)
  * [Task-Specific Fine-Tuning via Variational Information Bottleneck for Weakly-Supervised Pathology Whole Slide Image Classification](https://arxiv.org/abs/2303.08446)<br>:star:[code](https://github.com/invoker-LL/WSI-finetuning)
  * [A Loopback Network for Explainable Microvascular Invasion Classification](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_A_Loopback_Network_for_Explainable_Microvascular_Invasion_Classification_CVPR_2023_paper.pdf)
* 报告生成
  * [Dynamic Graph Enhanced Contrastive Learning for Chest X-ray Report Generation](https://arxiv.org/abs/2303.10323)<br>:star:[code](https://github.com/mlii0117/DCL)
  * [METransformer: Radiology Report Generation by Transformer With Multiple Learnable Expert Tokens](https://arxiv.org/abs/2304.02211)
    * [KiUT: Knowledge-Injected U-Transformer for Radiology Report Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_KiUT_Knowledge-Injected_U-Transformer_for_Radiology_Report_Generation_CVPR_2023_paper.pdf)
* 医学影像分割
  * [Orthogonal Annotation Benefits Barely-supervised Medical Image Segmentation](http://arxiv.org/abs/2303.13090v1)
  * [SDC-UDA: Volumetric Unsupervised Domain Adaptation Framework for Slice-Direction Continuous Cross-Modality Medical Image Segmentation](http://arxiv.org/abs/2305.11012v1)
  * [Pseudo-Label Guided Contrastive Learning for Semi-Supervised Medical Image Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Basak_Pseudo-Label_Guided_Contrastive_Learning_for_Semi-Supervised_Medical_Image_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/hritam-98/PatchCL-MedSeg)
  * [Fair Federated Medical Image Segmentation via Client Contribution Estimation](http://arxiv.org/abs/2303.16520v1)
  * [Bidirectional Copy-Paste for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2305.00673)<br>:star:[code](https://github.com/DeepMed-Lab-ECNU/BCP)
  * [Devil is in the Queries: Advancing Mask Transformers for Real-world Medical Image Segmentation and Out-of-Distribution Localization](http://arxiv.org/abs/2304.00212v1)
  * [Weakly supervised segmentation with point annotations for histopathology images via contrast-based variational model](http://arxiv.org/abs/2304.03572v1)
  * [MCF: Mutual Correction Framework for Semi-Supervised Medical Image Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MCF_Mutual_Correction_Framework_for_Semi-Supervised_Medical_Image_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/WYC-321/MCF)
  * [Rethinking Few-Shot Medical Segmentation: A Vector Quantization View](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Rethinking_Few-Shot_Medical_Segmentation_A_Vector_Quantization_View_CVPR_2023_paper.pdf)
  * [Devil Is in the Queries: Advancing Mask Transformers for Real-World Medical Image Segmentation and Out-of-Distribution Localization](https://arxiv.org/abs/2304.00212)
  * [MagicNet: Semi-Supervised Multi-Organ Segmentation via Magic-Cube Partition and Recovery](https://arxiv.org/abs/2212.14310)<br>:star:[code](https://github.com/DeepMed-Lab-ECNU/MagicNet)
  * [Ambiguous Medical Image Segmentation Using Diffusion Models](https://arxiv.org/abs/2304.04745)
  * [Directional Connectivity-Based Segmentation of Medical Images](http://arxiv.org/abs/2304.00145)
* 医学影像分析
  * [Best of Both Worlds: Multimodal Contrastive Learning with Tabular and Imaging Data](http://arxiv.org/abs/2303.14080v1)
  * [Directional Connectivity-based Segmentation of Medical Images](http://arxiv.org/abs/2304.00145v1)<br>:star:[code](https://github.com/Zyun-Y/DconnNet)
* 肿瘤分割
  * [Label-Free Liver Tumor Segmentation](http://arxiv.org/abs/2303.14869v1)
* 医学影像报告生成
  * [Interactive and Explainable Region-guided Radiology Report Generation](http://arxiv.org/abs/2304.08295v1)<br>:star:[code](https://github.com/ttanida/rgrg)自动生成放射学报告 
* 切片分析
  * [Histopathology Whole Slide Image Analysis With Heterogeneous Graph Representation Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/HKU-MedAI/WSI-HGNN)
* 细胞检测、跟踪与计数
  * [DeGPR: Deep Guided Posterior Regularization for Multi-Class Cell Detection and Counting](https://arxiv.org/abs/2304.00741)
  * [Overlapped Cell on Tissue Dataset for Histopathology](https://arxiv.org/abs/2303.13110)
  * [Unsupervised Contour Tracking of Live Cells by Mechanical and Cycle Consistency Losses](https://arxiv.org/abs/2303.08364)<br>:star:[code](https://github.com/JunbongJang/contour-tracking/)
  * [Masked Autoencoder Guided Segmentation at Pixel Resolution for Accurate, Self-Supervised Subcellular Structure Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_MAESTER_Masked_Autoencoder_Guided_Segmentation_at_Pixel_Resolution_for_Accurate_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/bowang-lab/MAESTER)
* 单目内窥镜跟踪
  * [Constrained Evolutionary Diffusion Filter for Monocular Endoscope Tracking](https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Constrained_Evolutionary_Diffusion_Filter_for_Monocular_Endoscope_Tracking_CVPR_2023_paper.pdf)
* 皮肤癌诊断
  * [Towards Trustable Skin Cancer Diagnosis via Rewriting Model's Decision](https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_Towards_Trustable_Skin_Cancer_Diagnosis_via_Rewriting_Models_Decision_CVPR_2023_paper.pdf)
* MRI 重建
  * [Learning Federated Visual Prompt in Null Space for MRI Reconstruction](https://arxiv.org/abs/2303.16181)

<a name="16"/>

## 16.Semi/self-supervised learning(半/自监督)
* 无监督学习
  * [Non-Contrastive Unsupervised Learning of Physiological Signals from Video](https://arxiv.org/abs/2303.07944)<br>:star:[code](https://github.com/CVRL/SiNC-rPPG)
  * [Neural Rate Estimator and Unsupervised Learning for Efficient Distributed Image Analytics in Split-DNN Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Ahuja_Neural_Rate_Estimator_and_Unsupervised_Learning_for_Efficient_Distributed_Image_CVPR_2023_paper.pdf)
  * [Non-Contrastive Unsupervised Learning of Physiological Signals From Video](https://arxiv.org/abs/2303.07944)
* 自监督
  * [Coreset Sampling from Open-Set for Fine-Grained Self-Supervised Learning](https://arxiv.org/abs/2303.11101)
  * [StepFormer: Self-Supervised Step Discovery and Localization in Instructional Videos](https://arxiv.org/abs/2304.13265)
  * [Self-Supervised Learning From Images With a Joint-Embedding Predictive Architecture](https://arxiv.org/abs/2301.08243)
  * [Defending Against Patch-Based Backdoor Attacks on Self-Supervised Learning](https://arxiv.org/abs/2304.01482)<br>:star:[code](https://github.com/UCDvision/PatchSearch)
  * [DrapeNet: Garment Generation and Self-Supervised Draping](https://arxiv.org/abs/2211.11277)<br>:star:[code](https://github.com/liren2515/DrapeNet)
  * [Neural Congealing: Aligning Images to a Joint Semantic Atlas](https://arxiv.org/abs/2302.03956)<br>:house:[project](https://neural-congealing.github.io/)
  * [Self-Supervised AutoFlow](https://arxiv.org/abs/2212.01762)
  * [Towards Realistic Long-Tailed Semi-Supervised Learning: Consistency Is All You Need](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Towards_Realistic_Long-Tailed_Semi-Supervised_Learning_Consistency_Is_All_You_Need_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Gank0078/ACR)
  * [Siamese Image Modeling for Self-Supervised Vision Representation Learning](https://arxiv.org/abs/2206.01204)<br>:star:[code](https://github.com/fundamentalvision/Siamese-Image-Modeling)
  * [SCOOP: Self-Supervised Correspondence and Optimization-Based Scene Flow](https://arxiv.org/abs/2211.14020)<br>:star:[code](https://github.com/itailang/SCOOP)
  * [Three Guidelines You Should Know for Universally Slimmable Self-Supervised Learning](https://arxiv.org/abs/2303.06870)<br>:star:[code](https://github.com/megvii-research/US3L-CVPR2023)
  * [Look, Radiate, and Learn: Self-Supervised Localisation via Radio-Visual Correspondence](https://arxiv.org/abs/2206.06424)
  * [Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss](https://arxiv.org/abs/2301.05709)
  * [Evolved Part Masking for Self-Supervised Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Evolved_Part_Masking_for_Self-Supervised_Learning_CVPR_2023_paper.pdf)
  * [Towards Professional Level Crowd Annotation of Expert Domain Data](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Towards_Professional_Level_Crowd_Annotation_of_Expert_Domain_Data_CVPR_2023_paper.pdf)
  * [ALSO: Automotive Lidar Self-Supervision by Occupancy Estimation](https://arxiv.org/abs/2212.05867)<br>:star:[code](https://github.com/valeoai/ALSO)
  * [Correlational Image Modeling for Self-Supervised Visual Pre-Training](http://arxiv.org/abs/2303.12670v1)
  * [Beyond Appearance: a Semantic Controllable Self-Supervised Learning Framework for Human-Centric Visual Tasks](http://arxiv.org/abs/2303.17602v1)<br>:star:[code](https://github.com/tinyvision/SOLIDER)<br>:thumbsup:[CVPR 2023 深挖无标签数据价值！自监督学习框架SOLIDER：用于以人为中心的视觉](https://mp.weixin.qq.com/s/XYewbXlp38MWSDJigrInmw)
  * [Mixed Autoencoder for Self-supervised Visual Representation Learning](http://arxiv.org/abs/2303.17152v1)
  * [Siamese DETR](http://arxiv.org/abs/2303.18144v1)<br>:star:[code](https://github.com/Zx55/SiameseDETR)
  * [Token Boosting for Robust Self-Supervised Visual Transformer Pre-training](https://eprints.lancs.ac.uk/id/eprint/189827/1/CVPR2023_Lingeng_Tianjiao_TBM.pdf)
* 半监督
  * [Boosting Semi-Supervised Learning by Exploiting All Unlabeled Data](https://arxiv.org/abs/2303.11066)<br>:star:[code](https://github.com/megvii-research/FullMatch)
  * [Out-of-Distributed Semantic Pruning for Robust Semi-Supervised Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Out-of-Distributed_Semantic_Pruning_for_Robust_Semi-Supervised_Learning_CVPR_2023_paper.pdf)
  * [DualRel: Semi-Supervised Mitochondria Segmentation From a Prototype Perspective](https://openaccess.thecvf.com/content/CVPR2023/papers/Mai_DualRel_Semi-Supervised_Mitochondria_Segmentation_From_a_Prototype_Perspective_CVPR_2023_paper.pdf)
  * [CHMATCH:Contrastive Hierarchical Matching and Robust Adaptive Threshold Boosted Semi-Supervised Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_CHMATCH_Contrastive_Hierarchical_Matching_and_Robust_Adaptive_Threshold_Boosted_Semi-Supervised_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/sailist/CHMatch)
  * [ProtoCon: Pseudo-label Refinement via Online Clustering and Prototypical Consistency for Efficient Semi-supervised Learning](http://arxiv.org/abs/2303.13556v1)
  * [Class Balanced Adaptive Pseudo Labeling for Federated Semi-Supervised Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Class_Balanced_Adaptive_Pseudo_Labeling_for_Federated_Semi-Supervised_Learning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/minglllli/)
  * [MarginMatch:Improving Semi-Supervised Learning with Pseudo-Margins](https://openaccess.thecvf.com/content/CVPR2023/papers/Sosea_MarginMatch_Improving_Semi-Supervised_Learning_with_Pseudo-Margins_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/tsosea%202/MarginMatch)
  * [Semi-Supervised Learning Made Simple With Self-Supervised Clustering](https://openaccess.thecvf.com/content/CVPR2023/papers/Fini_Semi-Supervised_Learning_Made_Simple_With_Self-Supervised_Clustering_CVPR_2023_paper.pdf)
* 弱监督
  * [Similarity Maps for Self-Training Weakly-Supervised Phrase Grounding](https://openaccess.thecvf.com/content/CVPR2023/papers/Shaharabany_Similarity_Maps_for_Self-Training_Weakly-Supervised_Phrase_Grounding_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/talshaharabany/Similarity-Maps-for-Self-Training-Weakly-Supervised-Phrase-Grounding)

<a name="15"/>

## 15.Vision Transformers
* [SVGformer: Representation Learning for Continuous Vector Graphics Using Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_SVGformer_Representation_Learning_for_Continuous_Vector_Graphics_Using_Transformers_CVPR_2023_paper.pdf)
* [Adversarial Normalization: I Can visualize Everything (ICE)](https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_Adversarial_Normalization_I_Can_Visualize_Everything_ICE_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Hanyang-HCC-Lab/ICE)
* [Hint-Aug: Drawing Hints From Foundation Vision Transformers Towards Boosted Few-Shot Parameter-Efficient Tuning](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Hint-Aug_Drawing_Hints_From_Foundation_Vision_Transformers_Towards_Boosted_Few-Shot_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/GATECH-EIC/Hint-Aug)
* [PanoSwin: A Pano-Style Swin Transformer for Panorama Understanding](https://openaccess.thecvf.com/content/CVPR2023/papers/Ling_PanoSwin_A_Pano-Style_Swin_Transformer_for_Panorama_Understanding_CVPR_2023_paper.pdf)
* [D2Former: Jointly Learning Hierarchical Detectors and Contextual Descriptors via Agent-Based Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/He_D2Former_Jointly_Learning_Hierarchical_Detectors_and_Contextual_Descriptors_via_Agent-Based_CVPR_2023_paper.pdf)
* [NAR-Former: Neural Architecture Representation Learning Towards Holistic Attributes Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_NAR-Former_Neural_Architecture_Representation_Learning_Towards_Holistic_Attributes_Prediction_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/yuny220/NAR-Former)
* [DropKey for Vision Transformer](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DropKey_for_Vision_Transformer_CVPR_2023_paper.pdf)
* [Integrally Pre-Trained Transformer Pyramid Networks](https://arxiv.org/abs/2211.12735)<br>:star:[code](https://github.com/sunsmarterjie/iTPN)
* [DSVT: Dynamic Sparse Voxel Transformer With Rotated Sets](http://arxiv.org/abs/2301.06051)
* [Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention](https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_Slide-Transformer_Hierarchical_Vision_Transformer_With_Local_Self-Attention_CVPR_2023_paper.pdf)
* [Trade-Off Between Robustness and Accuracy of Vision Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Trade-Off_Between_Robustness_and_Accuracy_of_Vision_Transformers_CVPR_2023_paper.pdf)
* [A Light Touch Approach to Teaching Transformers Multi-view Geometry](https://arxiv.org/abs/2211.15107)
* [Masked Jigsaw Puzzle: A Versatile Position Embedding for Vision Transformers](https://arxiv.org/abs/2205.12551)<br>:star:[code](https://github.com/yhlleo/MJP)
* [RGB no more: Minimally-decoded JPEG Vision Transformers](https://arxiv.org/abs/2211.16421)
* [Making Vision Transformers Efficient from A Token Sparsification View](https://arxiv.org/abs/2303.08685)<br>:star:[code](http://github.com/changsn/STViT-R)
* [Blur Interpolation Transformer for Real-World Motion from Blur](https://arxiv.org/abs/2211.11423)<br>:star:[code](https://github.com/zzh-tech/BiT)
* [Neighborhood Attention Transformer](https://arxiv.org/abs/2204.07143)<br>:star:[code](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer)
* [MixMAE: Mixed and Masked Autoencoder for Efficient Pretraining of Hierarchical Vision Transformers](https://arxiv.org/abs/2205.13137)<br>:star:[code](https://github.com/Sense-X/MixMIM)
* [Towards End-to-End Generative Modeling of Long Videos with Memory-Efficient Bidirectional Transformers](https://arxiv.org/abs/2303.11251)<br>:house:[project](https://sites.google.com/view/mebt-cvpr2023)
* [Improving Robustness of Vision Transformers by Reducing Sensitivity To Patch Corruptions](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Improving_Robustness_of_Vision_Transformers_by_Reducing_Sensitivity_To_Patch_CVPR_2023_paper.pdf)
* [Latency Matters: Real-Time Action Forecasting Transformer](https://openaccess.thecvf.com/content/CVPR2023/papers/Girase_Latency_Matters_Real-Time_Action_Forecasting_Transformer_CVPR_2023_paper.pdf)<br>:star:[code](https://karttikeya.github.io/publication/RAFTformer/)
* [OmniMAE: Single Model Masked Pretraining on Images and Videos](https://arxiv.org/abs/2206.08356)<br>:star:[code](https://github.com/facebookresearch/omnivore)
* [MAGVIT: Masked Generative Video Transformer](https://arxiv.org/abs/2212.05199)<br>:house:[project](https://magvit.cs.cmu.edu/)
* [Learning Imbalanced Data with Vision Transformers](https://arxiv.org/abs/2212.02015)<br>:star:[code](https://github.com/XuZhengzhuo/LiVT)
* [Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves](https://arxiv.org/pdf/2303.01112.pdf)<br>:house:[project](https://masora1030.github.io/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves/)
* [AMIGO: Sparse Multi-Modal Graph Transformer with Shared-Context Processing for Representation Learning of Giga-pixel Images](https://arxiv.org/pdf/2303.00865.pdf)<br>:star:[code](https://github.com/raminnakhli/AMIGO)
* [Generic-to-Specific Distillation of Masked Autoencoders](https://arxiv.org/pdf/2302.14771.pdf)<br>:star:[code](https://github.com/pengzhiliang/G2SD)
* [BiFormer: Vision Transformer with Bi-Level Routing Attention](https://arxiv.org/abs/2303.08810)<br>:star:[code](https://github.com/rayleizhu/BiFormer)
* [Making Vision Transformers Efficient from A Token Sparsification View](https://arxiv.org/abs/2303.08685)
* [Dual-path Adaptation from Image to Video Transformers](https://arxiv.org/abs/2303.09857)<br>:star:[code](https://github.com/park-jungin/DualPath)
* [Spherical Transformer for LiDAR-based 3D Recognition](http://arxiv.org/abs/2303.12766v1)<br>:star:[code](https://github.com/dvlab-research/SphereFormer.git)
* [MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models](http://arxiv.org/abs/2303.13009v1)<br>:star:[code](https://github.com/mlvlab/MELTR)
* [Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient Vision Transformers](http://arxiv.org/abs/2303.13755v1)
* [Learning Expressive Prompting With Residuals for Vision Transformers](http://arxiv.org/abs/2303.15591v1)
* [SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer](http://arxiv.org/abs/2303.17605v1)<br>:house:[project](https://sparsevit.mit.edu)
* [Visual Dependency Transformers: Dependency Tree Emerges from Reversed Attention](http://arxiv.org/abs/2304.03282v1)Transformer
* [Token Boosting for Robust Self-Supervised Visual Transformer Pre-training](http://arxiv.org/abs/2304.04175v1)Transformer
* [Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention](http://arxiv.org/abs/2304.04237v1)<br>:star:[code](https://github.com/LeapLabTHU/Slide-Transformer)
* [RIFormer: Keep Your Vision Backbone Effective While Removing Token Mixer](http://arxiv.org/abs/2304.05659v1)<br>:star:[code](https://techmonsterwang.github.io/RIFormer/)
* [DropKey](https://arxiv.org/abs/2208.02646)<br>:thumbsup:[CVPR 2023｜两行代码高效缓解视觉Transformer过拟合，美图&国科大联合提出正则化方法DropKey](https://mp.weixin.qq.com/s/rmr40kryYPILwEinzZOjUA)
* [Joint Token Pruning and Squeezing Towards More Aggressive Compression of Vision Transformers](http://arxiv.org/abs/2304.10716v1)<br>:star:[code](https://github.com/megvii-research/TPS-CVPR2023)
* [EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention](http://arxiv.org/abs/2305.07027v1)<br>:star:[code](https://github.com/microsoft/Cream/tree/main/EfficientViT)
* [TrojViT: Trojan Insertion in Vision Transformers](https://arxiv.org/abs/2208.13049)
* [Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference](https://openaccess.thecvf.com/content/CVPR2023/papers/You_Castling-ViT_Compressing_Self-Attention_via_Switching_Towards_Linear-Angular_Attention_at_Vision_CVPR_2023_paper.pdf)<br>:house:[project](https://www.haoranyou.com/castling-vit)
* [ResFormer: Scaling ViTs with Multi-Resolution Training](https://arxiv.org/abs/2212.00776)<br>:star:[code](https://github.com/ruitian12/resformer)
* [Vision Transformer With Super Token Sampling](https://arxiv.org/abs/2211.11167)<br>:star:[code](https://github.com/hhb072/SViT)
* [Vision Transformers Are Good Mask Auto-Labelers](https://arxiv.org/abs/2301.03992)

<a name="14"/>

## 14.Video
* [Modular Memorability: Tiered Representations for Video Memorability Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Dumont_Modular_Memorability_Tiered_Representations_for_Video_Memorability_Prediction_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/tekal-ai/modular-memorability)
* [Language-Guided Music Recommendation for Video via Prompt Analogies](https://openaccess.thecvf.com/content/CVPR2023/papers/McKee_Language-Guided_Music_Recommendation_for_Video_via_Prompt_Analogies_CVPR_2023_paper.pdf)
* [Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning](https://arxiv.org/abs/2212.03229)
* [1000 FPS HDR Video With a Spike-RGB Hybrid Camera](https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_1000_FPS_HDR_Video_With_a_Spike-RGB_Hybrid_Camera_CVPR_2023_paper.pdf)<br>:house:[project](https://changyakun.github.io/1000FPS-HDR)
* [Egocentric Video Task Translatio](https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_Egocentric_Video_Task_Translation_CVPR_2023_paper.pdf)<br>:house:[project](https://vision.cs.utexas.edu/projects/egot2/)
* [Relational Space-Time Query in Long-Form Videos](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Relational_Space-Time_Query_in_Long-Form_Videos_CVPR_2023_paper.pdf)
* [Spatial-Then-Temporal Self-Supervised Learning for Video Correspondence](https://arxiv.org/abs/2209.07778)<br>:star:[code](https://github.com/qianduoduolr/Spa-then-Temp)
* [Few-Shot Referring Relationships in Videos](https://openaccess.thecvf.com/content/CVPR2023/papers/Kumar_Few-Shot_Referring_Relationships_in_Videos_CVPR_2023_paper.pdf)<br>:house:[project](https://vl2g.github.io/projects/refRelations/)
* [Aligning Step-by-Step Instructional Diagrams to Video Demonstrations](https://arxiv.org/abs/2303.13800)<br>:house:[project](https://academic.davidz.cn/en/publication/zhang-cvpr-2023/)
* [3D Video Loops From Asynchronous Input](https://arxiv.org/abs/2303.05312)<br>:house:[project](https://limacv.github.io/VideoLoop3D_web/)
* [VideoMAE V2: Scaling Video Masked Autoencoders With Dual Masking](https://arxiv.org/abs/2303.16727)<br>:star:[code](https://github.com/OpenGVLab/VideoMAEv2)
* [Weakly Supervised Video Representation Learning with Unaligned Text for Sequential Videos](http://arxiv.org/abs/2303.12370v1)<br>:star:[code](https://github.com/svip-lab/WeakSVR)
* [StepFormer: Self-supervised Step Discovery and Localization in Instructional Videos](http://arxiv.org/abs/2304.13265v1)
* [VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking](http://arxiv.org/abs/2303.16727v1)
* [Implicit View-Time Interpolation of Stereo Videos using Multi-Plane Disparities and Non-Uniform Coordinates](http://arxiv.org/abs/2303.17181v1)<br>:house:[project](https://people.engr.tamu.edu/nimak/Papers/CVPR23StereoVideo/index.html)<br>:tv:[video](https://www.youtube.com/watch?v=XJa_bf8OCrc)
* [How You Feelin'? Learning Emotions and Mental States in Movie Scenes](https://arxiv.org/abs/2304.05634)<br>:house:[project](https://katha-ai.github.io/projects/emotx/)
* 视频时刻检索
  * [Towards Generalisable Video Moment Retrieval:Visual-Dynamic Injection to Image-Text Pre-Training](https://arxiv.org/pdf/2303.00040.pdf)
  * [Query-Dependent Video Representation for Moment Retrieval and Highlight Detection](http://arxiv.org/abs/2303.13874v1)<br>:star:[code](https://github.com/wjun0830/QD-DETR)
  * [Hierarchical Video-Moment Retrieval and Step-Captioning](http://arxiv.org/abs/2303.16406v1)<br>:star:[code](https://hirest-cvpr2023.github.io)
* 视频高亮检测
  * [Collaborative Noisy Label Cleaner: Learning Scene-aware Trailers for Multi-modal Highlight Detection in Movies](http://arxiv.org/abs/2303.14768v1)<br>:star:[code](https://github.com/TencentYoutuResearch/HighlightDetection-CLC)
* 视频帧插值
  * [Extracting Motion and Appearance via Inter-Frame Attention for Efficient Video Frame Interpolation](https://arxiv.org/pdf/2303.00440.pdf)<br>:star:[code](https://github.com/MCG-NJU/EMA-VFI)
  * [AMT: All-Pairs Multi-Field Transforms for Efficient Frame Interpolation](http://arxiv.org/abs/2304.09790v1)<br>:star:[code](https://github.com/MCG-NKU/AMT)
  * [Exploring Motion Ambiguity and Alignment for High-Quality Video Frame Interpolation](https://arxiv.org/abs/2203.10291)
  * [Exploring Discontinuity for Video Frame Interpolation](https://arxiv.org/abs/2202.07291)
  * [Event-Based Video Frame Interpolation With Cross-Modal Asymmetric Bidirectional Motion Fields](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Event-Based_Video_Frame_Interpolation_With_Cross-Modal_Asymmetric_Bidirectional_Motion_Fields_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/intelpro/CBMNet)
  * [A Unified Pyramid Recurrent Network for Video Frame Interpolation](https://arxiv.org/abs/2211.03456)<br>:star:[code](https://github.com/srcn-ivl/UPR-Net)
  * [Joint Video Multi-Frame Interpolation and Deblurring under Unknown Exposure Time](http://arxiv.org/abs/2303.15043v1)<br>:star:[code](https://github.com/shangwei5/VIDUE)
  * [BiFormer: Learning Bilateral Motion Estimation via Bilateral Transformer for 4K Video Frame Interpolation](http://arxiv.org/abs/2304.02225v1)<br>:star:[code](https://github.com/JunHeum/BiFormer)视频帧插值
  * [Frame Interpolation Transformer and Uncertainty Guidance](https://openaccess.thecvf.com/content/CVPR2023/papers/Plack_Frame_Interpolation_Transformer_and_Uncertainty_Guidance_CVPR_2023_paper.pdf)
* 视频合成
  * [Decomposed Diffusion Models for High-Quality Video Generation](https://arxiv.org/abs/2303.08320)
  * [MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Ruan_MM-Diffusion_Learning_Multi-Modal_Diffusion_Models_for_Joint_Audio_and_Video_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/researchmm/MM-Diffusion)
  * [Align Your Latents: High-Resolution Video Synthesis With Latent Diffusion Models](https://arxiv.org/abs/2304.08818)<br>:house:[project](https://research.nvidia.com/labs/toronto-ai/VideoLDM/)
* 视频预测
  * [MOSO: Decomposing MOtion, Scene and Object for Video Prediction](https://arxiv.org/abs/2303.03684)<br>:star:[code](https://github.com/anonymous202203/MOSO)
  * [A Dynamic Multi-Scale Voxel Flow Network for Video Prediction](https://arxiv.org/abs/2303.09875)<br>:star:[code](https://huxiaotaostasy.github.io/DMVFN/)
* 视频理解
  * [Selective Structured State-Spaces for Long-Form Video Understanding](http://arxiv.org/abs/2303.14526v1)
  * [How you feelin'? Learning Emotions and Mental States in Movie Scenes](http://arxiv.org/abs/2304.05634v1)<br>:star:[code](https://katha-ai.github.io/projects/emotx/)
  * [System-status-aware Adaptive Network for Online Streaming Video Understanding](http://arxiv.org/abs/2303.15742v1)
  * [LAVENDER: Unifying Video-Language Understanding As Masked Language Modeling](https://arxiv.org/abs/2206.07160)<br>:star:[code](https://github.com/microsoft/LAVENDER)
  * [System-Status-Aware Adaptive Network for Online Streaming Video Understanding](https://arxiv.org/abs/2303.15742)
  * [Therbligs in Action: Video Understanding Through Motion Primitives](https://openaccess.thecvf.com/content/CVPR2023/papers/Dessalene_Therbligs_in_Action_Video_Understanding_Through_Motion_Primitives_CVPR_2023_paper.pdf)
  * [Streaming Video Model](http://arxiv.org/abs/2303.17228v1)<br>:star:[code](https://github.com/yuzhms/Streaming-Video-Model)
  * [Procedure-Aware Pretraining for Instructional Video Understanding](http://arxiv.org/abs/2303.18230v1)<br>:star:[code](https://github.com/salesforce/paprika)
  * [Learning Procedure-aware Video Representation from Instructional Videos and Their Narrations](http://arxiv.org/abs/2303.17839v1)<br>:star:[code](https://github.com/facebookresearch/ProcedureVRL)
  * [Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring](https://arxiv.org/pdf/2301.11116.pdf)<br>:star:[code](https://github.com/farewellthree/STAN)
* 视频分类
  * [Vita-CLIP: Video and text adaptive CLIP via Multimodal Prompting](http://arxiv.org/abs/2304.03307v1)<br>:star:[code](https://github.com/TalalWasim/Vita-CLIP)
  * [Class Prototypes Based Contrastive Learning for Classifying Multi-Label and Fine-Grained Educational Videos](https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Class_Prototypes_Based_Contrastive_Learning_for_Classifying_Multi-Label_and_Fine-Grained_CVPR_2023_paper.pdf)<br>:house:[project](https://nusci.csl.sri.com/project/APPROVE)
* 视频描述
  * [Fine-grained Audible Video Description](http://arxiv.org/abs/2303.15616v1)
* 视频摘要
  * [Align and Attend: Multimodal Summarization with Dual Contrastive Losses](https://arxiv.org/abs/2303.07284)<br>:star:[code](https://boheumd.github.io/A2Summ/)
* 视频识别
  * [Frame Flexible Network](http://arxiv.org/abs/2303.14817v1)<br>:star:[code](https://github.com/BeSpontaneous/FFN)<br>:thumbsup:[CVPR-2023 | FFN: 针对视频识别的通用Once-For-All框架](https://mp.weixin.qq.com/s/-h0P7_mcxlqwTt5uWgeiMg)
  * [Use Your Head: Improving Long-Tail Video Recognition](http://arxiv.org/abs/2304.01143v1)
* Video Deflickering(去闪烁)
  * [Blind Video Deflickering by Neural Filtering with a Flawed Atlas](https://arxiv.org/abs/2303.08120)<br>:star:[code](http://github.com/ChenyangLEI/All-In-One-Deflicker)
* 时间句子定位(TSG)
  * [You Can Ground Earlier than See: An Effective and Efficient Pipeline for Temporal Sentence Grounding in Compressed Videos](https://arxiv.org/abs/2303.07863)
  * [Weakly Supervised Temporal Sentence Grounding With Uncertainty-Guided Self-Training](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Weakly_Supervised_Temporal_Sentence_Grounding_With_Uncertainty-Guided_Self-Training_CVPR_2023_paper.pdf)
* VAD
  * [Hierarchical Semantic Contrast for Scene-aware Video Anomaly Detection](http://arxiv.org/abs/2303.13051v1)
  * [Video Event Restoration Based on Keyframes for Video Anomaly Detection](http://arxiv.org/abs/2304.05112v1)
  * [Generating Anomalies for Video Anomaly Detection With Prompt-Based Feature Mapping](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Generating_Anomalies_for_Video_Anomaly_Detection_With_Prompt-Based_Feature_Mapping_CVPR_2023_paper.pdf) 
  * [Unbiased Multiple Instance Learning for Weakly Supervised Video Anomaly Detection](http://arxiv.org/abs/2303.12369)
  * [Exploiting Completeness and Uncertainty of Pseudo Labels for Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2212.04090)
  * [Look Around for Anomalies: Weakly-Supervised Anomaly Detection via Context-Motion Relational Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_Look_Around_for_Anomalies_Weakly-Supervised_Anomaly_Detection_via_Context-Motion_Relational_CVPR_2023_paper.pdf)
* 视频异常定位
  * [EVAL: Explainable Video Anomaly Localization](https://arxiv.org/abs/2212.07900)
* 视频镜像检测
  * [Learning To Detect Mirrors From Videos via Dual Correspondences](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Learning_To_Detect_Mirrors_From_Videos_via_Dual_Correspondences_CVPR_2023_paper.pdf)<br>:house:[project](https://jiaying.link/cvpr2023-vmd/)
  * 视频表示学习
    * [Weakly Supervised Video Representation Learning With Unaligned Text for Sequential Videos](https://arxiv.org/abs/2303.12370)<br>:star:[code](https://github.com/svip-lab/WeakSVR)
    * [Learning Procedure-Aware Video Representation From Instructional Videos and Their Narrations](https://arxiv.org/abs/2303.17839)<br>:star:[code](https://github.com/facebookresearch/ProcedureVRL)
    * [Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised Video Representation Learning](https://arxiv.org/abs/2212.04500)<br>:star:[code](https://github.com/ruiwang2021/mvd)
    * [Modeling Video As Stochastic Processes for Fine-Grained Video Representation Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Modeling_Video_As_Stochastic_Processes_for_Fine-Grained_Video_Representation_Learning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/hengRUC/VSP)
* Video Paragraph Grounding
  * [Hierarchical Semantic Correspondence Networks for Video Paragraph Grounding](https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Hierarchical_Semantic_Correspondence_Networks_for_Video_Paragraph_Grounding_CVPR_2023_paper.pdf)
* Video Grounding
  * [Text-Visual Prompting for Efficient 2D Temporal Video Grounding](https://arxiv.org/abs/2303.04995)
  * [WINNER: Weakly-Supervised hIerarchical decompositioN and aligNment for Spatio-tEmporal Video gRounding](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_WINNER_Weakly-Supervised_hIerarchical_decompositioN_and_aligNment_for_Spatio-tEmporal_Video_gRounding_CVPR_2023_paper.pdf)
  * [Iterative Proposal Refinement for Weakly-Supervised Video Grounding](https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Iterative_Proposal_Refinement_for_Weakly-Supervised_Video_Grounding_CVPR_2023_paper.pdf)
  * [Collaborative Static and Dynamic Vision-Language Streams for Spatio-Temporal Video Grounding](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Collaborative_Static_and_Dynamic_Vision-Language_Streams_for_Spatio-Temporal_Video_Grounding_CVPR_2023_paper.pdf)
  * [ProTeGe: Untrimmed Pretraining for Video Temporal Grounding by Video Temporal Grounding](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_ProTeGe_Untrimmed_Pretraining_for_Video_Temporal_Grounding_by_Video_Temporal_CVPR_2023_paper.pdf)
* 视频阴影检测
  * [A Transformer Video Shadow Detection Framework](https://arxiv.org/abs/2211.06885)<br>:house:[project](https://lihaoliu-cambridge.github.io/scotch_and_soda/)
* 视频关键点检测
  * [Recurrence Without Recurrence: Stable Video Landmark Detection With Deep Equilibrium Models](https://arxiv.org/abs/2304.00600)

<a name="13"/>

## 13.GAN
* [Spider GAN: Leveraging Friendly Neighbors To Accelerate GAN Training](http://arxiv.org/abs/2305.07613)
* [Learning on Gradients: Generalized Artifacts Representation for GAN-Generated Images Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Learning_on_Gradients_Generalized_Artifacts_Representation_for_GAN-Generated_Images_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/chuangchuangtan/LGrad)
* [MoStGAN-V: Video Generation With Temporal Motion Styles](https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_MoStGAN-V_Video_Generation_With_Temporal_Motion_Styles_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/xiaoqian-shen/MoStGAN-V)
* [Sequential Training of GANs Against GAN-Classifiers Reveals Correlated "Knowledge Gaps" Present Among Independently Trained GAN Instances](https://arxiv.org/abs/2303.15533)
* [Re-GAN: Data-Efficient GANs Training via Architectural Reconfiguration](https://openaccess.thecvf.com/content/CVPR2023/papers/Saxena_Re-GAN_Data-Efficient_GANs_Training_via_Architectural_Reconfiguration_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/IntellicentAI-Lab/Re-GAN)
* [HumanGen: Generating Human Radiance Fields With Explicit Priors](https://arxiv.org/abs/2212.05321)
* [Bi-Directional Feature Fusion Generative Adversarial Network for Ultra-High Resolution Pathological Image Virtual Re-Staining](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Bi-Directional_Feature_Fusion_Generative_Adversarial_Network_for_Ultra-High_Resolution_Pathological_CVPR_2023_paper.pdf)
* [GlassesGAN: Eyewear Personalization Using Synthetic Appearance Discovery and Targeted Subspace Modeling](https://arxiv.org/abs/2210.14145)
* [Delving StyleGAN Inversion for Image Editing: A Foundation Latent Space Viewpoint](https://arxiv.org/abs/2211.11448)<br>:house:[project](https://kumapowerliu.github.io/CLCAE)
* [3DAvatarGAN: Bridging Domains for Personalized Editable Avatars](https://arxiv.org/abs/2301.02700)<br>:house:[project](https://rameenabdal.github.io/3DAvatarGAN/)
* [GLeaD: Improving GANs With a Generator-Leading Task](https://arxiv.org/abs/2212.03752)<br>:house:[project](https://ezioby.github.io/glead/)
* [Transforming the Residuals for Real Image Editing With StyleGAN](https://arxiv.org/abs/2212.14359)<br>:star:[code](https://github.com/hamzapehlivan/StyleRes)
* [Improving GAN Training via Feature Space Shrinkage](https://arxiv.org/abs/2303.01559)<br>:star:[code](https://github.com/WentianZhang-ML/AdaptiveMix)
* [Spider GAN: Leveraging Friendly Neighbors to Accelerate GAN Training](http://arxiv.org/abs/2305.07613v1)
* [NoisyTwins: Class-Consistent and Diverse Image Generation through StyleGANs](http://arxiv.org/abs/2304.05866v1)<br>:star:[code](https://rangwani-harsh.github.io/NoisyTwins/)
* [Graph Transformer GANs for Graph-Constrained House Generation](https://arxiv.org/abs/2303.08225)
* [Cross-GAN Auditing: Unsupervised Identification of Attribute Level Similarities and Differences between Pretrained Generative Models](https://arxiv.org/abs/2303.10774)<br>:star:[code](https://github.com/mattolson93/cross_gan_auditing)
* [Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis](http://arxiv.org/abs/2303.14157v1)<br>:star:[code](https://github.com/VinAIResearch/CREPS)
* [VIVE3D: Viewpoint-Independent Video Editing using 3D-Aware GANs](http://arxiv.org/abs/2303.15893v1)<br>:star:[code](http://afruehstueck.github.io/vive3D)
* [Discriminator-Cooperated Feature Map Distillation for GAN Compression](https://arxiv.org/abs/2212.14169)<br>:star:[code](https://github.com/poopit/DCD-official)
* [Lift3D: Synthesize 3D Training Data by Lifting 2D GAN to 3D Generative Radiance Field](http://arxiv.org/abs/2304.03526v1)<br>:star:[code](https://len-li.github.io/lift3d-web)
* 图像-文本合成
  * [Scaling up GANs for Text-to-Image Synthesis](https://arxiv.org/abs/2303.05511)<br>:house:[project](https://mingukkang.github.io/GigaGAN/)
* 扩散模型
  * [How to Backdoor Diffusion Models?](https://arxiv.org/abs/2212.05400)<br>:star:[code](https://github.com/IBM/BadDiffusion)
  * [ObjectStitch: Object Compositing With Diffusion Model](https://openaccess.thecvf.com/content/CVPR2023/papers/Song_ObjectStitch_Object_Compositing_With_Diffusion_Model_CVPR_2023_paper.pdf)
  * [Solving 3D Inverse Problems Using Pre-Trained 2D Diffusion Models](https://arxiv.org/abs/2211.10655)
  * [Parallel Diffusion Models of Operator and Image for Blind Inverse Problems](https://arxiv.org/abs/2211.10656)
  * [RGBD2: Generative Scene Synthesis via Incremental View Inpainting using RGBD Diffusion Models](https://arxiv.org/abs/2212.05993)<br>:house:[project](https://jblei.site/proj/rgbd-diffusion)
  * [Dimensionality-Varying Diffusion Process](https://arxiv.org/abs/2211.16032)
  * [TrojDiff: Trojan Attacks on Diffusion Models With Diverse Targets](https://arxiv.org/abs/2303.05762)<br>:star:[code](https://github.com/chenweixin107/TrojDiff)
  * [Towards Practical Plug-and-Play Diffusion Models](https://arxiv.org/abs/2212.05973)<br>:star:[code](https://github.com/riiid/PPAP)
  * [All Are Worth Words: A ViT Backbone for Diffusion Models](https://arxiv.org/abs/2209.12152)
  * [Unite and Conquer: Plug & Play Multi-Modal Synthesis using Diffusion Models](https://arxiv.org/abs/2212.00793)<br>:house:[project](https://nithin-gk.github.io/projectpages/Multidiff/index.html)
  * [Binary Latent Diffusion](https://arxiv.org/abs/2304.04820)
  * [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105)
  * [Lookahead Diffusion Probabilistic Models for Refining Mean Estimation](https://arxiv.org/abs/2304.11312)
  * [EDICT: Exact Diffusion Inversion via Coupled Transformations](https://arxiv.org/abs/2211.12446)<br>:star:[code](https://github.com/salesforce/EDICT)
  * [ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model With Knowledge-Enhanced Mixture-of-Denoising-Experts](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_ERNIE-ViLG_2.0_Improving_Text-to-Image_Diffusion_Model_With_Knowledge-Enhanced_Mixture-of-Denoising-Experts_CVPR_2023_paper.pdf)
* GAN 逆映射
  * [3D GAN Inversion With Facial Symmetry Prior](https://arxiv.org/abs/2211.16927)<br>:house:[project](https://feiiyin.github.io/SPI/)
  * [NeRFInvertor: High Fidelity NeRF-GAN Inversion for Single-Shot Real Image Animation](https://arxiv.org/abs/2211.17235)
  * [Self-Supervised Geometry-Aware Encoder for Style-Based 3D GAN Inversion](https://arxiv.org/abs/2212.07409)<br>:house:[project](https://nirvanalan.github.io/projects/E3DGE/index.html)
  * [High-Fidelity 3D GAN Inversion by Pseudo-Multi-View Optimization](https://arxiv.org/abs/2211.15662)<br>:house:[project](https://ken-ouyang.github.io/HFGI3D/index.html)

<a name="12"/>

## 12.Image-to-Image Translation(图像到图像翻译)
* [Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation](http://arxiv.org/abs/2211.12572)
* [DSI2I: Dense Style for Unpaired Image-to-Image Translation](https://arxiv.org/abs/2212.13253)
* [Fix the Noise: Disentangling Source Feature for Controllable Domain Translation](https://arxiv.org/abs/2303.11545)<br>:star:[code](https://github.com/LeeDongYeun/FixNoise)
* [3D-Aware Multi-Class Image-to-Image Translation with NeRFs](http://arxiv.org/abs/2303.15012v1)  
* [LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data](https://arxiv.org/abs/2208.14889)<br>:house:[project](https://ku-cvlab.github.io/LANIT/)
* [Unpaired Image-to-Image Translation With Shortest Path Regularization](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Unpaired_Image-to-Image_Translation_With_Shortest_Path_Regularization_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Mid-Push/santa)
* [BBDM: Image-to-Image Translation With Brownian Bridge Diffusion Models](https://arxiv.org/abs/2205.07680)
* 图像翻译    
  * [Masked and Adaptive Transformer for Exemplar Based Image Translation](http://arxiv.org/abs/2303.17123v1)<br>:star:[code](https://github.com/AiArt-HDU/MATEBIT)
* 视频翻译
  * [On the Difficulty of Unpaired Infrared-to-Visible Video Translation: Fine-Grained Content-Rich Patches Transfer](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_On_the_Difficulty_of_Unpaired_Infrared-to-Visible_Video_Translation_Fine-Grained_Content-Rich_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/BIT-DA/I2V-Processing) 
  
<a name="11"/>

## 11.Face(人脸)
* [Learning Detailed Radiance Manifolds for High-Fidelity and 3D-Consistent Portrait Synthesis From Monocular Image](https://arxiv.org/abs/2211.13901)<br>:house:[project](https://yudeng.github.io/GRAMInverter/)
* [Learning Neural Proto-Face Field for Disentangled 3D Face Modeling in the Wild](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_Neural_Proto-Face_Field_for_Disentangled_3D_Face_Modeling_in_CVPR_2023_paper.pdf)
* [Evading Forensic Classifiers With Attribute-Conditioned Adversarial Faces](https://openaccess.thecvf.com/content/CVPR2023/papers/Shamshad_Evading_Forensic_Classifiers_With_Attribute-Conditioned_Adversarial_Faces_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/koushiksrivats/face_attribute_attack)
* [Improving Fairness in Facial Albedo Estimation via Visual-Textual Cues](https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Improving_Fairness_in_Facial_Albedo_Estimation_via_Visual-Textual_Cues_CVPR_2023_paper.pdf)
* [Attribute-Preserving Face Dataset Anonymization via Latent Code Optimization](https://arxiv.org/abs/2303.11296)<br>:star:[code](https://github.com/chi0tzp/FALCO)
* [Pose-Disentangled Contrastive Learning for Self-Supervised Facial Representation](https://arxiv.org/abs/2211.13490)<br>:star:[code](https://github.com/DreamMr/PCL)
* [Privacy-Preserving Adversarial Facial Features](https://arxiv.org/abs/2305.05391)
* [BioNet: A Biologically-Inspired Network for Face Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_BioNet_A_Biologically-Inspired_Network_for_Face_Recognition_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/pengyuLPY/BioNet.git)
* [High-Res Facial Appearance Capture From Polarized Smartphone Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Azinovic_High-Res_Facial_Appearance_Capture_From_Polarized_Smartphone_Images_CVPR_2023_paper.pdf)
* [MARLIN: Masked Autoencoder for Facial Video Representation LearnINg](https://arxiv.org/abs/2211.06627)<br>:star:[code](https://github.com/ControlNet/MARLIN)
* [Sibling-Attack: Rethinking Transferable Adversarial Attacks against Face Recognition](http://arxiv.org/abs/2303.12512v1)<br>:house:[project](https://carlyx.github.io/DPE/)
* [Disentanglement of Pose and Expression for General Video Portrait Editing](https://arxiv.org/abs/2301.06281)
* [BlendFields: Few-Shot Example-Driven Facial Modeling](http://arxiv.org/abs/2305.07514v1)<br>:star:[code](https://blendfields.github.io/)
* [Towards Effective Adversarial Textured 3D Meshes on Physical Face Recognition](http://arxiv.org/abs/2303.15818v1)<br>:thumbsup:[CVPR 2023 | 人脸识别路漫漫：清华、北大等提出AT3D人脸识别系统攻击方法](https://zhuanlan.zhihu.com/p/618189634)
* [Collaborative Diffusion for Multi-Modal Face Generation and Editing](http://arxiv.org/abs/2304.10530v1)<br>:star:[code](https://ziqihuangg.github.io/projects/collaborative-diffusion.html)<br>:star:[code](https://github.com/ziqihuangg/Collaborative-Diffusion)<br>:thumbsup:[CVPR 2023 | Collaborative Diffusion 怎样让不同的扩散模型合作？](https://mp.weixin.qq.com/s/yhxcKlj6m-M6Hw7X31-JYQ)
* [Recognizability Embedding Enhancement for Very Low-Resolution Face Recognition and Quality Estimation](http://arxiv.org/abs/2304.10066v1)
* [DiffusionRig: Learning Personalized Priors for Facial Appearance Editing](http://arxiv.org/abs/2304.06711v1)<br>:star:[code](https://diffusionrig.github.io)
* [Probabilistic Knowledge Distillation of Face Ensembles](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Probabilistic_Knowledge_Distillation_of_Face_Ensembles_CVPR_2023_paper.pdf)
* [DCFace: Synthetic Face Generation with Dual Condition Diffusion Model](http://arxiv.org/abs/2304.07060v1)<br>:star:[code](https://github.com/mk-minchul/dcface)
* [Discrete Point-Wise Attack Is Not Enough: Generalized Manifold Adversarial Attack for Face Recognition](https://arxiv.org/abs/2301.06083)
* 3D 人脸
  * [Graphics Capsule: Learning Hierarchical 3D Face Representations from 2D Images](https://arxiv.org/abs/2303.10896)
  * [Physical-World Optical Adversarial Attacks on 3D Face Recognition](https://arxiv.org/abs/2205.13412)
  * [Learning a 3D Morphable Face Reflectance Model from Low-cost Data](https://arxiv.org/abs/2303.11686)<br>:house:[project](https://yxuhan.github.io/ReflectanceMM/index.html)
  * [NeuFace: Realistic 3D Neural Face Rendering from Multi-view Images](http://arxiv.org/abs/2303.14092v1)<br>:star:[code](https://github.com/aejion/NeuFace)
  * [FaceLit: Neural 3D Relightable Faces](http://arxiv.org/abs/2303.15437v1)
  * [Learning Neural Proto-face Field for Disentangled 3D Face Modeling In the Wild](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_Neural_Proto-Face_Field_for_Disentangled_3D_Face_Modeling_in_CVPR_2023_paper.pdf)
  * [High-Fidelity 3D Face Generation From Natural Language Descriptions](https://arxiv.org/abs/2305.03302)<br>:star:[code](https://github.com/zhuhao-nju/describe3d)
  * [CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior](https://arxiv.org/abs/2301.02379)<br>:star:[code](https://github.com/Doubiiu/CodeTalker)
* 人脸重建
  * [A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images](https://arxiv.org/pdf/2302.14434.pdf)<br>:house:[project](https://younglbw.github.io/HRN-homepage/)
  * [Graphics Capsule: Learning Hierarchical 3D Face Representations From 2D Images](https://arxiv.org/abs/2303.10896)
  * [Robust Model-Based Face Reconstruction Through Weakly-Supervised Outlier Segmentation](https://arxiv.org/abs/2106.09614)
  * [AVFace: Towards Detailed Audio-Visual 4D Face Reconstruction](https://arxiv.org/abs/2304.13115)<br>:house:[project](https://aggelinacha.github.io/AVFace/)
* 人脸恢复
  * [DR2: Diffusion-based Robust Degradation Remover for Blind Face Restoration](https://arxiv.org/abs/2303.06885)
* 人脸对齐
  * [* [DSFNet: Dual Space Fusion Network for Occlusion-Robust 3D Dense Face Alignment](http://arxiv.org/abs/2305.11522v1)<br>:star:[code](https://github.com/lhyfst/DSFNet)]
* 人脸匿名化
  * [Attribute-preserving Face Dataset Anonymization via Latent Code Optimization](https://arxiv.org/abs/2303.11296)<br>:star:[code](https://github.com/chi0tzp/FALCO)
* 人脸超分辨率
  * [Spatial-Frequency Mutual Learning for Face Super-Resolution](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Spatial-Frequency_Mutual_Learning_for_Face_Super-Resolution_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/wcy-cs/SFMNet)
* 裸眼年龄识别
  * [DAA: A Delta Age AdaIN operation for age estimation via binary code transformer](https://arxiv.org/abs/2303.07929)
* 情绪识别
  * [Context De-confounded Emotion Recognition](https://arxiv.org/abs/2303.11921)
  * [Decoupled Multimodal Distilling for Emotion Recognition](http://arxiv.org/abs/2303.13802v1)<br>:star:[code](https://github.com/mdswyz/DMD)
  * [Multivariate, Multi-Frequency and Multimodal: Rethinking Graph Neural Networks for Emotion Recognition in Conversation](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Multivariate_Multi-Frequency_and_Multimodal_Rethinking_Graph_Neural_Networks_for_Emotion_CVPR_2023_paper.pdf)
  * [Learning Emotion Representations from Verbal and Nonverbal Communication](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_Emotion_Representations_From_Verbal_and_Nonverbal_Communication_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Xeaver/EmotionCLIP)
* 人像照明
  * [LightPainter: Interactive Portrait Relighting with Freehand Scribble](http://arxiv.org/abs/2303.12950v1)
* 人脸活体检测
  * [Rethinking Domain Generalization for Face Anti-spoofing: Separability and Alignment](http://arxiv.org/abs/2303.13662v1)
  * [Instance-Aware Domain Generalization for Face Anti-Spoofing](http://arxiv.org/abs/2304.05640v1)<br>:star:[code](https://github.com/qianyuzqy/IADG)
* 说话头
  * [OTAvatar: One-shot Talking Face Avatar with Controllable Tri-plane Rendering](http://arxiv.org/abs/2303.14662v1)<br>:star:[code](https://github.com/theEricMa/OTAvatar)
  * [Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis](https://arxiv.org/abs/2211.14506)
  * [LipFormer: High-Fidelity and Generalizable Talking Face Generation With a Pre-Learned Facial Codebook](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_LipFormer_High-Fidelity_and_Generalizable_Talking_Face_Generation_With_a_Pre-Learned_CVPR_2023_paper.pdf)
  * [SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation](https://arxiv.org/abs/2211.12194)<br>:star:[code](https://github.com/Winfredy/SadTalker)
  * [Implicit Neural Head Synthesis via Controllable Local Deformation Fields](http://arxiv.org/abs/2304.11113v1)
  * [Identity-Preserving Talking Face Generation with Landmark and Appearance Priors](http://arxiv.org/abs/2305.08293v1)<br>:star:[code](https://github.com/Weizhi-Zhong/IP_LAP)
  * [Seeing What You Said: Talking Face Generation Guided by a Lip Reading Expert](http://arxiv.org/abs/2303.17480v1)
  * [High-Fidelity and Freely Controllable Talking Head Video Generation](http://arxiv.org/abs/2304.10168v1)<br>:house:[project](https://yuegao.me/PECHead)
  * [High-fidelity Generalized Emotional Talking Face Generation with Multi-modal Emotion Space Learning](http://arxiv.org/abs/2305.02572)
  * [GANHead: Towards Generative Animatable Neural Head Avatars](http://arxiv.org/abs/2304.03950v1)<br>:star:[code](https://wsj-sjtu.github.io/GANHead/)
  * [One-Shot High-Fidelity Talking-Head Synthesis with Deformable Neural Radiance Field](http://arxiv.org/abs/2304.05097v1)<br>:house:[project](https://www.waytron.net/hidenerf/)
  * [MetaPortrait: Identity-Preserving Talking Head Generation with Fast Personalized Adaptation](https://arxiv.org/abs/2212.08062)<br>:house:[project](https://meta-portrait.github.io/)
* 人脸分割
  * [Parameter Efficient Local Implicit Image Function Network for Face Segmentation](http://arxiv.org/abs/2303.15122v1)
* 眨眼检测
  * [Real-time Multi-person Eyeblink Detection in the Wild for Untrimmed Video](http://arxiv.org/abs/2303.16053v1)
* 三维头像生成
  * [Learning Personalized High Quality Volumetric Head Avatars from Monocular RGB Videos](http://arxiv.org/abs/2304.01436v1)<br>:star:[code](https://augmentedperception.github.io/monoavatar/)
  * [Instant Volumetric Head Avatars](https://arxiv.org/abs/2211.12499)<br>:house:[project](https://zielon.github.io/insta/)
  * [Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars](https://arxiv.org/abs/2211.11208)<br>:house:[project](https://mrtornado24.github.io/Next3D/)
  * [OmniAvatar: Geometry-Guided Controllable 3D Head Synthesis](http://arxiv.org/abs/2303.15539)
  * [PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360◦](https://arxiv.org/abs/2303.13071)
* 人脸表情识别
  * [Rethinking the Learning Paradigm for Dynamic Facial Expression Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Rethinking_the_Learning_Paradigm_for_Dynamic_Facial_Expression_Recognition_CVPR_2023_paper.pdf)
* 微表情识别
  * [Micron-BERT: BERT-based Facial Micro-Expression Recognition](http://arxiv.org/abs/2304.03195v1)<br>:star:[code](https://github.com/uark-cviu/Micron-BERT)微表情识别
  * [Feature Representation Learning With Adaptive Displacement Generation and Transformer Fusion for Micro-Expression Recognition](https://arxiv.org/abs/2304.04420)
  * [SelfME: Self-Supervised Motion Learning for Micro-Expression Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_SelfME_Self-Supervised_Motion_Learning_for_Micro-Expression_Recognition_CVPR_2023_paper.pdf)
* 人脸合成
  * [High-Fidelity 3D Face Generation from Natural Language Descriptions](https://arxiv.org/abs/2305.03302)<br>:house:[project](https://github.com/zhuhao-nju/describe3d)
  * [StyleGene: Crossover and Mutation of Region-Level Facial Genes for Kinship Face Synthesis](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_StyleGene_Crossover_and_Mutation_of_Region-Level_Facial_Genes_for_Kinship_CVPR_2023_paper.pdf)
* 假脸检测
  * [AUNet: Learning Relations Between Action Units for Face Forgery Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_AUNet_Learning_Relations_Between_Action_Units_for_Face_Forgery_Detection_CVPR_2023_paper.pdf)
* Facial Action Unit Detection
  * [Biomechanics-guided Facial Action Unit Detection through Force Modeling](https://openaccess.thecvf.com/content/CVPR2023/papers/Cui_Biomechanics-Guided_Facial_Action_Unit_Detection_Through_Force_Modeling_CVPR_2023_paper.pdf)
* 人脸视频编辑
  * [Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding](https://arxiv.org/abs/2212.02802)<br>:house:[project](https://diff-video-ae.github.io/)
* 人脸质量评估
  * [Face Image Quality Assessment by Learning Sample Relative Classifiability](https://openaccess.thecvf.com/content/CVPR2023/papers/Boutros_CR-FIQA_Face_Image_Quality_Assessment_by_Learning_Sample_Relative_Classifiability_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/fdbtrs/CR-FIQA)
* 人脸交换
  * [3D-Aware Face Swapping](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_3D-Aware_Face_Swapping_CVPR_2023_paper.pdf)<br>:star:[code](https://lyx0208.github.io/3dSwap)
  * [StyleIPSB: Identity-Preserving Semantic Basis of StyleGAN for High Fidelity Face Swapping](https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_StyleIPSB_Identity-Preserving_Semantic_Basis_of_StyleGAN_for_High_Fidelity_Face_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/a686432/StyleIPSB)
  * [Fine-Grained Face Swapping via Regional GAN Inversion](https://arxiv.org/abs/2211.14068)<br>:house:[project](http://e4s2022.github.io/)
  * [DiffSwap: High-Fidelity and Controllable Face Swapping via 3D-Aware Masked Diffusion](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_DiffSwap_High-Fidelity_and_Controllable_Face_Swapping_via_3D-Aware_Masked_Diffusion_CVPR_2023_paper.pdf)
* 人脸聚类
  * [Local Connectivity-Based Density Estimation for Face Clustering](https://openaccess.thecvf.com/content/CVPR2023/papers/Shin_Local_Connectivity-Based_Density_Estimation_for_Face_Clustering_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/illian01/LCE-PCENet)
* 人脸修饰
  * [Blemish-Aware and Progressive Face Retouching With Limited Paired Data](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Blemish-Aware_and_Progressive_Face_Retouching_With_Limited_Paired_Data_CVPR_2023_paper.pdf)
* 三维数字头像
  * [A Generative Model for Sculpting 3D Digital Avatars Using Diffusion](https://arxiv.org/abs/2212.06135)<br>:house:[project](https://3d-avatar-diffusion.microsoft.com/)
  * [High-fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors](https://arxiv.org/abs/2211.15064)
* 音频驱动的人脸重演
  * [Parametric Implicit Face Representation for Audio-Driven Facial Reenactment](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Parametric_Implicit_Face_Representation_for_Audio-Driven_Facial_Reenactment_CVPR_2023_paper.pdf)
* 隐私保护
  * [DartBlur: Privacy Preservation With Detection Artifact Suppression](https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_DartBlur_Privacy_Preservation_With_Detection_Artifact_Suppression_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/JaNg2333/DartBlur)
* 人脸关键点检测
  * [3D-Aware Facial Landmark Detection via Multi-View Consistent Training on Synthetic Data](https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_3D-Aware_Facial_Landmark_Detection_via_Multi-View_Consistent_Training_on_Synthetic_CVPR_2023_paper.pdf)
* 头部捕获
  * [Instant Multi-View Head Capture Through Learnable Registration](https://openaccess.thecvf.com/content/CVPR2023/papers/Bolkart_Instant_Multi-View_Head_Capture_Through_Learnable_Registration_CVPR_2023_paper.pdf)<br>:house:[project](https://tempeh.is.tue.mpg.de/)

<a name="10"/>

## 10.3D(三维重建\三维视觉)
* [Structured 3D Features for Reconstructing Controllable Avatars](https://arxiv.org/abs/2212.06820)<br>:house:[project](https://enriccorona.github.io/s3f/)
* [In-Hand 3D Object Scanning from an RGB Sequence](https://arxiv.org/abs/2211.16193)
* [Learning Geometric-Aware Properties in 2D Representation Using Lightweight CAD Models, or Zero Real 3D Pairs](https://openaccess.thecvf.com/content/CVPR2023/papers/Arsomngern_Learning_Geometric-Aware_Properties_in_2D_Representation_Using_Lightweight_CAD_Models_CVPR_2023_paper.pdf)<br>:star:[code]( https://GeoAware2dRepUsingCAD.github.io/)
* [3D Concept Learning and Reasoning from Multi-View Images](https://arxiv.org/abs/2303.11327)<br>:house:[project](https://vis-www.cs.umass.edu/3d-clr/)
* [LP-DIF: Learning Local Pattern-Specific Deep Implicit Function for 3D Objects and Scenes](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_LP-DIF_Learning_Local_Pattern-Specific_Deep_Implicit_Function_for_3D_Objects_CVPR_2023_paper.pdf)<br>:house:[project](https://github.com/gtyxyz/lpdif)
* [DynamicStereo: Consistent Dynamic Depth From Stereo Videos](https://arxiv.org/abs/2305.02296)<br>:house:[project](https://dynamic-stereo.github.io/)
* [ARO-Net: Learning Implicit Fields from Anchored Radial Observations](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_ARO-Net_Learning_Implicit_Fields_From_Anchored_Radial_Observations_CVPR_2023_paper.pdf)
* [G-MSM:Unsupervised Multi-Shape Matching With Graph-Based Affinity Priors](https://openaccess.thecvf.com/content/CVPR2023/papers/Eisenberger_G-MSM_Unsupervised_Multi-Shape_Matching_With_Graph-Based_Affinity_Priors_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/marvin-eisenberger/gmsm-matching)
* [Magic3D: High-Resolution Text-to-3D Content Creation](https://arxiv.org/abs/2211.10440)<br>:house:[project](https://research.nvidia.com/labs/dir/magic3d/)
* [PointListNet: Deep Learning on 3D Point Lists](https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_PointListNet_Deep_Learning_on_3D_Point_Lists_CVPR_2023_paper.pdf)
* [Omnimatte3D: Associating Objects and Their Effects in Unconstrained Monocular Video](https://openaccess.thecvf.com/content/CVPR2023/papers/Suhail_Omnimatte3D_Associating_Objects_and_Their_Effects_in_Unconstrained_Monocular_Video_CVPR_2023_paper.pdf)
* [HexPlane: A Fast Representation for Dynamic Scenes](https://arxiv.org/abs/2301.09632)<br>:house:[project](https://caoang327.github.io/HexPlane)
* [Energy-Efficient Adaptive 3D Sensing](https://openaccess.thecvf.com/content/CVPR2023/papers/Tilmon_Energy-Efficient_Adaptive_3D_Sensing_CVPR_2023_paper.pdf)<br>:house:[project](https://btilmon.github.io/e3d.html)
* [Objaverse: A Universe of Annotated 3D Objects](https://arxiv.org/abs/2212.08051)<br>:house:[project](http://objaverse.allenai.org/)
* [Level-S2fM: Structure from Motion on Neural Level Set of Implicit Surfaces](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_Level-S2fM_Structure_From_Motion_on_Neural_Level_Set_of_Implicit_CVPR_2023_paper.pdf)<br>:house:[project](https://henry123-boy.github.io/level-s2fm/)
* [3D Highlighter: Localizing Regions on 3D Shapes via Text Descriptions](https://arxiv.org/abs/2212.11263)<br>:star:[code](https://github.com/threedle/3DHighlighter)
* [OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation](https://arxiv.org/abs/2301.07525)<br>:house:[project](https://omniobject3d.github.io/)<br>:thumbsup:[CVPR 2023 Award Candidate | 真实高精三维物体数据集OmniObject3D](https://mp.weixin.qq.com/s/aJQCc-Iu50bYP2cE1BrY-w)
* [Neural Scene Chronology](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Neural_Scene_Chronology_CVPR_2023_paper.pdf)<br>:house:[project](https://zju3dv.github.io/NeuSC/)
* [3D Neural Field Generation Using Triplane Diffusion](https://arxiv.org/abs/2211.16677)<br>:house:[project](https://jryanshue.com/nfd)
* [Learning Adaptive Dense Event Stereo From the Image Domain](https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_Learning_Adaptive_Dense_Event_Stereo_From_the_Image_Domain_CVPR_2023_paper.pdf)
* [GANmouflage: 3D Object Nondetection With Texture Fields](https://arxiv.org/abs/2201.07202)<br>:house:[project](https://rrrrrguo.github.io/ganmouflage/)
* [Learning Accurate 3D Shape Based on Stereo Polarimetric Imaging](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Learning_Accurate_3D_Shape_Based_on_Stereo_Polarimetric_Imaging_CVPR_2023_paper.pdf)
* [Sphere-Guided Training of Neural Implicit Surfaces](https://arxiv.org/abs/2209.15511)<br>:house:[project](https://andreeadogaru.github.io/SphereGuided/)
* [PartNeRF: Generating Part-Aware Editable 3D Shapes without 3D Supervision](https://arxiv.org/abs/2303.09554)<br>:house:[project](https://ktertikas.github.io/part_nerf)
* [Masked Scene Contrast: A Scalable Framework for Unsupervised 3D Representation Learning](http://arxiv.org/abs/2303.14191v1)<br>:star:[code](https://github.com/Pointcept/Pointcept)
* [Self-Supervised Learning for Multimodal Non-Rigid 3D Shape Matching](https://arxiv.org/abs/2303.10971)<br>:star:[code](https://github.com/dongliangcao/Self-Supervised-Multimodal-Shape-Matching)
* [SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field](http://arxiv.org/abs/2303.13277v1)<br>:star:[code](https://zju3dv.github.io/sine/)
* [3DQD: Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process](https://arxiv.org/abs/2303.10406)<br>:star:[code](https://github.com/colorful-liyu/3DQD)
* [DynamicStereo: Consistent Dynamic Depth from Stereo Videos](https://arxiv.org/abs/2305.02296)<br>:house:[project](https://dynamic-stereo.github.io/)
* [3D Concept Learning and Reasoning from Multi-View Images](https://arxiv.org/abs/2303.11327)<br>:house:[project](https://vis-www.cs.umass.edu/3d-clr/)
* [PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360$^{\circ}$](http://arxiv.org/abs/2303.13071v1)<br>:star:[code](https://sizhean.github.io/panohead)
* [Persistent Nature: A Generative Model of Unbounded 3D Worlds](https://arxiv.org/abs/2303.13515)<br>:house:[project](https://chail.github.io/persistent-nature/)
* [TAPS3D: Text-Guided 3D Textured Shape Generation from Pseudo Supervision](http://arxiv.org/abs/2303.13273v1)
* [Transforming Radiance Field with Lipschitz Network for Photorealistic 3D Scene Stylization](http://arxiv.org/abs/2303.13232v1)
* [Robust Outlier Rejection for 3D Registration With Variational Bayes](http://arxiv.org/abs/2304.01514)<br>:star:[code](https://github.com/Jiang-HB/VBReg)
* [On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks](http://arxiv.org/abs/2303.14840v1)<br>:star:[code](https://github.com/Junggy/HAMMER-dataset)
* [SUDS: Scalable Urban Dynamic Scenes](http://arxiv.org/abs/2303.14536v1)<br>:house:[project](https://haithemturki.com/suds/)
* [Understanding and Improving Features Learned in Deep Functional Maps](http://arxiv.org/abs/2303.16527v1)<br>:star:[code](https://github.com/pvnieo/clover)
* [TMO: Textured Mesh Acquisition of Objects with a Mobile Device by using Differentiable Rendering](http://arxiv.org/abs/2303.15060v1)<br>:star:[code](https://jh-choi.github.io/TMO/)
* [Generalizable Local Feature Pre-training for Deformable Shape Analysis](http://arxiv.org/abs/2303.15104v1)<br>:star:[code](https://github.com/pvnieo/vader)
* [CARTO: Category and Joint Agnostic Reconstruction of ARTiculated Objects](http://arxiv.org/abs/2303.15782v1)<br>:house:[project](http://carto.cs.uni-freiburg.de)
* [CCuantuMM: Cycle-Consistent Quantum-Hybrid Matching of Multiple Shapes](http://arxiv.org/abs/2303.16202v1)<br>:house:[project](https://4dqv.mpi-inf.mpg.de/CCuantuMM/)
* [HOLODIFFUSION: Training a 3D Diffusion Model using 2D Images](http://arxiv.org/abs/2303.16509v1)<br>:star:[code](https://holodiffusion.github.io/)
* [Multi-View Azimuth Stereo via Tangent Space Consistency](http://arxiv.org/abs/2303.16447v1)<br>:star:[code](https://xucao-42.github.io/mvas_homepage/)
* [3D Line Mapping Revisited](http://arxiv.org/abs/2303.17504v1)<br>:star:[code](https://github.com/cvg/limap)
* [NeRF-Supervised Deep Stereo](http://arxiv.org/abs/2303.17603v1)<br>:star:[code](https://nerfstereo.github.io/)<br>:star:[code](https://github.com/fabiotosi92/NeRF-Supervised-Deep-Stereo)
* [Robust Outlier Rejection for 3D Registration with Variational Bayes](http://arxiv.org/abs/2304.01514v1)三维
* [Incremental 3D Semantic Scene Graph Prediction from RGB Sequences](http://arxiv.org/abs/2305.02743v1)
* Stereo Matching
  * [Iterative Geometry Encoding Volume for Stereo Matching](https://arxiv.org/abs/2303.06615)<br>:star:[code](https://github.com/gangweiX/IGEV)
  * [Masked representation learning for domain generalized stereo matching](https://openaccess.thecvf.com/content/CVPR2023/papers/Rao_Masked_Representation_Learning_for_Domain_Generalized_Stereo_Matching_CVPR_2023_paper.pdf)
  * [Learning the Distribution of Errors in Stereo Matching for Joint Disparity and Uncertainty Estimation](http://arxiv.org/abs/2304.00152v1)
  * [Domain Generalized Stereo Matching via Hierarchical Visual Transformation](https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_Domain_Generalized_Stereo_Matching_via_Hierarchical_Visual_Transformation_CVPR_2023_paper.pdf)
  * [Unsupervised Deep Asymmetric Stereo Matching With Spatially-Adaptive Self-Similarity](https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Unsupervised_Deep_Asymmetric_Stereo_Matching_With_Spatially-Adaptive_Self-Similarity_CVPR_2023_paper.pdf)
  * [High-frequency Stereo Matching Network](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_High-Frequency_Stereo_Matching_Network_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/David-Zhao-1997/High-frequency-Stereo-Matching-Network)
* 三维视觉
  * [Context-Aware Alignment and Mutual Masking for 3D-Language Pre-Training](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Context-Aware_Alignment_and_Mutual_Masking_for_3D-Language_Pre-Training_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/leolyj/3D-VLP)
* 三维重建
  * [Neural Lens Modeling](http://arxiv.org/abs/2304.04848v1)<br>:star:[code](https://neural-lens.github.io)
  * [Self-Supervised Super-Plane for Neural 3D Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_Self-Supervised_Super-Plane_for_Neural_3D_Reconstruction_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/botaoye/S3PRecon)
  * [Towards Unbiased Volume Rendering of Neural Implicit Surfaces With Geometry Priors](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Towards_Unbiased_Volume_Rendering_of_Neural_Implicit_Surfaces_With_Geometry_CVPR_2023_paper.pdf)
  * [Multiview Compressive Coding for 3D Reconstruction](https://arxiv.org/abs/2301.08247)<br>:house:[project](https://mcc3d.github.io/)
  * [PC2: Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Melas-Kyriazi_PC2_Projection-Conditioned_Point_Cloud_Diffusion_for_Single-Image_3D_Reconstruction_CVPR_2023_paper.pdf)<br>:house:[project](https://lukemelas.github.io/projection-conditioned-point-cloud-diffusion)
  * [RealFusion: 360deg Reconstruction of Any Object From a Single Image](https://openaccess.thecvf.com/content/CVPR2023/papers/Melas-Kyriazi_RealFusion_360deg_Reconstruction_of_Any_Object_From_a_Single_Image_CVPR_2023_paper.pdf)<br>:house:[project](https://lukemelas.github.io/realfusion)
  * [Deep Polarization Reconstruction With PDAVIS Events](https://openaccess.thecvf.com/content/CVPR2023/papers/Mei_Deep_Polarization_Reconstruction_With_PDAVIS_Events_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/SensorsINI/e2p)
  * [RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Anciukevicius_RenderDiffusion_Image_Diffusion_for_3D_Reconstruction_Inpainting_and_Generation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Anciukevicius/RenderDiffusion)
  * [Distilling Neural Fields for Real-Time Articulated Shape Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Distilling_Neural_Fields_for_Real-Time_Articulated_Shape_Reconstruction_CVPR_2023_paper.pdf)<br>:house:[project](https://jefftan969.github.io/dasr)
  * [High-Fidelity Clothed Avatar Reconstruction from a Single Image](https://arxiv.org/abs/2304.03903)
  * [Efficient Second-Order Plane Adjustment](https://arxiv.org/abs/2211.11542)
  * [SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation](https://arxiv.org/abs/2212.04493)<br>:house:[project](https://yccyenchicheng.github.io/SDFusion/)
  * [Reconstructing Animatable Categories From Videos](https://arxiv.org/abs/2305.06351)<br>:house:[project](https://gengshan-y.github.io/rac-www/)
  * [OReX: Object Reconstruction From Planar Cross-Sections Using Neural Fields](https://arxiv.org/abs/2211.12886)
  * [Learning Articulated Shape with Keypoint Pseudo-labels from Web Images](http://arxiv.org/abs/2304.14396v1)<br>:star:[code](https://statho.github.io/projects/animals3d/index.html)
  * [SparseFusion: Distilling View-Conditioned Diffusion for 3D Reconstruction](https://arxiv.org/abs/2212.00792)<br>:house:[project](https://sparsefusion.github.io/)
  * [3D Shape Reconstruction of Semi-Transparent Worms](https://arxiv.org/abs/2304.14841)
  * [Power Bundle Adjustment for Large-Scale 3D Reconstruction](https://arxiv.org/abs/2204.12834)
  * [PET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces](http://arxiv.org/abs/2305.05594v1)<br>:star:[code](https://github.com/yiqun-wang/PET-NeuS)
  * [AutoRecon: Automated 3D Object Discovery and Reconstruction](http://arxiv.org/abs/2305.08810v1)<br>:star:[code](https://zju3dv.github.io/autorecon)
  * [3D Registration with Maximal Cliques](http://arxiv.org/abs/2305.10854v1)
  * [3D shape reconstruction of semi-transparent worms](https://arxiv.org/abs/2304.14841)
  * [VisFusion: Visibility-aware Online 3D Scene Reconstruction from Videos](http://arxiv.org/abs/2304.10687v1)<br>:star:[code](https://github.com/huiyu-gao/VisFusion)
  * [NeUDF: Leaning Neural Unsigned Distance Fields with Volume Rendering](http://arxiv.org/abs/2304.10080v1)<br>:house:[project](http://geometrylearning.com/neudf/)
  * [ShapeClipper: Scalable 3D Shape Learning from Single-View Images via Geometric and CLIP-based Consistency](http://arxiv.org/abs/2304.06247v1)<br>:house:[project](https://zixuanh.com/projects/shapeclipper.html)
  * [BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects](http://arxiv.org/abs/2303.14158v1)<br>:star:[code](https://bundlesdf.github.io)
  * [PAniC-3D: Stylized Single-view 3D Reconstruction from Portraits of Anime Characters](http://arxiv.org/abs/2303.14587v1)<br>:star:[code](https://github.com/ShuhongChen/panic3d-anime-reconstruction)
  * [Unsupervised 3D Shape Reconstruction by Part Retrieval and Assembly](https://arxiv.org/abs/2303.01999)
  * [MobileBrick: Building LEGO for 3D Reconstruction on Mobile Devices](https://arxiv.org/abs/2303.01932)<br>:house:[project](http://code.active.vision/MobileBrick/)
  * [Seeing Through the Glass: Neural 3D Reconstruction of Object Inside a Transparent Container](http://arxiv.org/abs/2303.13805v1)<br>:star:[code](https://github.com/hirotong/ReNeuS)
  * [SCADE: NeRFs from Space Carving with Ambiguity-Aware Depth Estimates](http://arxiv.org/abs/2303.13582v1)<br>:star:[code](https://scade-spacecarving-nerfs.github.io)
  * [MACARONS: Mapping And Coverage Anticipation with RGB Online Self-Supervision](https://arxiv.org/abs/2303.03315)<br>:house:[project](https://imagine.enpc.fr/~guedona/MACARONS/)
  * [Scalable, Detailed and Mask-Free Universal Photometric Stereo](http://arxiv.org/abs/2303.15724v1)<br>:star:[code](https://github.com/satoshi-ikehata/SDM-UniPS-CVPR2023)
  * [Structural Multiplane Image: Bridging Neural View Synthesis and 3D Reconstruction](https://arxiv.org/abs/2303.05937)
  * [NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction from Multi-view Images](https://arxiv.org/abs/2303.07653)<br>:house:[project](https://yunfan1202.github.io/NEF/)
  * [Behind the Scenes: Density Fields for Single View Reconstruction](https://arxiv.org/abs/2301.07668)<br>:house:[project](https://fwmb.github.io/bts/)
  * [VolRecon: Volume Rendering of Signed Ray Distance Functions for Generalizable Multi-View Reconstruction](https://arxiv.org/abs/2212.08067)
  * Surface Reconstruction(曲面重建)
    * [NeuDA: Neural Deformable Anchor for High-Fidelity Implicit Surface Reconstruction](https://arxiv.org/abs/2303.02375)
    * [Octree Guided Unoriented Surface Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Koneputugodage_Octree_Guided_Unoriented_Surface_Reconstruction_CVPR_2023_paper.pdf)
    * [Neuralangelo: High-Fidelity Neural Surface Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Neuralangelo_High-Fidelity_Neural_Surface_Reconstruction_CVPR_2023_paper.pdf)<br>:house:[project](https://research.nvidia.com/labs/dir/neuralangelo)
    * [Neural Kernel Surface Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Neural_Kernel_Surface_Reconstruction_CVPR_2023_paper.pdf)
    * [Looking Through the Glass: Neural Surface Reconstruction Against High Specular Reflections](https://arxiv.org/abs/2304.08706)<br>:star:[code](https://github.com/JiaxiongQ/NeuS-HSR)
* 深度估计
  * [Fully Self-Supervised Depth Estimation from Defocus Clue](https://arxiv.org/abs/2303.10752)<br>:star:[code](https://github.com/Ehzoahis/DEReD)
  * [Gated Stereo: Joint Depth Estimation From Gated and Wide-Baseline Active Stereo Cues](https://openaccess.thecvf.com/content/CVPR2023/papers/Walz_Gated_Stereo_Joint_Depth_Estimation_From_Gated_and_Wide-Baseline_Active_CVPR_2023_paper.pdf)<br>:house:[project](https://light.princeton.edu/gatedstereo/)
  * [OmniVidar: Omnidirectional Depth Estimation From Multi-Fisheye Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_OmniVidar_Omnidirectional_Depth_Estimation_From_Multi-Fisheye_Images_CVPR_2023_paper.pdf)
  * [Learning To Fuse Monocular and Multi-View Cues for Multi-Frame Depth E](https://arxiv.org/abs/2304.08993)<br>:star:[code](https://github.com/ruili3/dynamic-multiframe-depth)
  * [SfM-TTR: Using Structure From Motion for Test-Time Refinement of Single-View Depth Networks](https://openaccess.thecvf.com/content/CVPR2023/papers/Izquierdo_SfM-TTR_Using_Structure_From_Motion_for_Test-Time_Refinement_of_Single-View_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/serizba/SfM-TTR)
  * [Shakes on a Plane: Unsupervised Depth Estimation From Unstabilized Photography](https://arxiv.org/abs/2212.12324)<br>:house:[project](https://light.princeton.edu/publication/soap)
  * [Depth Estimation From Camera Image and mmWave Radar Point Cloud](https://openaccess.thecvf.com/content/CVPR2023/papers/Singh_Depth_Estimation_From_Camera_Image_and_mmWave_Radar_Point_Cloud_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/nesl/radar-camera-fusion-depth)
  * [Deep Depth Estimation From Thermal Image](https://openaccess.thecvf.com/content/CVPR2023/papers/Shin_Deep_Depth_Estimation_From_Thermal_Image_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/UkcheolShin/MS2-MultiSpectralStereoDataset)
  * [LightedDepth: Video Depth Estimation in Light of Limited Inference View Angles](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_LightedDepth_Video_Depth_Estimation_in_Light_of_Limited_Inference_View_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ShngJZ/LightedDepth)
  * [Trap Attention: Monocular Depth Estimation With Manual Traps](https://openaccess.thecvf.com/content/CVPR2023/papers/Ning_Trap_Attention_Monocular_Depth_Estimation_With_Manual_Traps_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ICSResearch/TrapAttention)
  * [PlaneDepth: Self-supervised Depth Estimation via Orthogonal Planes](https://arxiv.org/abs/2210.01612)<br>:star:[code](https://github.com/svip-lab/PlaneDepth)
  * [Depth Estimation From Indoor Panoramas With Neural Scene Representation](https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_Depth_Estimation_From_Indoor_Panoramas_With_Neural_Scene_Representation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/WJ-Chang-42/IndoorPanoDepth)
  * [Polarimetric iToF:Measuring High-Fidelity Depth Through Scattering Media](https://openaccess.thecvf.com/content/CVPR2023/papers/Jeon_Polarimetric_iToF_Measuring_High-Fidelity_Depth_Through_Scattering_Media_CVPR_2023_paper.pdf)
  * [SCADE: NeRFs from Space Carving With Ambiguity-Aware Depth Estimates](https://arxiv.org/abs/2303.13582)<br>:star:[code](https://scade-spacecarving-nerfs.github.io/)
  * [iDisc: Internal Discretization for Monocular Depth Estimation](http://arxiv.org/abs/2304.06334v1)<br>:house:[project](http://vis.xyz/pub/idisc)
  * [HRDFuse: Monocular 360°Depth Estimation by Collaboratively Learning Holistic-with-Regional Depth Distributions](https://arxiv.org/abs/2303.11616)<br>:house:[project](https://haoai-1997.github.io/HRDFuse/)
  * [Learning to Fuse Monocular and Multi-view Cues for Multi-frame Depth Estimation in Dynamic Scenes](http://arxiv.org/abs/2304.08993v1)<br>:star:[code](https://github.com/ruili3/dynamic-multiframe-depth)
  * [Temporally Consistent Online Depth Estimation Using Point-Based Fusion](http://arxiv.org/abs/2304.07435v1)<br>:house:[project](https://research.facebook.com/publications/temporally-consistent-online-depth-estimation-using-point-based-fusion/)
  * [DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium](http://arxiv.org/abs/2304.03560v1)<br>:star:[code](https://antabangun.github.io/projects/DualRefine/)<br>:star:[code](https://github.com/antabangun/DualRefine)
  * [Lite-Mono: A Lightweight CNN and Transformer Architecture for Self-Supervised Monocular Depth Estimation](https://arxiv.org/abs/2211.13202)<br>:star:[code](https://github.com/noahzn/Lite-Mono)<br>:thumbsup:[CVPR2023 | 轻量高效的自监督深度估计框架Lite-Mono](https://zhuanlan.zhihu.com/p/616672642)
* 深度补全
  * [CompletionFormer: Depth Completion with Convolutions and Vision Transformers](http://arxiv.org/abs/2304.13030v1)<br>:star:[code](https://github.com/youmi-zym/CompletionFormer)<br>:star:[code](https://youmi-zym.github.io/projects/CompletionFormer/)
  * [BEV@DC: Bird’s-Eye View Assisted Training for Depth Completion](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_BEVDC_Birds-Eye_View_Assisted_Training_for_Depth_Completion_CVPR_2023_paper.pdf)
* 室内场景重建
  * [I2-SDF: Intrinsic Indoor Scene Reconstruction and Editing via Raytracing in Neural SDFs](https://arxiv.org/abs/2303.07634)<br>:house:[project](https://jingsenzhu.github.io/i2-sdf)
  * [SurfelNeRF: Neural Surfel Radiance Fields for Online Photorealistic Reconstruction of Indoor Scenes](http://arxiv.org/abs/2304.08971v1)
  * [U2RLE: Uncertainty-Guided 2-Stage Room Layout Estimation](http://arxiv.org/abs/2304.08580v1)
* 场景重建
  * [Neural Fields meet Explicit Geometric Representation for Inverse Rendering of Urban Scenes](http://arxiv.org/abs/2304.03266v1)<br>:star:[code](https://nv-tlabs.github.io/fegr/)
  * [Fast Monocular Scene Reconstruction With Global-Sparse Local-Dense Grids](https://arxiv.org/abs/2305.13220)
  * [BUOL: A Bottom-Up Framework With Occupancy-Aware Lifting for Panoptic 3D Scene Reconstruction From a Single Image](https://openaccess.thecvf.com/content/CVPR2023/papers/Chu_BUOL_A_Bottom-Up_Framework_With_Occupancy-Aware_Lifting_for_Panoptic_3D_CVPR_2023_paper.pdf)
* 3D场景生成
  * [Patch-Based 3D Natural Scene Generation From a Single Example](https://arxiv.org/abs/2304.12670)<br>:house:[project](http://weiyuli.xyz/Sin3DGen/)
  * [MIME: Human-Aware 3D Scene Generation](https://arxiv.org/abs/2212.04360)<br>:house:[project](https://mime.is.tue.mpg.de/)
* MVS
  * [Multi-View Stereo Representation Revist: Region-Aware MVSNet](http://arxiv.org/abs/2304.13614v1)
  * [Adaptive Patch Deformation for Textureless-Resilient Multi-View Stereo](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Adaptive_Patch_Deformation_for_Textureless-Resilient_Multi-View_Stereo_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/whoiszzj/APD-MVS)
  * [RIAV-MVS: Recurrent-Indexing an Asymmetric Volume for Multi-View Stereo](https://openaccess.thecvf.com/content/CVPR2023/papers/Cai_RIAV-MVS_Recurrent-Indexing_an_Asymmetric_Volume_for_Multi-View_Stereo_CVPR_2023_paper.pdf)
  * [GeoMVSNet: Learning Multi-View Stereo with Geometry Perception](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_GeoMVSNet_Learning_Multi-View_Stereo_With_Geometry_Perception_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/doubleZ0108/GeoMVSNet)
* 三维形状分类
  * [Robust 3D Shape Classification via Non-Local Graph Attention Network](https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Robust_3D_Shape_Classification_via_Non-Local_Graph_Attention_Network_CVPR_2023_paper.pdf)
* 三维图像
  * [Seeing a Rose in Five Thousand Ways](https://arxiv.org/abs/2212.04965)
* 三维形状
  * [Conjugate Product Graphs for Globally Optimal 2D-3D Shape Matching](https://openaccess.thecvf.com/content/CVPR2023/papers/Roetzer_Conjugate_Product_Graphs_for_Globally_Optimal_2D-3D_Shape_Matching_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/paul0noah/sm-2D3D)
  * [Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures](https://arxiv.org/abs/2211.07600)<br>:star:[code](https://github.com/eladrich/latent-nerf)
  * [Diffusion-SDF: Text-To-Shape via Voxelized Diffusion](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Diffusion-SDF_Text-To-Shape_via_Voxelized_Diffusion_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ttlmh/Diffusion-SDF)
* 三维形状生成
  *[Diffusion-Based Signed Distance Fields for 3D Shape Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Shim_Diffusion-Based_Signed_Distance_Fields_for_3D_Shape_Generation_CVPR_2023_paper.pdf)
* 三维形状重建
  * [Teleidoscopic Imaging System for Microscale 3D Shape Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Kawahara_Teleidoscopic_Imaging_System_for_Microscale_3D_Shape_Reconstruction_CVPR_2023_paper.pdf)
  * [What You Can Reconstruct From a Shadow](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_What_You_Can_Reconstruct_From_a_Shadow_CVPR_2023_paper.pdf)
* 3D动画
  * [RaBit: Parametric Modeling of 3D Biped Cartoon Characters With a Topological-Consistent Dataset](https://arxiv.org/abs/2303.12564)<br>:house:[project](https://gaplab.cuhk.edu.cn/projects/RaBit/)
* 室内布局
  * [Disentangling Orthogonal Planes for Indoor Panoramic Room Layout Estimation with Cross-Scale Distortion Awareness](https://arxiv.org/abs/2303.00971)<br>:star:[code](https://github.com/zhijieshen-bjtu/DOPNet)
* 视频重建
  * [Learning Event Guided High Dynamic Range Video Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Learning_Event_Guided_High_Dynamic_Range_Video_Reconstruction_CVPR_2023_paper.pdf)<br>:house:[project](https://yixinyang-00.github.io/HDRev/)

<a name="9"/>

## 9.Human Pose Estimation(人体姿态估计)
* 手势
  * [A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation from a Single RGB Image](http://arxiv.org/abs/2304.03635v1)<br>:star:[code](https://github.com/ChanglongJiangGit/A2J-Transformer)3D交互手势姿势估计
  * [Neural Voting Field for Camera-Space 3D Hand Pose Estimation](http://arxiv.org/abs/2305.04328v1)
  * [AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation](http://arxiv.org/abs/2304.12301v1)<br>:star:[code](https://assemblyhands.github.io/)
  * [Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition from Egocentric RGB Videos](https://arxiv.org/abs/2209.09484)<br>:house:[project](https://fylwen.github.io/htt.html)
  * [Cross-Domain 3D Hand Pose Estimation with Dual Modalities](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Cross-Domain_3D_Hand_Pose_Estimation_With_Dual_Modalities_CVPR_2023_paper.pdf)
  * 音频驱动的联合语音手势生成
    * [Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation](https://arxiv.org/abs/2303.09119)<br>:star:[code](https://github.com/Advocate99/DiffGesture)
    * [QPGesture: Quantization-Based and Phase-Guided Motion Matching for Natural Speech-Driven Gesture Generation](http://arxiv.org/abs/2305.11094v1)<br>:star:[code](https://github.com/YoungSeng/QPGesture)
  * 手势合成
    * [Co-Speech Gesture Synthesis by Reinforcement Learning With Contrastive Pre-Trained Rewards](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Co-Speech_Gesture_Synthesis_by_Reinforcement_Learning_With_Contrastive_Pre-Trained_Rewards_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/RLracer/RACER.git)
    * 3D手势合成
      * [Diverse 3D Hand Gesture Prediction from Body Dynamics by Bilateral Hand Disentanglement](https://arxiv.org/abs/2303.01765)
  * 手部重建
    * [ACR: Attention Collaboration-based Regressor for Arbitrary Two-Hand Reconstruction](https://arxiv.org/abs/2303.05938)<br>:star:[code](https://github.com/ZhengdiYu/Arbitrary-Hands-3D-Reconstruction)
    * [High Fidelity 3D Hand Shape Reconstruction via Scalable Graph Frequency Decomposition](https://openaccess.thecvf.com/content/CVPR2023/papers/Luan_High_Fidelity_3D_Hand_Shape_Reconstruction_via_Scalable_Graph_Frequency_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/tyluann/FreqHand)
    * [ACR: Attention Collaboration-Based Regressor for Arbitrary Two-Hand Reconstruction](https://arxiv.org/abs/2303.05938)<br>:star:[code](https://github.com/ZhengdiYu/Arbitrary-Hands-3D-Reconstruction)
    * [HARP: Personalized Hand Reconstruction From a Monocular RGB Video](https://arxiv.org/abs/2212.09530)<br>:house:[project](https://korrawe.github.io/harp-project/)
    * [Overcoming the Trade-off Between Accuracy and Plausibility in 3D Hand Shape Reconstruction](https://arxiv.org/abs/2305.00646)
    * [A Probabilistic Attention Model with Occlusion-aware Texture Regression for 3D Hand Reconstruction from a Single RGB Image](https://arxiv.org/abs/2304.14299)
    * [gSDF: Geometry-Driven Signed Distance Functions for 3D Hand-Object Reconstruction](http://arxiv.org/abs/2304.11970v1)<br>:star:[code](https://zerchen.github.io/projects/gsdf.html)
    * [MeMaHand: Exploiting Mesh-Mano Interaction for Single Image Two-Hand Reconstruction](http://arxiv.org/abs/2303.15718)
    * [POEM: Reconstructing Hand in a Point Embedded Multi-view Stereo](http://arxiv.org/abs/2304.04038v1)<br>:star:[code](https://github.com/lixiny/POEM)
    * [Handy: Towards a high fidelity 3D hand shape and appearance model](https://openaccess.thecvf.com/content/CVPR2023/papers/Potamias_Handy_Towards_a_High_Fidelity_3D_Hand_Shape_and_Appearance_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/rolpotamias/handy)
  * 3D手部恢复
    * [Bringing Inputs to Shared Domains for 3D Interacting Hands Recovery in the Wild](http://arxiv.org/abs/2303.13652v1)<br>:star:[code](https://github.com/facebookresearch/InterWild)
    * [Recovering 3D Hand Mesh Sequence from a Single Blurry Image: A New Dataset and Temporal Unfolding](http://arxiv.org/abs/2303.15417v1)<br>:star:[code](https://github.com/JaehaKim97/BlurHand_RELEASE)
    * [Semi-Supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination](https://arxiv.org/abs/2303.06380)<br>:house:[project](https://www.yangangwang.com/)
    * [H2ONet: Hand-Occlusion-and-Orientation-Aware Network for Real-Time 3D Hand Mesh Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_H2ONet_Hand-Occlusion-and-Orientation-Aware_Network_for_Real-Time_3D_Hand_Mesh_Reconstruction_CVPR_2023_paper.pdf)
  * 手物姿态估计
    * [Harmonious Feature Learning for Interactive Hand-Object Pose Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Harmonious_Feature_Learning_for_Interactive_Hand-Object_Pose_Estimation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/lzfff12/HFL-Net)
  * 3D手势预测
    * [Diverse 3D Hand Gesture Prediction From Body Dynamics by Bilateral Hand Disentanglement](https://arxiv.org/abs/2303.01765)
* 人体
  * HPE  
    * [DistilPose: Tokenized Pose Regression with Heatmap Distillation](https://arxiv.org/abs/2303.02455)
    * [Towards Stable Human Pose Estimation via Cross-View Fusion and Foot Stabilization](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhuo_Towards_Stable_Human_Pose_Estimation_via_Cross-View_Fusion_and_Foot_CVPR_2023_paper.pdf)
    * [Human Pose As Compositional Tokens](https://arxiv.org/abs/2303.11638)<br>:star:[code](https://github.com/Gengzigang/PCT)
    * [Semi-Supervised 2D Human Pose Estimation Driven by Position Inconsistency Pseudo Label Correction Module](https://arxiv.org/abs/2303.04346)<br>:star:[code](https://github.com/hlz0606/SSPCM)
    * [TokenHPE: Learning Orientation Tokens for Efficient Head Pose Estimation via Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_TokenHPE_Learning_Orientation_Tokens_for_Efficient_Head_Pose_Estimation_via_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/zc2023/TokenHPE)
    * [A Characteristic Function-Based Method for Bottom-Up Human Pose Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_A_Characteristic_Function-Based_Method_for_Bottom-Up_Human_Pose_Estimation_CVPR_2023_paper.pdf)
    * [Analyzing and Diagnosing Pose Estimation With Attributions](https://openaccess.thecvf.com/content/CVPR2023/papers/He_Analyzing_and_Diagnosing_Pose_Estimation_With_Attributions_CVPR_2023_paper.pdf)<br>:house:[project](https://qy-h00.github.io/poseig/)
    * [PoseExaminer: Automated Testing of Out-of-Distribution Robustness in Human Pose and Shape Estimation](https://arxiv.org/abs/2303.07337)<br>:star:[code](https://github.com/qihao067/PoseExaminer)
    * [Human Pose as Compositional Tokens](https://arxiv.org/abs/2303.11638)<br>:star:[code](https://github.com/Gengzigang/PCT)
    * [Unified Pose Sequence Modeling](https://openaccess.thecvf.com/content/CVPR2023/papers/Foo_Unified_Pose_Sequence_Modeling_CVPR_2023_paper.pdf)
    * [Mutual Information-Based Temporal Difference Learning for Human Pose Estimation in Video](https://arxiv.org/abs/2303.08475)
    * [Self-Correctable and Adaptable Inference for Generalizable Human Pose Estimation](https://arxiv.org/abs/2303.11180)
    * [HuManiFlow: Ancestor-Conditioned Normalising Flows on SO(3) Manifolds for Human Pose and Shape Distribution Estimation](http://arxiv.org/abs/2305.06968v1)<br>:star:[code](https://github.com/akashsengupta1997/HuManiFlow)
    * [Human Pose Estimation in Extremely Low-Light Conditions](http://arxiv.org/abs/2303.15410v1)
  * 3D HPE
    * [PSVT: End-to-End Multi-person 3D Pose and Shape Estimation with Progressive Video Transformers](https://arxiv.org/abs/2303.09187)
    * [PLIKS: A Pseudo-Linear Inverse Kinematic Solver for 3D Human Body Estimation](https://arxiv.org/abs/2211.11734)
    * [NIKI: Neural Inverse Kinematics With Invertible Neural Networks for 3D Human Pose and Shape Estimation](https://arxiv.org/abs/2305.08590)<br>:star:[code](https://github.com/Jeff-sjtu/NIKI)
    * [DiffPose: Toward More Reliable 3D Pose Estimation](https://arxiv.org/abs/2211.16940)<br>:house:[project](https://gongjia0208.github.io/Diffpose/)
    * [Scene-Aware Egocentric 3D Human Pose Estimation](https://arxiv.org/abs/2212.11684)
    * [Self-Supervised 3D Keypoint Discovery From Multi-View Videos](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_BKinD-3D_Self-Supervised_3D_Keypoint_Discovery_From_Multi-View_Videos_CVPR_2023_paper.pdf)<br>:house:[project](https://sites.google.com/view/b-kind/3d)
    * [Global-to-Local Modeling for Video-Based 3D Human Pose and Shape Estimation](https://arxiv.org/abs/2303.14747)<br>:star:[code](https://github.com/sxl142/GLoT)
    * [3D Human Pose Estimation With Spatio-Temporal Criss-Cross Attention](https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_3D_Human_Pose_Estimation_With_Spatio-Temporal_Criss-Cross_Attention_CVPR_2023_paper.pdf)
    * [Ego-Body Pose Estimation via Ego-Head Pose Estimation](https://arxiv.org/abs/2212.04636)<br>获奖论文候选
    * [Listening Human Behavior: 3D Human Pose Estimation With Acoustic Signals](https://openaccess.thecvf.com/content/CVPR2023/papers/Shibata_Listening_Human_Behavior_3D_Human_Pose_Estimation_With_Acoustic_Signals_CVPR_2023_paper.pdf)
    * [NIKI: Neural Inverse Kinematics with Invertible Neural Networks for 3D Human Pose and Shape Estimation](http://arxiv.org/abs/2305.08590v1)<br>:star:[code](https://github.com/Jeff-sjtu/NIKI)
    * [GFPose: Learning 3D Human Pose Prior With Gradient Fields](https://arxiv.org/abs/2212.08641)<br>:house:[project](https://sites.google.com/view/gfpose/)
    * [PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D Human Pose Estimation](http://arxiv.org/abs/2303.17472v1)<br>:star:[code](https://qitaozhao.github.io/PoseFormerV2)<br>:star:[code](https://github.com/QitaoZhao/PoseFormerV2)
    * [3D Human Pose Estimation via Intuitive Physics](http://arxiv.org/abs/2303.18246v1)<br>:house:[project](https://ipman.is.tue.mpg.de)
    * 3D 人体关键点估计
      * [3D Human Keypoints Estimation From Point Clouds in the Wild Without Human Labels](https://openaccess.thecvf.com/content/CVPR2023/papers/Weng_3D_Human_Keypoints_Estimation_From_Point_Clouds_in_the_Wild_CVPR_2023_paper.pdf)
  * 4D HPE
    * [SLOPER4D: A Scene-Aware Dataset for Global 4D Human Pose Estimation in Urban Environments](https://arxiv.org/abs/2303.09095)<br>:star:[code](http://www.lidarhumanmotion.net/sloper4d/) 
  * 网格恢复
    * [POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery](http://arxiv.org/abs/2303.13357v1)<br>:star:[code](https://zczcwh.github.io/potter_page) 
    * [Deformable Mesh Transformer for 3D Human Mesh Recovery](https://openaccess.thecvf.com/content/CVPR2023/papers/Yoshiyasu_Deformable_Mesh_Transformer_for_3D_Human_Mesh_Recovery_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/yusukey03012/DeFormer)
    * [One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer](https://arxiv.org/abs/2303.16160)<br>:house:[project](https://osx-ubody.github.io/)
    * [Learning Human Mesh Recovery in 3D Scenes](https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_Learning_Human_Mesh_Recovery_in_3D_Scenes_CVPR_2023_paper.pdf)<br>:star:[code](https://zju3dv.github.io/sahmr/)
    * [One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer](http://arxiv.org/abs/2303.16160v1)<br>:star:[code](https://osx-ubody.github.io/)<br>:thumbsup:[CVPR2023 IDEA与清华提出首个一阶段3D全身人体网格重建算法OSX](https://mp.weixin.qq.com/s/vAhPl4PJqR6LWWq-K28GQA)
    * [Learning Analytical Posterior Probability for Human Mesh Recovery](https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_Learning_Analytical_Posterior_Probability_for_Human_Mesh_Recovery_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/NetEase-GameAI/ProPose)
    * [Implicit 3D Human Mesh Recovery Using Consistency With Pose and Shape From Unseen-View](https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_Implicit_3D_Human_Mesh_Recovery_Using_Consistency_With_Pose_and_CVPR_2023_paper.pdf)
  * 三维人体网格估计
    * [3D Human Mesh Estimation from Virtual Markers](https://arxiv.org/abs/2303.11726)<br>:star:[code](https://github.com/ShirleyMaxx/VirtualMarker) 
  * 三维人体网格重建
    * [Sampling is Matter: Point-guided 3D Human Mesh Reconstruction](http://arxiv.org/abs/2304.09502v1)<br>:star:[code](https://github.com/DCVL-3D/PointHMR_release)
  * 3D人体重建
    * [High-fidelity 3D Human Digitization from Single 2K Resolution Images](http://arxiv.org/abs/2303.15108v1)<br>:star:[code](https://github.com/SangHunHan92/2K2K)
    * [PersonNeRF: Personalized Reconstruction From Photo Collections](https://arxiv.org/abs/2302.08504)<br>:house:[project](https://grail.cs.washington.edu/projects/personnerf/)
    * [NeMo: 3D Neural Motion Fields from Multiple Video Instances of the Same Action](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_NeMo_Learning_3D_Neural_Motion_Fields_From_Multiple_Video_Instances_CVPR_2023_paper.pdf)<br>:house:[project](https://sites.google.com/view/nemo-neural-motion-field)
    * [FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER](https://arxiv.org/abs/2205.15448)<br>:house:[project](https://zczcwh.github.io/feater_page/)
    * [CloSET: Modeling Clothed Humans on Continuous Surface With Explicit Template Decomposition](https://arxiv.org/abs/2304.03167)<br>:star:[code](https://www.liuyebin.com/closet)
    * [Learning Visibility Field for Detailed 3D Human Reconstruction and Relighting](http://arxiv.org/abs/2304.11900v1)
    * [FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER](https://arxiv.org/pdf/2205.15448.pdf)<br>:house:[project](https://zczcwh.github.io/feater_page/)
    * [Learning Semantic-Aware Disentangled Representation for Flexible 3D Human Body Editing](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Learning_Semantic-Aware_Disentangled_Representation_for_Flexible_3D_Human_Body_Editing_CVPR_2023_paper.pdf)<br>:house:[project](http://cic.tju.edu.cn/faculty/likun/projects/SemanticHuman)
    * [Complete 3D Human Reconstruction From a Single Incomplete Image](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Complete_3D_Human_Reconstruction_From_a_Single_Incomplete_Image_CVPR_2023_paper.pdf)
    * [High-Fidelity 3D Human Digitization From Single 2K Resolution Images](https://arxiv.org/abs/2303.15108)<br>:star:[code](https://github.com/SangHunHan92/2K2K)
    * [BAAM: Monocular 3D Pose and Shape Reconstruction With Bi-Contextual Attention Module and Attention-Guided Modeling](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_BAAM_Monocular_3D_Pose_and_Shape_Reconstruction_With_Bi-Contextual_Attention_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/gywns6287/BAAM)
    * [SeSDF: Self-evolved Signed Distance Field for Implicit 3D Clothed Human Reconstruction](https://arxiv.org/abs/2304.00359)
  * 人体形状补全
    * [Human Body Shape Completion With Implicit Shape and Flow Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Human_Body_Shape_Completion_With_Implicit_Shape_and_Flow_Learning_CVPR_2023_paper.pdf)  
* 多人姿态预测
  * [Trajectory-Aware Body Interaction Transformer for Multi-Person Pose Forecasting](https://arxiv.org/abs/2303.05095)<br>:star:[code](https://github.com/xiaogangpeng/TBIFormer)
* 人体解析
  * [Semantic Human Parsing via Scalable Semantic Transfer over Multiple Label Domains](http://arxiv.org/abs/2304.04140v1)<br>:star:[code](https://github.com/yangjie-cv/SST)
* 姿势迁移
  * [Zero-shot Pose Transfer for Unrigged Stylized 3D Characters](https://arxiv.org/abs/2306.00200)<br>:house:[project](https://jiashunwang.github.io/ZPT/)
* Avatar
  * [X-Avatar: Expressive Human Avatars](https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_X-Avatar_Expressive_Human_Avatars_CVPR_2023_paper.pdf)<br>:house:[project](https://ait.ethz.ch/X-Avatar)
  * [Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition](https://arxiv.org/abs/2302.11566)<br>:house:[project](https://moygcc.github.io/vid2avatar/)
* Clothed Human Reconstruction(穿衣人体重建)
  * [DIFu: Depth-Guided Implicit Function for Clothed Human Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Song_DIFu_Depth-Guided_Implicit_Function_for_Clothed_Human_Reconstruction_CVPR_2023_paper.pdf)<br>:house:[project](https://eadcat.github.io/DIFu)
  * [REC-MV: REconstructing 3D Dynamic Cloth from Monocular Videos](https://openaccess.thecvf.com/content/CVPR2023/papers/Qiu_REC-MV_REconstructing_3D_Dynamic_Cloth_From_Monocular_Videos_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/GAP-LAB-CUHK-SZ/REC-MV)


<a name="8"/>

## 8.Action Detection(人体动作检测与识别)
* [How Can Objects Help Action Recognitio](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_How_Can_Objects_Help_Action_Recognition_CVPR_2023_paper.pdf)
* [MMG-Ego4D: Multimodal Generalization in Egocentric Action Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Gong_MMG-Ego4D_Multimodal_Generalization_in_Egocentric_Action_Recognition_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/MMG)
* [Dual-Path Adaptation From Image to Video Transformers](https://arxiv.org/abs/2303.09857)<br>:star:[code](https://github.com/park-jungin/DualPath)
* [Hybrid Active Learning via Deep Clustering for Video Action Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Rana_Hybrid_Active_Learning_via_Deep_Clustering_for_Video_Action_Detection_CVPR_2023_paper.pdf)<br>:house:[project](https://tinyurl.com/hybridclaus)
* [Prompt-Guided Zero-Shot Anomaly Action Recognition using Pretrained Deep Skeleton Features](http://arxiv.org/abs/2303.15167v1)
* [Learning Action Changes by Measuring Verb-Adverb Textual Relationships](http://arxiv.org/abs/2303.15086v1)<br>:star:[code](https://github.com/dmoltisanti/air-cvpr23)
* [STMixer: A One-Stage Sparse Action Detector](http://arxiv.org/abs/2303.15879v1)
* [AutoLabel: CLIP-based framework for Open-set Video Domain Adaptation](http://arxiv.org/abs/2304.01110v1)
* [Search-Map-Search: A Frame Selection Paradigm for Action Recognition](http://arxiv.org/abs/2304.10316v1)
* [On the Benefits of 3D Pose and Tracking for Human Action Recognition](http://arxiv.org/abs/2304.01199v1)<br>:star:[code](https://brjathu.github.io/LART)
* [MMG-Ego4D: Multi-Modal Generalization in Egocentric Action Recognition](http://arxiv.org/abs/2305.07214v1)<br>:star:[code](https://github.com/facebookresearch/MMG_Ego4D)
* [SVFormer: Semi-Supervised Video Transformer for Action Recognition](https://arxiv.org/abs/2211.13222)
* 基于骨架的动作识别
  * [Learning Discriminative Representations for Skeleton Based Action Recognition](https://arxiv.org/abs/2303.03729)
  * [Actionlet-Dependent Contrastive Learning for Unsupervised Skeleton-Based Action Recognition](https://arxiv.org/abs/2303.10904)<br>:house:[project](https://langlandslin.github.io/projects/ActCLR/)
  * [3Mformer: Multi-order Multi-mode Transformer for Skeletal Action Recognition](http://arxiv.org/abs/2303.14474v1)
  * [HaLP: Hallucinating Latent Positives for Skeleton-based Self-Supervised Learning of Actions](http://arxiv.org/abs/2304.00387v1)<br>:star:[code](https://github.com/anshulbshah/HaLP)
  * [Neural Koopman Pooling: Control-Inspired Temporal Dynamics Encoding for Skeleton-Based Action Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Neural_Koopman_Pooling_Control-Inspired_Temporal_Dynamics_Encoding_for_Skeleton-Based_Action_CVPR_2023_paper.pdf)
* 基于关键点的动作识别
  * [Unified Keypoint-based Action Recognition Framework via Structured Keypoint Pooling](http://arxiv.org/abs/2303.15270v1)
* 时序动作识别
  * [TriDet: Temporal Action Detection with Relative Boundary Modeling](https://arxiv.org/abs/2303.07347)<br>:star:[code](https://github.com/sssste/TriDet)
  * [Proposal-Based Multiple Instance Learning for Weakly-Supervised Temporal Action Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Proposal-Based_Multiple_Instance_Learning_for_Weakly-Supervised_Temporal_Action_Localization_CVPR_2023_paper.pdf)<br>:star:[code](github.com/RenHuan1999/CVPR2023_P-MIL)
  * [Post-Processing Temporal Action Detection](https://arxiv.org/abs/2211.14924)<br>:star:[code](https://github.com/sauradip/GAP)
  * [Decomposed Cross-modal Distillation for RGB-based Temporal Action Detection](http://arxiv.org/abs/2303.17285v1)
  * [PivoTAL: Prior-Driven Supervision for Weakly-Supervised Temporal Action Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Rizve_PivoTAL_Prior-Driven_Supervision_for_Weakly-Supervised_Temporal_Action_Localization_CVPR_2023_paper.pdf)
* 开集动作识别
  * [Open Set Action Recognition via Multi-Label Evidential Learning](http://arxiv.org/abs/2303.12698v1) 
  * [Enlarging Instance-specific and Class-specific Information for Open-set Action Recognition](http://arxiv.org/abs/2303.15467v1)<br>:star:[code](https://github.com/Jun-CEN/PSL)
* 基于MoCap的动作识别
  * [STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action Recognition](http://arxiv.org/abs/2303.18177v1)<br>:star:[code](https://github.com/zgzxy001/STMT)
* 小样本动作识别
  * [MoLo: Motion-augmented Long-short Contrastive Learning for Few-shot Action Recognition](http://arxiv.org/abs/2304.00946v1)<br>:star:[code](https://github.com/alibaba-mmai-research/MoLo)
  * [Active Exploration of Multimodal Complementarity for Few-Shot Action Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Wanyan_Active_Exploration_of_Multimodal_Complementarity_for_Few-Shot_Action_Recognition_CVPR_2023_paper.pdf)
* 半监督动作识别
  * [TimeBalance: Temporally-Invariant and Temporally-Distinctive Video Representations for Semi-Supervised Action Recognition](http://arxiv.org/abs/2303.16268v1)<br>:star:[code](https://github.com/DAVEISHAN/TimeBalance)
* 时序动作定位
  * [Boosting Weakly-Supervised Temporal Action Localization with Text Information](https://arxiv.org/abs/2305.00607)<br>:star:[code](https://github.com/lgzlIlIlI/Boosting-WTAL)
  * [Distilling Vision-Language Pre-Training To Collaborate With Weakly-Supervised Temporal Action Localization](https://arxiv.org/abs/2212.09335)
  * [AdamsFormer for Spatial Action Localization in the Future](https://openaccess.thecvf.com/content/CVPR2023/papers/Chi_AdamsFormer_for_Spatial_Action_Localization_in_the_Future_CVPR_2023_paper.pdf)
  * [Re2TAL: Rewiring Pretrained Video Backbones for Reversible Temporal Action Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Re2TAL_Rewiring_Pretrained_Video_Backbones_for_Reversible_Temporal_Action_Localization_CVPR_2023_paper.pdf)
* 群组动作质量评估
  * [LOGO: A Long-Form Video Dataset for Group Action Quality Assessment](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_LOGO_A_Long-Form_Video_Dataset_for_Group_Action_Quality_Assessment_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/shiyi-zh0408/LOGO)
* 群体动作识别
  * [An Actor-Centric Causality Graph for Asynchronous Temporal Inference in Group Activity](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_An_Actor-Centric_Causality_Graph_for_Asynchronous_Temporal_Inference_in_Group_CVPR_2023_paper.pdf)

<a name="7"/>

## 7.Point Cloud(点云)
* [PointClustering: Unsupervised Point Cloud Pre-Training Using Transformation Invariance in Clustering](https://openaccess.thecvf.com/content/CVPR2023/papers/Long_PointClustering_Unsupervised_Point_Cloud_Pre-Training_Using_Transformation_Invariance_in_Clustering_CVPR_2023_paper.pdf)
* [Adversarially Masking Synthetic To Mimic Real: Adaptive Noise Injection for Point Cloud Segmentation Adaptation](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Adversarially_Masking_Synthetic_To_Mimic_Real_Adaptive_Noise_Injection_for_CVPR_2023_paper.pdf)
* [Parts2Words: Learning Joint Embedding of Point Clouds and Texts by Bidirectional Matching Between Parts and Words](http://arxiv.org/abs/2107.01872)
* [Attention-Based Point Cloud Edge Sampling](https://arxiv.org/abs/2302.14673)
* [Meta Architecture for Point Cloud Analysis](https://arxiv.org/abs/2211.14462)
* [Building Rearticulable Models for Arbitrary 3D Objects From 4D Point Clouds](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Building_Rearticulable_Models_for_Arbitrary_3D_Objects_From_4D_Point_CVPR_2023_paper.pdf)<br>:house:[project](https://stevenlsw.github.io/reart/)
* [Implicit Surface Contrastive Clustering for LiDAR Point Clouds](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Implicit_Surface_Contrastive_Clustering_for_LiDAR_Point_Clouds_CVPR_2023_paper.pdf)
* [Poly-PC: A Polyhedral Network for Multiple Point Cloud Tasks at Once](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Poly-PC_A_Polyhedral_Network_for_Multiple_Point_Cloud_Tasks_at_CVPR_2023_paper.pdf)
* [TriVol: Point Cloud Rendering via Triple Volumes](https://arxiv.org/abs/2303.16485)
* [PartManip: Learning Cross-Category Generalizable Part Manipulation Policy from Point Cloud Observations](https://arxiv.org/abs/2303.16958)
* [PointCMP: Contrastive Mask Prediction for Self-supervised Learning on Point Cloud Videos](https://arxiv.org/abs/2305.04075)
* [GrowSP: Unsupervised Semantic Segmentation of 3D Point Clouds](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_GrowSP_Unsupervised_Semantic_Segmentation_of_3D_Point_Clouds_CVPR_2023_paper.pdf)
* [Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting](https://arxiv.org/abs/2302.13130)<br>:star:[code](https://github.com/tarashakhurana/4d-occ-forecasting)
* [ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_ULIP_Learning_a_Unified_Representation_of_Language_Images_and_Point_CVPR_2023_paper.pdf)
* [SE-ORNet: Self-Ensembling Orientation-Aware Network fhttpsor Unsupervised Point Cloud Shape Correspondence](https://openaccess.thecvf.com/content/CVPR2023/papers/Deng_SE-ORNet_Self-Ensembling_Orientation-Aware_Network_for_Unsupervised_Point_Cloud_Shape_Correspondence_CVPR_2023_paper.pdf)
* [GeoMAE: Masked Geometric Target Prediction for Self-Supervised Point Cloud Pre-Training](https://arxiv.org/abs/2305.08808)
* [Neural Intrinsic Embedding for Non-rigid Point Cloud Matching](https://arxiv.org/pdf/2303.01038.pdf)
* [3D Spatial Multimodal Knowledge Accumulation for Scene Graph Prediction in Point Cloud](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_3D_Spatial_Multimodal_Knowledge_Accumulation_for_Scene_Graph_Prediction_in_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/HHrEtvP/SMKA)
* [SHS-Net: Learning Signed Hyper Surfaces for Oriented Normal Estimation of Point Clouds](http://arxiv.org/abs/2305.05873v1)
* [GeoMAE: Masked Geometric Target Prediction for Self-supervised Point Cloud Pre-Training](http://arxiv.org/abs/2305.08808v1)
* [SCPNet: Semantic Scene Completion on Point Cloud](https://arxiv.org/abs/2303.06884)
* [NeuralEditor: Editing Neural Radiance Fields via Manipulating Point Clouds](http://arxiv.org/abs/2305.03049v1)<br>:star:[code](https://immortalco.github.io/NeuralEditor)
* [Rotation-Invariant Transformer for Point Cloud Matching](https://arxiv.org/abs/2303.08231)
* [Recognizing Rigid Patterns of Unlabeled Point Clouds by Complete and Continuous Isometry Invariants with no False Negatives and no False Positives](http://arxiv.org/abs/2303.15385v1)<br>:house:[project](http://kurlin.org/projects/cloud-isometry-spaces/distance-based-invariants.pdf)
* [PointCMP: Contrastive Mask Prediction for Self-supervised Learning on Point Cloud Videos](http://arxiv.org/abs/2305.04075v1)
* [VL-SAT: Visual-Linguistic Semantics Assisted Training for 3D Semantic Scene Graph Prediction in Point Cloud](http://arxiv.org/abs/2303.14408v1)<br>:star:[code](https://github.com/wz7in/CVPR2023-VLSAT)
* [Unsupervised Inference of Signed Distance Functions from Single Sparse Point Clouds without Learning Priors](http://arxiv.org/abs/2303.14505v1)<br>:star:[code](https://github.com/chenchao15/NeuralTPS)
* [Grad-PU: Arbitrary-Scale Point Cloud Upsampling via Gradient Descent with Learned Distance Functions](http://arxiv.org/abs/2304.11846v1)<br>:star:[code](https://github.com/yunhe20/Grad-PU)
* [Binarizing Sparse Convolutional Networks for Efficient Point Cloud Analysis](http://arxiv.org/abs/2303.15493v1)
* [Spatiotemporal Self-supervised Learning for Point Clouds in the Wild](https://arxiv.org/pdf/2303.16235.pdf)<br>:star:[code](https://github.com/YanhaoWu/STSSL)
* [NerVE: Neural Volumetric Edges for Parametric Curve Extraction from Point Cloud](http://arxiv.org/abs/2303.16465v1)<br>:star:[code](https://dongdu3.github.io/projects/2023/NerVE/)
* [IterativePFN: True Iterative Point Cloud Filtering](http://arxiv.org/abs/2304.01529v1)<br>:star:[code](https://github.com/ddsediri/IterativePFN)
* [Fast Point Cloud Generation With Straight Flows](https://arxiv.org/abs/2212.01747)
* [GD-MAE: Generative Decoder for MAE Pre-Training on LiDAR Point Clouds](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_GD-MAE_Generative_Decoder_for_MAE_Pre-Training_on_LiDAR_Point_Clouds_CVPR_2023_paper.pdf)
* 3D点云
  * [Parameter is Not All You Need: Starting from Non-Parametric Networks for 3D Point Cloud Analysis](https://arxiv.org/abs/2303.08134)<br>:star:[code](https://github.com/ZrrSkywalker/Point-NN)
  * [ToThePoint: Efficient Contrastive Learning of 3D Point Clouds via Recycling](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_ToThePoint_Efficient_Contrastive_Learning_of_3D_Point_Clouds_via_Recycling_CVPR_2023_paper.pdf)
  * [PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models](https://arxiv.org/abs/2212.01558)<br>:house:[project](https://colin97.github.io/PartSLIP_page/)
  * [Starting From Non-Parametric Networks for 3D Point Cloud Analysis](https://arxiv.org/abs/2303.08134)<br>:star:[code](https://github.com/ZrrSkywalker/Point-NN)
  * [Learnable Skeleton-Aware 3D Point Cloud Sampling](https://openaccess.thecvf.com/content/CVPR2023/papers/Wen_Learnable_Skeleton-Aware_3D_Point_Cloud_Sampling_CVPR_2023_paper.pdf)
  * [GraVoS: Voxel Selection for 3D Point-Cloud Detection](https://arxiv.org/abs/2208.08780)
  * [MarS3D: A Plug-and-Play Motion-Aware Model for Semantic Segmentation on Multi-Scan 3D Point Clouds](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_MarS3D_A_Plug-and-Play_Motion-Aware_Model_for_Semantic_Segmentation_on_Multi-Scan_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/CVMI-Lab/MarS3D)
  * [NeuralPCI: Spatio-temporal Neural Field for 3D Point Cloud Multi-frame Non-linear Interpolation](http://arxiv.org/abs/2303.15126v1)<br>:star:[code](https://github.com/ispc-lab/NeuralPCI)<br>:star:[code](https://dyfcalid.github.io/NeuralPCI)
  * [Rethinking the Approximation Error in 3D Surface Fitting for Point Cloud Normal Estimation](http://arxiv.org/abs/2303.17167v1)<br>:star:[code](https://github.com/hikvision-research/3DVision)
* 点云实例分割
  * [ISBNet: a 3D Point Cloud Instance Segmentation Network with Instance-aware Sampling and Box-aware Dynamic Convolution](https://arxiv.org/pdf/2303.00246.pdf)
* 点云分类
  * [PointCert: Point Cloud Classification with Deterministic Certified Robustness Guarantees](https://arxiv.org/abs/2303.01959)
  * [CAP: Robust Point Cloud Classification via Semantic and Structural Modeling](https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_CAP_Robust_Point_Cloud_Classification_via_Semantic_and_Structural_Modeling_CVPR_2023_paper.pdf)
  * [ViewNet: A Novel Projection-Based Backbone With View Pooling for Few-Shot Point Cloud Classification](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_ViewNet_A_Novel_Projection-Based_Backbone_With_View_Pooling_for_Few-Shot_CVPR_2023_paper.pdf)
* 点云补全
  * [ProxyFormer: Proxy Alignment Assisted Point Cloud Completion with Missing Part Sensitive Transformer](https://arxiv.org/pdf/2302.14435.pdf)<br>:star:[code](https://github.com/I2-Multimedia-Lab/ProxyFormer)
  * [ACL-SPC: Adaptive Closed-Loop system for Self-Supervised Point Cloud Completion](https://arxiv.org/abs/2303.01979)<br>:star:[code](https://github.com/Sangminhong/ACL-SPC_PyTorch)
  * [AnchorFormer: Point Cloud Completion From Discriminative Nodes](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_AnchorFormer_Point_Cloud_Completion_From_Discriminative_Nodes_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/chenzhik/AnchorFormer)
  * [Hyperspherical Embedding for Point Cloud Completion](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Hyperspherical_Embedding_for_Point_Cloud_Completion_CVPR_2023_paper.pdf)
* 点云配准
  * [Deep Graph-based Spatial Consistency for Robust Non-rigid Point Cloud Registration](https://arxiv.org/abs/2303.09950)<br>:star:[code](https://github.com/qinzheng93/GraphSCNet)
  * [PEAL: Prior-Embedded Explicit Attention Learning for Low-Overlap Point Cloud Registration](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_PEAL_Prior-Embedded_Explicit_Attention_Learning_for_Low-Overlap_Point_Cloud_Registration_CVPR_2023_paper.pdf)
  * [Unsupervised Deep Probabilistic Approach for Partial Point Cloud Registration](http://arxiv.org/abs/2303.13290v1)
  * [Robust Multiview Point Cloud Registration with Reliable Pose Graph Initialization and History Reweighting](http://arxiv.org/abs/2304.00467v1)<br>:star:[code](https://github.com/WHU-USI3DV/SGHR)
  * [BUFFER: Balancing Accuracy, Efficiency, and Generalizability in Point Cloud Registration](https://openaccess.thecvf.com/content/CVPR2023/papers/Ao_BUFFER_Balancing_Accuracy_Efficiency_and_Generalizability_in_Point_Cloud_Registration_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/aosheng1996/BUFFER)
* 点云理解
  * [Self-positioning Point-based Transformer for Point Cloud Understanding](http://arxiv.org/abs/2303.16450v1)<br>:star:[code](https://github.com/mlvlab/SPoTr) 
* 点云重建  
  * [Learning To Measure the Point Cloud Reconstruction Loss in a Representation Space](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Learning_To_Measure_the_Point_Cloud_Reconstruction_Loss_in_a_CVPR_2023_paper.pdf)
* 点云匹配
  * [Neural Intrinsic Embedding for Non-Rigid Point Cloud Matching](https://arxiv.org/abs/2303.01038)
* 点云分割
  *[Improving Graph Representation for Point Cloud Segmentation via Attentive Filtering](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Improving_Graph_Representation_for_Point_Cloud_Segmentation_via_Attentive_Filtering_CVPR_2023_paper.pdf)
* 点云压缩
  * [Efficient Hierarchical Entropy Model for Learned Point Cloud Compression](https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Efficient_Hierarchical_Entropy_Model_for_Learned_Point_Cloud_Compression_CVPR_2023_paper.pdf)

<a name="6"/>

## 6.Object Tracking(目标跟踪)
* [Data-Driven Feature Tracking for Event Cameras](http://arxiv.org/abs/2211.12826)
* [Autoregressive Visual Tracking](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Autoregressive_Visual_Tracking_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/MIV-XJTU/ARTrack)
* [Propagate And Calibrate: Real-time Passive Non-line-of-sight Tracking](https://arxiv.org/abs/2303.11791)<br>:house:[project](https://againstentropy.github.io/NLOS-Track/)
* [Unifying Short and Long-Term Tracking With Graph Hierarchies](https://openaccess.thecvf.com/content/CVPR2023/papers/Cetintas_Unifying_Short_and_Long-Term_Tracking_With_Graph_Hierarchies_CVPR_2023_paper.pdf)<br>:house:[project](bit.ly/sushi-mot)
* [VideoTrack: Learning To Track Objects via Video Transformer](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_VideoTrack_Learning_To_Track_Objects_via_Video_Transformer_CVPR_2023_paper.pdf)
* [Tracking Through Containers and Occluders in the Wild](https://arxiv.org/abs/2305.03052)<br>:house:[project](https://tcow.cs.columbia.edu/)
* [Frame-Event Alignment and Fusion Network for High Frame Rate Tracking](https://arxiv.org/abs/2305.15688)
* [Propagate And Calibrate: Real-time Passive Non-line-of-sight Tracking](https://arxiv.org/abs/2303.11791)<br>:star:[code](https://againstentropy.github.io/NLOS-Track/)
* [Joint Visual Grounding and Tracking with Natural Language Specification](https://arxiv.org/abs/2303.12027)<br>:star:[code](https://github.com/lizhou-cs/JointNLT)
* [Generalized Relation Modeling for Transformer Tracking](http://arxiv.org/abs/2303.16580v1)<br>:star:[code](https://github.com/Little-Podi/GRM)
* [SeqTrack: Sequence to Sequence Learning for Visual Object Tracking](http://arxiv.org/abs/2304.14394v1)
* [Tracking through Containers and Occluders in the Wild](http://arxiv.org/abs/2305.03052v1)<br>:house:[project](https://tcow.cs.columbia.edu/)
* [DropMAE: Masked Autoencoders with Spatial-Attention Dropout for Tracking Tasks](http://arxiv.org/abs/2304.00571v1)<br>:star:[code](https://github.com/jimmy-dq/DropMAE.git)
* [CXTrack: Improving 3D Point Cloud Tracking With Contextual Information](https://arxiv.org/abs/2211.08542)
* [Representation Learning for Visual Object Tracking by Masked Appearance Transfer](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Representation_Learning_for_Visual_Object_Tracking_by_Masked_Appearance_Transfer_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/difhnp/MAT)
* 多目标跟踪
  * [Referring Multi-Object Tracking](https://arxiv.org/abs/2303.03366)<br>:star:[code](https://github.com/wudongming97/RMOT)
  * [Tracking Multiple Deformable Objects in Egocentric Videos](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Tracking_Multiple_Deformable_Objects_in_Egocentric_Videos_CVPR_2023_paper.pdf)<br>:house:[project](https://mingzhenhuang.com/projects/detracker.html)
  * [MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors](https://arxiv.org/abs/2211.09791)<br>:star:[code](https://github.com/megvii-research/MOTRv2)
  * [UTM: A Unified Multiple Object Tracking Model With Identity-Aware Feature Enhancement](https://openaccess.thecvf.com/content/CVPR2023/papers/You_UTM_A_Unified_Multiple_Object_Tracking_Model_With_Identity-Aware_Feature_CVPR_2023_paper.pdf)
  * [Focus on Details: Online Multi-Object Tracking With Diverse Fine-Grained Representation](https://arxiv.org/abs/2302.14589)
  * [Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking](https://arxiv.org/abs/2302.03802)<br>:star:[code](https://github.com/TRI-ML/PF-Track)
  * [MotionTrack: Learning Robust Short-term and Long-term Motions for Multi-Object Tracking](https://arxiv.org/abs/2303.10404)
  * [OVTrack: Open-Vocabulary Multiple Object Tracking](http://arxiv.org/abs/2304.08408v1)<br>:house:[project](https://www.vis.xyz/pub/ovtrack/)
* 多模态跟踪
  * [Visual Prompt Multi-Modal Tracking](https://arxiv.org/abs/2303.10826)<br>:star:[code](https://github.com/jiawen-zhu/ViPT)
* RGB-T tracking(可见光图像（RGB）和热红外图像（T）结合起来进行目标追踪)
  * [Bridging Search Region Interaction With Template for RGB-T Tracking](https://openaccess.thecvf.com/content/CVPR2023/papers/Hui_Bridging_Search_Region_Interaction_With_Template_for_RGB-T_Tracking_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/RyanHTR/TBSI)
  * [Efficient RGB-T Tracking via Cross-Modality Distillation](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Efficient_RGB-T_Tracking_via_Cross-Modality_Distillation_CVPR_2023_paper.pdf)

<a name="5"/>

## 5.Object Detection(目标检测)
* [Enhanced Training of Query-Based Object Detection via Selective Query Recollection](http://arxiv.org/abs/2212.07593)
* [The Differentiable Lens: Compound Lens Search Over Glass Surfaces and Materials for Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Cote_The_Differentiable_Lens_Compound_Lens_Search_Over_Glass_Surfaces_and_CVPR_2023_paper.pdf)
* [Multi-view Adversarial Discriminator: Mine the Non-causal Factors for Object Detection in Unseen Domains](https://arxiv.org/abs/2304.02950)<br>:star:[code](https://github.com/K2OKOH/MAD)
* [Detection Hub: Unifying Object Detection Datasets via Query Adaptation on Language Embedding](https://arxiv.org/abs/2206.03484)
* [NeRF-RPN: A General Framework for Object Detection in NeRFs](https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_NeRF-RPN_A_General_Framework_for_Object_Detection_in_NeRFs_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/lyclyc52/NeRF_RPN)
* [Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors](https://arxiv.org/abs/2208.11356)
* [Towards Building Self-Aware Object Detectors via Reliable Uncertainty Quantification and Calibration](https://openaccess.thecvf.com/content/CVPR2023/papers/Oksuz_Towards_Building_Self-Aware_Object_Detectors_via_Reliable_Uncertainty_Quantification_and_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/fiveai/saod)
* [Gaussian Label Distribution Learning for Spherical Image Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Gaussian_Label_Distribution_Learning_for_Spherical_Image_Object_Detection_CVPR_2023_paper.pdf)
* [Rawgment: Noise-Accounted RAW Augmentation Enables Recognition in a Wide Variety of Environments](https://arxiv.org/abs/2210.16046)
* [Towards Unsupervised Object Detection From LiDAR Point Clouds](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Towards_Unsupervised_Object_Detection_From_LiDAR_Point_Clouds_CVPR_2023_paper.pdf)<br>:house:[project](https://waabi.ai/research/oyster)
* [Mask DINO: Towards a Unified Transformer-Based Framework for Object Detection and Segmentation](https://arxiv.org/abs/2206.02777)<br>:star:[code](https://github.com/IDEACVR/MaskDINO)
* [T-SEA: Transfer-Based Self-Ensemble Attack on Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_T-SEA_Transfer-Based_Self-Ensemble_Attack_on_Object_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/VDIGPKU/T-SEA)
* [Recurrent Vision Transformers for Object Detection With Event Cameras](https://arxiv.org/abs/2212.05598)
* [Learned Two-Plane Perspective Prior Based Image Resampling for Efficient Object Detection](https://arxiv.org/abs/2303.14311)
* [Normalizing Flow Based Feature Synthesis for Outlier-Aware Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Kumar_Normalizing_Flow_Based_Feature_Synthesis_for_Outlier-Aware_Object_Detection_CVPR_2023_paper.pdf)
* [YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors](https://arxiv.org/abs/2207.02696)<br>:star:[code](https://github.com/WongKinYiu/yolov7)
* [MetaFusion: Infrared and Visible Image Fusion via Meta-Feature Embedding From Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_MetaFusion_Infrared_and_Visible_Image_Fusion_via_Meta-Feature_Embedding_From_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/wdzhao123/MetaFusion)
* [Doubly Right Object Recognition: A Why Prompt for Visual Rationales](https://arxiv.org/abs/2212.06202)
* [Phase-Shifting Coder: Predicting Accurate Orientation in Oriented Object Detection](https://arxiv.org/abs/2211.06368)<br>:star:[code](https://github.com/open-mmlab/mmrotate)
* [Unbalanced Optimal Transport: A Unified Framework for Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/De_Plaen_Unbalanced_Optimal_Transport_A_Unified_Framework_for_Object_Detection_CVPR_2023_paper.pdf)
* [CLIP the Gap: A Single Domain Generalization Approach for Object Detection](https://arxiv.org/abs/2301.05499)
* [Learning Transformations To Reduce the Geometric Shift in Object Detection](https://arxiv.org/abs/2301.05496)
* [Object Detection With Self-Supervised Scene Adaptation](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Object_Detection_With_Self-Supervised_Scene_Adaptation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/cvlab-stonybrook/scenes100)
* [Lite DETR : An Interleaved Multi-Scale Encoder for Efficient DETR](https://arxiv.org/abs/2303.07335)<br>:star:[code](https://github.com/IDEA-Research/Lite-DETR)
* [SAP-DETR: Bridging the Gap Between Salient Points and Queries-Based Transformer Detector for Fast Model Convergency](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_SAP-DETR_Bridging_the_Gap_Between_Salient_Points_and_Queries-Based_Transformer_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/liuyang-ict/SAP-DETR)
* [Multiclass Confidence and Localization Calibration for Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Pathiraja_Multiclass_Confidence_and_Localization_Calibration_for_Object_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/bimsarapathiraja/MCCL)
* [Mobile User Interface Element Detection Via Adaptively Prompt Tuning](https://arxiv.org/abs/2305.09699)
* [DynamicDet: A Unified Dynamic Architecture for Object Detection](http://arxiv.org/abs/2304.05552v1)<br>:star:[code](https://github.com/VDIGPKU/DynamicDet) 
* [ZBS: Zero-shot Background Subtraction via Instance-level Background Modeling and Foreground Selection](http://arxiv.org/abs/2303.14679v1)<br>:star:[code](https://github.com/CASIA-IVA-Lab/ZBS)
* [Curricular Object Manipulation in LiDAR-based Object Detection](http://arxiv.org/abs/2304.04248v1)<br>:star:[code](https://github.com/ZZY816/COM)
* [STDLens: Model Hijacking-resilient Federated Learning for Object Detection](https://arxiv.org/abs/2303.11511)<br>:star:[code](https://github.com/git-disl/STDLens)
* [What Can Human Sketches Do for Object Detection?](http://arxiv.org/abs/2303.15149v1)<br>:star:[code](https://pinakinathc.github.io/sketch-detect)
* [Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects](http://arxiv.org/abs/2303.13769v1)<br>:star:[code](https://github.com/Went-Liang/UnSniffer)
* [Bridging Precision and Confidence: A Train-Time Loss for Calibrating Object Detection](http://arxiv.org/abs/2303.14404v1)<br>:star:[code](https://github.com/akhtarvision/bpc_calibration)
* [Learned Two-Plane Perspective Prior based Image Resampling for Efficient Object Detection](http://arxiv.org/abs/2303.14311v1)
* [T-SEA: Transfer-based Self-Ensemble Attack on Object Detection](https://arxiv.org/pdf/2211.09773.pdf)<br>:star:[code](https://github.com/VDIGPKU/T-SEA)<br>:thumbsup:[CVPR 2023 | 北大提出T-SEA: 自集成策略实现更强的黑盒攻击迁移性](https://mp.weixin.qq.com/s/UPFnuHwHe1YqNOYCcHQ1rQ)
* [Knowledge Combination to Learn Rotated Detection Without Rotated Annotation](http://arxiv.org/abs/2304.02199v1)
* [Universal Instance Perception as Object Discovery and Retrieval](https://arxiv.org/abs/2303.06674)<br>:star:[code](https://github.com/MasterBin-IIAU/UNINEXT)
* [Continual Detection Transformer for Incremental Object Detection](http://arxiv.org/abs/2304.03110v1)目标检测
* [Multi-view Adversarial Discriminator: Mine the Non-causal Factors for Object Detection in Unseen Domains](http://arxiv.org/abs/2304.02950v1)<br>:star:[code](https://github.com/K2OKOH/MAD")目标检测
* 开放词汇目标检测
  * [Aligning Bag of Regions for Open-Vocabulary Object Detection](https://arxiv.org/abs/2302.13996)<br>:star:[code](https://github.com/wusize/ovdet)
  * [Region-Aware Pretraining for Open-Vocabulary Object Detection With Vision Transformers](https://arxiv.org/abs/2305.07011)
  * [DetCLIPv2: Scalable Open-Vocabulary Object Detection Pre-Training via Word-Region Alignment](https://arxiv.org/abs/2304.04514)
  * [OvarNet: Towards Open-vocabulary Object Attribute Recognition](https://arxiv.org/abs/2301.09506)<br>:thumbsup:[CVPR2023｜小红书提出 OvarNet 模型：开集预测的新SOTA，“万物识别”有了新玩法](https://mp.weixin.qq.com/s/EkYz5FO2PPpK_uVD7i2q-g)
  * [Learning To Detect and Segment for Open Vocabulary Object Detection](https://arxiv.org/abs/2212.12130)
  * [Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers](http://arxiv.org/abs/2305.07011v1)
  * [Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection](https://arxiv.org/abs/2303.05892)<br>:star:[code](https://github.com/LutingWang/OADP)
  * [CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching](http://arxiv.org/abs/2303.13076v1)
  * [DetCLIPv2: Scalable Open-Vocabulary Object Detection Pre-training via Word-Region Alignment](http://arxiv.org/abs/2304.04514v1)
* 开放世界目标检测
  * [Annealing-Based Label-Transfer Learning for Open World Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_Annealing-Based_Label-Transfer_Learning_for_Open_World_Object_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/DIG-Beihang/ALLOW.git)
  * [CapDet: Unifying Dense Captioning and Open-World Detection Pretraining](https://arxiv.org/abs/2303.02489)
  * [PROB: Probabilistic Objectness for Open World Object Detection](https://arxiv.org/abs/2212.01424)<br>:star:[code](https://github.com/orrzohar/PROB)
  * [CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection](https://arxiv.org/abs/2301.01970)
  * [Detecting Everything in the Open World: Towards Universal Object Detection](https://arxiv.org/abs/2303.11749)<br>:star:[code](https://github.com/zhenyuw16/UniDetector)<br>:thumbsup:[CVPR 2023 | 标注500类，检测7000类！清华大学等提出通用目标检测算法UniDetector](https://zhuanlan.zhihu.com/p/616328874)
* 目标定位
  * [LOCATE: Localize and Transfer Object Parts for Weakly Supervised Affordance Grounding](https://arxiv.org/abs/2303.09665)<br>:house:[project](https://reagan1311.github.io/locate/)
  * [Egocentric Audio-Visual Object Localization](http://arxiv.org/abs/2303.13471v1)
  * [Unsupervised Object Localization: Observing the Background To Discover Objects](https://openaccess.thecvf.com/content/CVPR2023/papers/Simeoni_Unsupervised_Object_Localization_Observing_the_Background_To_Discover_Objects_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/valeoai/FOUND)
* 3D OD
  * [Virtual Sparse Convolution for Multimodal 3D Object Detection](https://arxiv.org/abs/2303.02314)<br>:star:[code](https://github.com/hailanyi/VirConv)
  * [BEVHeight: A Robust Framework for Vision-Based Roadside 3D Object Detection](http://arxiv.org/abs/2303.08498)
  * [UniDistill: A Universal Cross-Modality Knowledge Distillation Framework for 3D Object Detection in Bird's-Eye View](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_UniDistill_A_Universal_Cross-Modality_Knowledge_Distillation_Framework_for_3D_Object_CVPR_2023_paper.pdf)
  * [PointDistiller: Structured Knowledge Distillation Towards Efficient and Compact 3D Detection](https://arxiv.org/abs/2205.11098)<br>:star:[code](https://github.com/RunpeiDong/PointDistiller)
  * [AShapeFormer: Semantics-Guided Object-Level Active Shape Encoding for 3D Object Detection via Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_AShapeFormer_Semantics-Guided_Object-Level_Active_Shape_Encoding_for_3D_Object_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ZechuanLi/AShapeFormer)
  * [BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks](https://openaccess.thecvf.com/content/CVPR2023/papers/Chi_BEV-SAN_Accurate_BEV_3D_Object_Detection_via_Slice_Attention_Networks_CVPR_2023_paper.pdf)
  * [3D Video Object Detection With Learnable Object-Centric Global Optimization](https://arxiv.org/abs/2303.15416)<br>:star:[code](https://github.com/jiaweihe1996/BA-Det)
  * [ConQueR: Query Contrast Voxel-DETR for 3D Object Detection](https://arxiv.org/abs/2212.07289)<br>:house:[project](https://benjin.me/projects/2022_conquer/)
  * [Bi-LRFusion: Bi-Directional LiDAR-Radar Fusion for 3D Dynamic Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Bi-LRFusion_Bi-Directional_LiDAR-Radar_Fusion_for_3D_Dynamic_Object_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/JessieW0806/Bi-LRFusion)
  * [Uni3D: A Unified Baseline for Multi-Dataset 3D Object Detection](https://arxiv.org/abs/2303.06880)<br>:star:[code](https://github.com/PJLab-ADG/3DTrans)
  * [Distilling Focal Knowledge From Imperfect Expert for 3D Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_Distilling_Focal_Knowledge_From_Imperfect_Expert_for_3D_Object_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/OpenPerceptionX/BEVPerception-Survey-Recipe)
  * [Are We Ready for Vision-Centric Driving Streaming Perception? The ASAP Benchmark](https://arxiv.org/abs/2212.08914)<br>:star:[code](https://github.com/JeffWang987/ASAP)
  * [Deep Dive Into Gradients: Better Optimization for 3D Object Detection With Gradient-Corrected IoU Supervision](https://openaccess.thecvf.com/content/CVPR2023/papers/Ming_Deep_Dive_Into_Gradients_Better_Optimization_for_3D_Object_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ming71/GCIoU-loss)
  * [AeDet: Azimuth-invariant Multi-view 3D Object Detection](https://arxiv.org/abs/2211.12501)<br>:star:[code](https://fcjian.github.io/aedet)
  * [FrustumFormer: Adaptive Instance-Aware Resampling for Multi-View 3D Detection](https://arxiv.org/abs/2301.04467)<br>:star:[code](https://github.com/Robertwyq/Frustum)
  * [PVT-SSD: Single-Stage 3D Object Detector With Point-Voxel Transformer](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_PVT-SSD_Single-Stage_3D_Object_Detector_With_Point-Voxel_Transformer_CVPR_2023_paper.pdf)
  * [itKD: Interchange Transfer-Based Knowledge Distillation for 3D Object Detection](https://arxiv.org/abs/2205.15531)
  * [OcTr: Octree-Based Transformer for 3D Object Detection](https://arxiv.org/abs/2303.12621)
  * [MoDAR: Using Motion Forecasting for 3D Object Detection in Point Cloud Sequences](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_MoDAR_Using_Motion_Forecasting_for_3D_Object_Detection_in_Point_CVPR_2023_paper.pdf)
  * [Semi-Supervised Stereo-Based 3D Object Detection via Cross-View Consensus](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Semi-Supervised_Stereo-Based_3D_Object_Detection_via_Cross-View_Consensus_CVPR_2023_paper.pdf)
  * [LinK: Linear Kernel for LiDAR-based 3D Perception](http://arxiv.org/abs/2303.16094v1)<br>:star:[code](https://github.com/MCG-NJU/LinK)
  * [PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds](http://arxiv.org/abs/2305.04925v1)
  * [PVT-SSD: Single-Stage 3D Object Detector with Point-Voxel Transformer](http://arxiv.org/abs/2305.06621v1)<br>:star:[code](https://github.com/Nightmare-n/PVT-SSD)
  * [3D Video Object Detection with Learnable Object-Centric Global Optimization](http://arxiv.org/abs/2303.15416v1)<br>:star:[code](https://github.com/jiaweihe1996/BA-Det)
  * [Density-Insensitive Unsupervised Domain Adaption on 3D Object Detection](http://arxiv.org/abs/2304.09446v1)<br>:star:[code](https://github.com/WoodwindHu/DTS)
  * [X3KD: Knowledge Distillation Across Modalities, Tasks and Stages for Multi-Camera 3D Object Detection](https://arxiv.org/abs/2303.02203)<br>:star:[code](https://youtu.be/1do9DPFmr38)
  * [Understanding the Robustness of 3D Object Detection with Bird's-Eye-View Representations in Autonomous Driving](http://arxiv.org/abs/2303.17297v1)
  * [Weakly Supervised Monocular 3D Object Detection using Multi-View Projection and Direction Consistency](https://arxiv.org/abs/2303.08686)<br>:star:[code](https://github.com/weakmono3d/weakmono3d)
  * [Viewpoint Equivariance for Multi-View 3D Object Detection](http://arxiv.org/abs/2303.14548v1)<br>:star:[code](https://github.com/TRI-ML/VEDet)
  * [Benchmarking Robustness of 3D Object Detection to Common Corruptions in Autonomous Driving](https://arxiv.org/abs/2303.11040)<br>:star:[code](https://github.com/kkkcx/3D_Corruptions_AD)
  * [Collaboration Helps Camera Overtake LiDAR in 3D Detection](http://arxiv.org/abs/2303.13560v1)<br>:star:[code](https://siheng-chen.github.io/dataset/CoPerception+)<br>:star:[code](https://github.com/MediaBrain-SJTU/CoCa3D)
  * [OcTr: Octree-based Transformer for 3D Object Detection](http://arxiv.org/abs/2303.12621v1)
  * [MSF: Motion-guided Sequential Fusion for Efficient 3D Object Detection from Point Cloud Sequences](https://arxiv.org/abs/2303.08316)<br>:star:[code](https://github.com/skyhehe123/MSF)
  * [MonoATT: Online Monocular 3D Object Detection with Adaptive Token Transformer](http://arxiv.org/abs/2303.13018v1)
  * [MV-JAR: Masked Voxel Jigsaw and Reconstruction for LiDAR-Based Self-Supervised Pre-Training](http://arxiv.org/abs/2303.13510v1)<br>:star:[code](https://github.com/SmartBot-PJLab/MV-JAR)
  * [NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations](http://arxiv.org/abs/2303.13483v1)
  * [VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking](https://arxiv.org/abs/2303.11301)<br>:star:[code](https://github.com/dvlab-research/VoxelNeXt)
  * [Bi3D: Bi-domain Active Learning for Cross-domain 3D Object Detection](https://arxiv.org/abs/2303.05886)<br>:star:[code](https://github.com/PJLabADG/3DTrans)
  * [LoGoNet: Towards Accurate 3D Object Detection with Local-to-Global Cross-Modal Fusion](https://arxiv.org/abs/2303.03595)<br>:star:[code](https://github.com/sankin97/LoGoNet)
  * [PiMAE: Point Cloud and Image Interactive Masked Autoencoders for 3D Object Detection](https://arxiv.org/abs/2303.08129)<br>:star:[code](https://github.com/BLVLab/PiMAE)
  * [CAPE: Camera View Position Embedding for Multi-View 3D Object Detection](https://arxiv.org/abs/2303.10209)<br>:star:[code](https://github.com/kaixinbear/CAPE)
  * [Uni3D: A Unified Baseline for Multi-dataset 3D Object Detection](https://arxiv.org/abs/2303.06880)<br>:star:[code](https://github.com/PJLab-ADG/3DTrans)
  * [Hierarchical Supervision and Shuffle Data Augmentation for 3D Semi-Supervised Object Detection](http://arxiv.org/abs/2304.01464v1)<br>:star:[code](https://github.com/azhuantou/HSSDA)3D目标检测
* 端到端目标检测
  * [Dense Distinct Query for End-to-End Object Detection](http://arxiv.org/abs/2303.12776v1)<br>:star:[code](https://github.com/jshilong/DDQ)
* 半监督目标检测
  * [Active Teacher for Semi-Supervised Object Detection](https://arxiv.org/abs/2303.08348)<br>:star:[code](https://github.com/HunterJ-Lin/ActiveTeacher)
  * [Consistent-Teacher: Towards Reducing Inconsistent Pseudo-Targets in Semi-Supervised Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Consistent-Teacher_Towards_Reducing_Inconsistent_Pseudo-Targets_in_Semi-Supervised_Object_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Adamdad/ConsistentTeacher)
  * [SOOD: Towards Semi-Supervised Oriented Object Detection](http://arxiv.org/abs/2304.04515v1)<br>:star:[code](https://github.com/HamPerdredes/SOOD)
  * [MixTeacher: Mining Promising Labels with Mixed Scale Teacher for Semi-Supervised Object Detection](https://arxiv.org/abs/2303.09061)<br>:star:[code](https://github.com/lliuz/MixTeacher)
* 弱监督目标检测
  * [DETR with Additional Global Aggregation for Cross-domain Weakly Supervised Object Detection](http://arxiv.org/abs/2304.07082v1)
* 小样本目标检测
  * [NIFF: Alleviating Forgetting in Generalized Few-Shot Object Detection via Neural Instance Feature Forging](https://arxiv.org/abs/2303.04958)
  * [Generating Features with Increased Crop-related Diversity for Few-Shot Object Detection](http://arxiv.org/abs/2304.05096v1)
  * [Meta-tuning Loss Functions and Data Augmentation for Few-shot Object Detection](http://arxiv.org/abs/2304.12161v1)
  * [DiGeo: Discriminative Geometry-Aware Learning for Generalized Few-Shot Object Detection](https://arxiv.org/abs/2303.09674)<br>:star:[code](https://github.com/Phoenix-V/DiGeo)
* 域适应目标检测
  * [2PCNet: Two-Phase Consistency Training for Day-to-Night Unsupervised Domain Adaptive Object Detection](http://arxiv.org/abs/2303.13853v1)
  * [AsyFOD: An Asymmetric Adaptation Paradigm for Few-Shot Domain Adaptive Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_AsyFOD_An_Asymmetric_Adaptation_Paradigm_for_Few-Shot_Domain_Adaptive_Object_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Hlings/AsyFOD)
  * [CIGAR: Cross-Modality Graph Reasoning for Domain Adaptive Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_CIGAR_Cross-Modality_Graph_Reasoning_for_Domain_Adaptive_Object_Detection_CVPR_2023_paper.pdf)
  * [Instance Relation Graph Guided Source-Free Domain Adaptive Object Detection](https://arxiv.org/abs/2203.15793)<br>:house:[project](https://viudomain.github.io/irg-sfda-web/)
  * [Domain Adaptive Detection Transformer With Information Fusion](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_DA-DETR_Domain_Adaptive_Detection_Transformer_With_Information_Fusion_CVPR_2023_paper.pdf)
  * [Harmonious Teacher for Cross-Domain Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Deng_Harmonious_Teacher_for_Cross-Domain_Object_Detection_CVPR_2023_paper.pdf)
  * [Contrastive Mean Teacher for Domain Adaptive Object Detectors](http://arxiv.org/abs/2305.03034v1)
* 显著目标检测
  * [Sketch2Saliency: Learning to Detect Salient Objects from Human Drawings](https://arxiv.org/abs/2303.11502)
  * [Pixels, Regions, and Objects: Multiple Enhancement for Salient Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Pixels_Regions_and_Objects_Multiple_Enhancement_for_Salient_Object_Detection_CVPR_2023_paper.pdf)
  * [Modeling the Distributional Uncertainty for Salient Object Detection Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_Modeling_the_Distributional_Uncertainty_for_Salient_Object_Detection_Models_CVPR_2023_paper.pdf)<br>:star:[code](https://npucvr.github.io/Distributional_uncer/)
  * [Test Time Adaptation With Regularized Loss for Weakly Supervised Salient Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Veksler_Test_Time_Adaptation_With_Regularized_Loss_for_Weakly_Supervised_Salient_CVPR_2023_paper.pdf)
* 红外目标检测
  * [Physically Adversarial Infrared Patches with Learnable Shapes and Locations](http://arxiv.org/abs/2303.13868v1)
  * [TOPLight: Lightweight Neural Networks With Task-Oriented Pretraining for Visible-Infrared Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_TOPLight_Lightweight_Neural_Networks_With_Task-Oriented_Pretraining_for_Visible-Infrared_Recognition_CVPR_2023_paper.pdf)
* 伪装目标检测
  * [Feature Shrinkage Pyramid for Camouflaged Object Detection with Transformers](http://arxiv.org/abs/2303.14816v1)<br>:star:[code](https://tzxiang.github.io/project/COD-FSPNet/index.html)<br>:star:[code](https://github.com/ZhouHuang23/FSPNet)
* 密集目标检测
  * [Ambiguity-Resistant Semi-Supervised Learning for Dense Object Detection](http://arxiv.org/abs/2303.14960v1)<br>:star:[code](https://github.com/PaddlePaddle/PaddleDetection)
* 协同目标检测
  * [Discriminative Co-Saliency and Background Mining Transformer for Co-Salient Object Detection](https://arxiv.org/abs/2305.00514)<br>:star:[code](https://github.com/dragonlee258079/DMT)
  * [Co-Salient Object Detection With Uncertainty-Aware Group Exchange-Masking](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Co-Salient_Object_Detection_With_Uncertainty-Aware_Group_Exchange-Masking_CVPR_2023_paper.pdf)
* 点云目标检测
  * [Open-Vocabulary Point-Cloud Object Detection Without 3D Annotation](https://arxiv.org/abs/2304.00788)
* 目标发现
  * [Object Discovery from Motion-Guided Tokens](http://arxiv.org/abs/2303.15555v1)
* 视频目标检测
  * [Feature Aggregated Queries for Transformer-Based Video Object Detectors](https://arxiv.org/abs/2303.08319)
* 小目标检测
  * [Dynamic Coarse-to-Fine Learning for Oriented Tiny Object Detection](http://arxiv.org/abs/2304.08876v1)<br>:star:[code](https://github.com/Chasel-Tsui/mmrotate-dcfl)
  * [Mapping Degeneration Meets Label Evolution: Learning Infrared Small Target Detection With Single Point Supervision](https://arxiv.org/abs/2304.01484)<br>:star:[code](https://github.com/XinyiYing/LESPS)
  * [Distilling Scale-Aware Knowledge in Small Object Detector](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_ScaleKD_Distilling_Scale-Aware_Knowledge_in_Small_Object_Detector_CVPR_2023_paper.pdf)
  * [LSTFE-Net:Long Short-Term Feature Enhancement Network for Video Small Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_LSTFE-NetLong_Short-Term_Feature_Enhancement_Network_for_Video_Small_Object_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/xiaojs18/LSTFE-Net)
  * 红外小目标检测
    * [Mapping Degeneration Meets Label Evolution: Learning Infrared Small Target Detection with Single Point Supervision](http://arxiv.org/abs/2304.01484v1)<br>:star:[code](https://github.com/XinyiYing/LESPS)
* 线段检测
  * [DeepLSD: Line Segment Detection and Refinement with Deep Image Gradients](https://arxiv.org/abs/2212.07766)<br>:star:[code](https://github.com/cvg/DeepLSD)
* 目标导航
  * [CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation](https://arxiv.org/abs/2203.10421)

<a name="4"/>

## 4.Image Captioning(图像字幕生成)
* 视频字幕
  * [Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning](https://arxiv.org/pdf/2302.14115.pdf)<br>:house:[project](https://antoyang.github.io/vid2seq.html)
  * [Text with Knowledge Graph Augmented Transformer for Video Captioning](http://arxiv.org/abs/2303.12423v1)
  * [Positive-Augmented Constrastive Learning for Image and Video Captioning Evaluation](http://arxiv.org/abs/2303.12112v1)<br>:star:[code](https://github.com/aimagelab/pacscore)
* 图像字幕
  * [Cross-Domain Image Captioning with Discriminative Finetuning](http://arxiv.org/abs/2304.01662v1)
  * [Crossing the Gap: Domain Generalization for Image Captioning](https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Crossing_the_Gap_Domain_Generalization_for_Image_Captioning_CVPR_2023_paper.pdf)
  * [Model-Agnostic Gender Debiased Image Captioning](http://arxiv.org/abs/2304.03693v1)
  * [A-CAP: Anticipation Captioning with Commonsense Knowledge](http://arxiv.org/abs/2304.06602v1)字幕
  * [Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation](https://arxiv.org/abs/2303.12112)<br>:star:[code](https://github.com/aimagelab/pacscore)
  * [HAAV: Hierarchical Aggregation of Augmented Views for Image Captioning](https://openaccess.thecvf.com/content/CVPR2023/papers/Kuo_HAAV_Hierarchical_Aggregation_of_Augmented_Views_for_Image_Captioning_CVPR_2023_paper.pdf)
  * [Semantic-Conditional Diffusion Networks for Image Captioning](https://arxiv.org/abs/2212.03099)<br>:star:[code](https://github.com/YehLi/xmodaler/tree/master/configs/image_caption/scdnet)
  * [ConZIC: Controllable Zero-Shot Image Captioning by Sampling-Based Polishing](https://arxiv.org/abs/2303.02437)
  * [SmallCap: Lightweight Image Captioning Prompted With Retrieval Augmentation](https://arxiv.org/abs/2209.15323)
* story generation(视觉故事生成)
  * [Make-A-Story: Visual Memory Conditioned Consistent Story Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Rahman_Make-a-Story_Visual_Memory_Conditioned_Consistent_Story_Generation_CVPR_2023_paper.pdf)  
* 3D密集字幕
  * [End-to-End 3D Dense Captioning With Vote2Cap-DETR](https://arxiv.org/abs/2301.02508) 
 
<a name="3"/>

## 3.Image Progress(低层图像处理、质量评价)
* [Initialization Noise in Image Gradients and Saliency Maps](https://openaccess.thecvf.com/content/CVPR2023/papers/Woerl_Initialization_Noise_in_Image_Gradients_and_Saliency_Maps_CVPR_2023_paper.pdf)
* [Learning a Practical SDR-to-HDRTV Up-conversion using New Dataset and Degradation Models](http://arxiv.org/abs/2303.13031v1)<br>:star:[code](https://github.com/AndreGuo/HDRTVDM)
* [Tunable Convolutions with Parametric Multi-Loss Optimization](http://arxiv.org/abs/2304.00898v1)
* 图像着色
  * [L-CoIns: Language-based Colorization with Instance Awareness](https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_L-CoIns_Language-Based_Colorization_With_Instance_Awareness_CVPR_2023_paper.pdf)
  * 色彩恢复
    * [GamutMLP: A Lightweight MLP for Color Loss Recovery](https://arxiv.org/abs/2304.11743)<br>:house:[project](https://gamut-mlp.github.io/)
* 阴影去除
  * [ShadowDiffusion: When Degradation Prior Meets Diffusion Model for Shadow Removal](https://arxiv.org/abs/2212.04711)
  * [DANI-Net: Uncalibrated Photometric Stereo by Differentiable Shadow Handling, Anisotropic Reflectance Modeling, and Neural Inverse Rendering](http://arxiv.org/abs/2303.15101v1)
* 图像恢复
  * [Efficient and Explicit Modelling of Image Hierarchies for Image Restoration](https://arxiv.org/pdf/2303.00748.pdf)<br>:star:[code](https://github.com/ofsoundof/GRL-Image-Restoration.git)
  * [Ingredient-Oriented Multi-Degradation Learning for Image Restoration](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Ingredient-Oriented_Multi-Degradation_Learning_for_Image_Restoration_CVPR_2023_paper.pdf)
  * [All-in-One Image Restoration for Unknown Degradations Using Adaptive Discriminative Filters for Specific Degradations](https://openaccess.thecvf.com/content/CVPR2023/papers/Park_All-in-One_Image_Restoration_for_Unknown_Degradations_Using_Adaptive_Discriminative_Filters_CVPR_2023_paper.pdf)
  * [Contrastive Semi-Supervised Learning for Underwater Image Restoration via Reliable Bank](https://arxiv.org/abs/2303.09101)<br>:star:[code](https://github.com/Huang-ShiRui/Semi-UIR)
  * [Burstormer: Burst Image Restoration and Enhancement Transformer](http://arxiv.org/abs/2304.01194v1)
  * [Generating Aligned Pseudo-Supervision from Non-Aligned Data for Image Restoration in Under-Display Camera](http://arxiv.org/abs/2304.06019v1)<br>:star:[code](https://github.com/jnjaby/AlignFormer)
  * [Generative Diffusion Prior for Unified Image Restoration and Enhancement](http://arxiv.org/abs/2304.01247v1)
  * [Bitstream-Corrupted JPEG Images are Restorable: Two-stage Compensation and Alignment Framework for Image Restoration](http://arxiv.org/abs/2304.06976v1)<br>:star:[code](https://github.com/wenyang001/Two-ACIR)
  * [Learning Distortion Invariant Representation for Image Restoration From a Causality Perspective](https://arxiv.org/abs/2303.06859)<br>:star:[code](https://github.com/lixinustc/Causal-IR-DIL)
  * [Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack](https://arxiv.org/abs/2304.11436)
  * [Robust Unsupervised StyleGAN Image Restoration](https://arxiv.org/abs/2302.06733)<br>:house:[project](https://lvsn.github.io/RobustUnsupervised/)
* 图像修复
  * [NUWA-LIP: Language-guided Image Inpainting with Defect-free VQGAN](https://openaccess.thecvf.com/content/CVPR2023/papers/Ni_NUWA-LIP_Language-Guided_Image_Inpainting_With_Defect-Free_VQGAN_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/kodenii/NUWA-LIP)
  * [Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting](https://arxiv.org/abs/2212.06909)
  * [SmartBrush: Text and Shape Guided Object Inpainting With Diffusion Model](https://arxiv.org/abs/2212.05034)
* 视频恢复
  * [A Simple Baseline for Video Restoration With Grouped Spatial-Temporal Shift](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_A_Simple_Baseline_for_Video_Restoration_With_Grouped_Spatial-Temporal_Shift_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/dasongli1/Shift-Net)
* 视频修复
  * [Deep Stereo Video Inpainting](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Deep_Stereo_Video_Inpainting_CVPR_2023_paper.pdf)
  * [Semi-Supervised Video Inpainting With Cycle Consistency Constraints](https://arxiv.org/abs/2208.06807)
* 图像照明
  * [Controllable Light Diffusion for Portraits](http://arxiv.org/abs/2305.04745v1)
* 图像质量评估
  * [Quality-aware Pre-trained Models for Blind Image Quality Assessment](https://arxiv.org/pdf/2303.00521.pdf)
  * [Blind Image Quality Assessment via Vision-Language Correspondence: A Multitask Learning Perspective](http://arxiv.org/abs/2303.14968v1)<br>:star:[code](https://github.com/zwx8981/LIQE)
  * [Quality-Aware Pre-Trained Models for Blind Image Quality Assessment](https://arxiv.org/abs/2303.00521)
  * [An Image Quality Assessment Dataset for Portraits](https://arxiv.org/abs/2304.05772)<br>:star:[code](https://github.com/DXOMARK-Research/PIQ2023)
  * [Re-IQA: Unsupervised Learning for Image Quality Assessment in the Wild](http://arxiv.org/abs/2304.00451v1)
* 去雾
  * [Video Dehazing via a Multi-Range Temporal Alignment Network with Physical Prior](https://arxiv.org/abs/2303.09757)<br>:star:[code](https://github.com/jiaqixuac/MAP-Net)
  * [Curricular Contrastive Regularization for Physics-aware Single Image Dehazing](http://arxiv.org/abs/2303.14218v1)
  * [Curricular Contrastive Regularization for Physics-Aware Single Image Dehazing](https://arxiv.org/abs/2303.14218)
  * [Efficient Frequency Domain-Based Transformers for High-Quality Image Deblurring](https://arxiv.org/abs/2211.12250)<br>:star:[code](https://github.com/kkkls/FFTformer)
  * [RIDCP: Revitalizing Real Image Dehazing via High-Quality Codebook Priors](http://arxiv.org/abs/2304.03994v1)
* 去雨
  * [Learning A Sparse Transformer Network for Effective Image Deraining](https://arxiv.org/abs/2303.11950)<br>:star:[code](https://github.com/cschenxiang/DRSformer) 
  * [SmartAssign: Learning a Smart Knowledge Assignment Strategy for Deraining and Desnowing](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_SmartAssign_Learning_a_Smart_Knowledge_Assignment_Strategy_for_Deraining_and_CVPR_2023_paper.pdf)<br>:house:[project](https://gitee.com/mindspore/models/tree/master/research/cv/SmartAssign)
* 去噪
  * [Masked Image Training for Generalizable Deep Image Denoising](http://arxiv.org/abs/2303.13132v1)
  * [Patch-Craft Self-Supervised Training for Correlated Image Denoising](https://arxiv.org/abs/2211.09919)
  * [Polarized Color Image Denoising](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Polarized_Color_Image_Denoising_CVPR_2023_paper.pdf)
  * [sRGB Real Noise Synthesizing With Neighboring Correlation-Aware Noise Model](https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_sRGB_Real_Noise_Synthesizing_With_Neighboring_Correlation-Aware_Noise_Model_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/xuan611/sRGB-Real-Noise-Synthesizing)
  * [Zero-Shot Noise2Noise: Efficient Image Denoising Without Any Data](https://arxiv.org/abs/2303.11253)<br>:house:[project](https://colab.research.google.com/drive/1i82nyizTdszyHkaHBuKPbWnTzao8HF9b)
  * [HouseDiffusion: Vector Floorplan Generation via a Diffusion Model With Discrete and Continuous Denoising](https://arxiv.org/abs/2211.13287)<br>:house:[project](https://aminshabani.github.io/housediffusion)
  * [Structure Aggregation for Cross-Spectral Stereo Image Guided Denoising](https://openaccess.thecvf.com/content/CVPR2023/papers/Sheng_Structure_Aggregation_for_Cross-Spectral_Stereo_Image_Guided_Denoising_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/lustrouselixir/SANet)
  * [Spatially Adaptive Self-Supervised Learning for Real-World Image Denoising](http://arxiv.org/abs/2303.14934v1)<br>:star:[code](https://github.com/nagejacob/SpatiallyAdaptiveSSID) 
  * [Spectral Enhanced Rectangle Transformer for Hyperspectral Image Denoising](https://arxiv.org/abs/2304.00844)<br>:star:[code](https://github.com/MyuLi/SERT)
  * [Real-time Controllable Denoising for Image and Video](http://arxiv.org/abs/2303.16425v1)
  * [LG-BPN: Local and Global Blind-Patch Network for Self-Supervised Real-World Denoising](http://arxiv.org/abs/2304.00534v1)<br>:star:[code](https://github.com/Wang-XIaoDingdd/LGBPN)
  * [Efficient View Synthesis and 3D-based Multi-Frame Denoising with Multiplane Feature Representations](http://arxiv.org/abs/2303.18139v1)
  * [Learning with Noisy labels via Self-supervised Adversarial Noisy Masking](https://arxiv.org/abs/2302.06805)去噪
  * [Learning from Noisy Labels with Decoupled Meta Label Purifier](https://arxiv.org/abs/2302.06810)去噪
* 去模糊
  * [HyperCUT: Video Sequence from a Single Blurry Image using Unsupervised Ordering](http://arxiv.org/abs/2304.01686v1)<br>:star:[code](https://github.com/VinAIResearch/HyperCUT.git)
  * [Neumann Network With Recursive Kernels for Single Image Defocus Deblurring](https://openaccess.thecvf.com/content/CVPR2023/papers/Quan_Neumann_Network_With_Recursive_Kernels_for_Single_Image_Defocus_Deblurring_CVPR_2023_paper.pdf)
  * [K3DN: Disparity-Aware Kernel Estimation for Dual-Pixel Defocus Deblurring](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_K3DN_Disparity-Aware_Kernel_Estimation_for_Dual-Pixel_Defocus_Deblurring_CVPR_2023_paper.pdf)
  * [Uncertainty-Aware Unsupervised Image Deblurring With Deep Residual Prior](https://arxiv.org/abs/2210.05361)
  * [$\text{DC}^2$: Dual-Camera Defocus Control by Learning to Refocus](http://arxiv.org/abs/2304.03285v1)<br>:star:[code](https://defocus-control.github.io)去模糊
  * [Self-Supervised Non-Uniform Kernel Estimation With Flow-Based Motion Prior for Blind Image Deblurring](https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_Self-Supervised_Non-Uniform_Kernel_Estimation_With_Flow-Based_Motion_Prior_for_Blind_CVPR_2023_paper.pdf)<br>:house:[project](https://see.xidian.edu.cn/faculty/wsdong/Projects/UFPNet.htm)
  * [Joint Video Multi-Frame Interpolation and Deblurring Under Unknown Exposure Time](https://arxiv.org/abs/2303.15043)<br>:star:[code](https://github.com/shangwei5/VIDUE)
  * [Event-Based Frame Interpolation With Ad-Hoc Deblurring](https://arxiv.org/abs/2301.05191)
* 去鬼影
  * [A Unified HDR Imaging Method with Pixel and Patch Level](https://arxiv.org/abs/2304.06943)
  * [SMAE: Few-shot Learning for HDR Deghosting with Saturation-Aware Masked Autoencoders](http://arxiv.org/abs/2304.06914v1)
* 去反射光斑
  * [Nighttime Smartphone Reflective Flare Removal Using Optical Center Symmetry Prior](http://arxiv.org/abs/2303.15046v1)<br>:star:[code](https://github.com/ykdai/BracketFlare)
* image deweathering
  * [WeatherStream: Light Transport Automation of Single Image Deweathering](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_WeatherStream_Light_Transport_Automation_of_Single_Image_Deweathering_CVPR_2023_paper.pdf)<br>:house:[project](http://visual.ee.ucla.edu/wstream.htm/)
* 图像缩放
  * [HyperThumbnail: Real-time 6K Image Rescaling with Rate-distortion Optimization](http://arxiv.org/abs/2304.01064v1)<br>:star:[code](https://github.com/AbnerVictor/HyperThumbnail)
  * [Real-time 6K Image Rescaling with Rate-distortion Optimization](https://arxiv.org/abs/2304.01064)<br>:star:[code](https://github.com/AbnerVictor/HyperThumbnail)
* 瞬间恢复与增强
  * [Gated Multi-Resolution Transfer Network for Burst Restoration and Enhancement](http://arxiv.org/abs/2304.06703v1)
* 图像增强
  * [Learning Semantic-Aware Knowledge Guidance for Low-Light Image Enhancement](http://arxiv.org/abs/2304.07039v1)
  * [Realistic Saliency Guided Image Enhancement](https://openaccess.thecvf.com/content/CVPR2023/papers/Miangoleh_Realistic_Saliency_Guided_Image_Enhancement_CVPR_2023_paper.pdf)
  * [Learning a Simple Low-Light Image Enhancer From Paired Low-Light Instances](https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Learning_a_Simple_Low-Light_Image_Enhancer_From_Paired_Low-Light_Instances_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/zhenqifu/PairLIE)
  * [Low-Light Image Enhancement via Structure Modeling and Guidance](https://arxiv.org/abs/2305.05839)
  * [You Do Not Need Additional Priors or Regularizers in Retinex-Based Low-Light Image Enhancement](https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_You_Do_Not_Need_Additional_Priors_or_Regularizers_in_Retinex-Based_CVPR_2023_paper.pdf)
* 图像和谐化
  * [LEMaRT: Label-Efficient Masked Region Transform for Image Harmonization](http://arxiv.org/abs/2304.13166v1)
  * [Semi-supervised Parametric Real-world Image Harmonization](https://arxiv.org/abs/2303.00157)
  * [PCT-Net: Full Resolution Image Harmonization Using Pixel-Wise Color Transformations](https://openaccess.thecvf.com/content/CVPR2023/papers/Guerreiro_PCT-Net_Full_Resolution_Image_Harmonization_Using_Pixel-Wise_Color_Transformations_CVPR_2023_paper.pdf)
* 图像曝光校正
  * [Decoupling-and-Aggregating for Image Exposure Correction](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Decoupling-and-Aggregating_for_Image_Exposure_Correction_CVPR_2023_paper.pdf)
* 物体移除
  * [Automatic High Resolution Wire Segmentation and Removal](https://arxiv.org/abs/2304.00221)<br>:star:[code](https://github.com/adobe-research/auto-wire-removal)
* Image Decomposition 
  * [Light Source Separation and Intrinsic Image Decomposition Under AC Illumination](https://openaccess.thecvf.com/content/CVPR2023/papers/Yoshida_Light_Source_Separation_and_Intrinsic_Image_Decomposition_Under_AC_Illumination_CVPR_2023_paper.pdf)
  * [Context-aware Pretraining for Efficient Blind Image Decomposition](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Context-Aware_Pretraining_for_Efficient_Blind_Image_Decomposition_CVPR_2023_paper.pdf)
* 图像重建
  * [Raw Image Reconstruction With Learned Compact Metadata](https://arxiv.org/abs/2302.12995)<br>:star:[code](https://github.com/wyf0912/R2LCM)
  * [Catch Missing Details: Image Reconstruction with Frequency Augmented Variational Autoencoder](https://arxiv.org/abs/2305.02541)
  * [High-Resolution Image Reconstruction With Latent Diffusion Models From Human Brain Activity](https://openaccess.thecvf.com/content/CVPR2023/papers/Takagi_High-Resolution_Image_Reconstruction_With_Latent_Diffusion_Models_From_Human_Brain_CVPR_2023_paper.pdf)<br>:house:[project](https://sites.google.com/view/stablediffusion-with-brain/)
  * [PermutoSDF: Fast Multi-View Reconstruction with Implicit Surfaces using Permutohedral Lattices](https://arxiv.org/abs/2211.12562)<br>:house:[project](https://radualexandru.github.io/permuto_sdf)
* 文本驱动的图像处理
  * [DeltaEdit: Exploring Text-Free Training for Text-Driven Image Manipulation](https://arxiv.org/abs/2303.06285)<br>:star:[code](https://github.com/Yueming6568/DeltaEdit)
* 运动模糊
  * [Self-supervised Blind Motion Deblurring with Deep Expectation Maximization](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Self-Supervised_Blind_Motion_Deblurring_With_Deep_Expectation_Maximization_CVPR_2023_paper.pdf)
* 图像裁剪
  * [Image Cropping With Spatial-Aware Feature and Rank Consistency](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Image_Cropping_With_Spatial-Aware_Feature_and_Rank_Consistency_CVPR_2023_paper.pdf)
* 图像重照明
  * [Weakly-supervised Single-view Image Relighting](https://arxiv.org/abs/2303.13852)<br>:house:[project](https://renjiaoyi.github.io/relighting/)
* 模糊帧插值
  * [Event-Based Blurry Frame Interpolation Under Blind Exposure](https://openaccess.thecvf.com/content/CVPR2023/papers/Weng_Event-Based_Blurry_Frame_Interpolation_Under_Blind_Exposure_CVPR_2023_paper.pdf)

<a name="2"/>

## 2.Image Segmentation(图像分割)
* [Towards Open-World Segmentation of Parts](https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_Towards_Open-World_Segmentation_of_Parts_CVPR_2023_paper.pdf)
* [Heat Diffusion Based Multi-Scale and Geometric Structure-Aware Transformer for Mesh Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wong_Heat_Diffusion_Based_Multi-Scale_and_Geometric_Structure-Aware_Transformer_for_Mesh_CVPR_2023_paper.pdf)
* [MOVES: Manipulated Objects in Video Enable Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Higgins_MOVES_Manipulated_Objects_in_Video_Enable_Segmentation_CVPR_2023_paper.pdf)
* [Decoupled Semantic Prototypes Enable Learning From Diverse Annotation Types for Semi-Weakly Segmentation in Expert-Driven Domains](https://openaccess.thecvf.com/content/CVPR2023/papers/Reiss_Decoupled_Semantic_Prototypes_Enable_Learning_From_Diverse_Annotation_Types_for_CVPR_2023_paper.pdf)
* [Compositor: Bottom-Up Clustering and Compositing for Robust Part and Object Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/He_Compositor_Bottom-Up_Clustering_and_Compositing_for_Robust_Part_and_Object_CVPR_2023_paper.pdf)
* [VectorFloorSeg: Two-Stream Graph Attention Network for Vectorized Roughcast Floorplan Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_VectorFloorSeg_Two-Stream_Graph_Attention_Network_for_Vectorized_Roughcast_Floorplan_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/DrZiji/VecFloorSeg)
* [Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervisio](https://arxiv.org/abs/2303.03361)
* [OneFormer: One Transformer To Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220)<br>:house:[project](https://praeclarumjj3.github.io/oneformer)
* [PanelNet: Understanding 360 Indoor Environment via Panel Representation](http://arxiv.org/abs/2305.09078v1)
* [AutoFocusFormer: Image Segmentation off the Grid](http://arxiv.org/abs/2304.12406v1)
* [MP-Former: Mask-Piloted Transformer for Image Segmentation](https://arxiv.org/abs/2303.07336)<br>:star:[code](https://github.com/IDEA-Research/MP-Former)
* [Explicit Visual Prompting for Low-Level Structure Segmentations](https://arxiv.org/abs/2303.10883)<br>:star:[code](https://github.com/NiFangBaAGe/Explict-Visual-Prompt)
* [Focused and Collaborative Feedback Integration for Interactive Image Segmentation](https://arxiv.org/abs/2303.11880)<br>:star:[code](https://github.com/veizgyauzgyauz/FCFI)
* [FreeSeg: Unified, Universal and Open-Vocabulary Image Segmentation](http://arxiv.org/abs/2303.17225v1)<br>:house:[project](https://FreeSeg.github.io)<br>在 VIS、VOS、MOTS 三个下游视频分割任务的五个数据集上，将 InstMove 插入到现有 SOTA 模型可以进一步带来 1~5 个点的提升。
* [MED-VT: Multiscale Encoder-Decoder Video Transformer with Application to Object Segmentation](http://arxiv.org/abs/2304.05930v1)分割
* 零样本分割
  * [Primitive Generation and Semantic-Related Alignment for Universal Zero-Shot Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/He_Primitive_Generation_and_Semantic-Related_Alignment_for_Universal_Zero-Shot_Segmentation_CVPR_2023_paper.pdf)<br>:house:[project](https://henghuiding.github.io/PADing)<br>:thumbsup:[CVPR23 | 浙大、NTU提出零样本通用分割框架PADing](https://mp.weixin.qq.com/s/IngwwSYXKQbkAYOI7NaXJw)
* 3D分割
  * [EFEM: Equivariant Neural Field Expectation Maximization for 3D Object Segmentation Without Scene Supervision](http://arxiv.org/abs/2303.15440v1)<br>:house:[project](https://www.cis.upenn.edu/~leijh/projects/efem)
* 全景分割
  * [CoMFormer: Continual Learning in Semantic and Panoptic Segmentation](https://arxiv.org/abs/2211.13999)
  * [Center Focusing Network for Real-Time LiDAR Panoptic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Center_Focusing_Network_for_Real-Time_LiDAR_Panoptic_Segmentation_CVPR_2023_paper.pdf)
  * [Context-Aware Relative Object Queries To Unify Video Instance and Panoptic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Choudhuri_Context-Aware_Relative_Object_Queries_To_Unify_Video_Instance_and_Panoptic_CVPR_2023_paper.pdf)
  * 实时全景分割
    * [You Only Segment Once: Towards Real-Time Panoptic Segmentation](http://arxiv.org/abs/2303.14651v1)<br>:star:[code](https://github.com/hujiecpp/YOSO)
  * 域适应全景分割
    * [UniDAformer: Unified Domain Adaptive Panoptic Segmentation Transformer via Hierarchical Mask Calibration](https://arxiv.org/abs/2206.15083)
  * 开放词汇全景分割
    * [Open-Vocabulary Panoptic Segmentation With Text-to-Image Diffusion Models](https://arxiv.org/abs/2303.04803)<br>:star:[code](https://github.com/NVlabs/ODISE)
* 实例分割
  * [DynaMask: Dynamic Mask Selection for Instance Segmentation](https://arxiv.org/abs/2303.07868)<br>:star:[code](https://github.com/lslrh/DynaMask)
  * [Boosting Low-Data Instance Segmentation by Unsupervised Pre-Training With Saliency Prompt](https://arxiv.org/abs/2302.01171)
  * [Cut and Learn for Unsupervised Object Detection and Instance Segmentation](https://arxiv.org/abs/2301.11320)<br>:star:[code](https://github.com/facebookresearch/CutLER)
  * [PartDistillation: Learning Parts From Instance Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_PartDistillation_Learning_Parts_From_Instance_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/PartDistillation)
  * [Iterative Next Boundary Detection for Instance Segmentation of Tree Rings in Microscopy Images of Shrub Cross Sections](https://arxiv.org/abs/2212.03022)<br>:star:[code](http://github.com/alexander-g/INBD)
  * [AttentionShift: Iteratively Estimated Part-Based Attention Map for Pointly Supervised Instance Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_AttentionShift_Iteratively_Estimated_Part-Based_Attention_Map_for_Pointly_Supervised_Instance_CVPR_2023_paper.pdf)
  * [DoNet: Deep De-overlapping Network for Cytology Instance Segmentation](http://arxiv.org/abs/2303.14373v1)<br>:star:[code](https://github.com/DeepDoNet/DoNet)
  * [FastInst: A Simple Query-Based Model for Real-Time Instance Segmentation](https://arxiv.org/abs/2303.08594)<br>:star:[code](https://github.com/junjiehe96/FastInst)
  * [Camouflaged Instance Segmentation via Explicit De-Camouflaging](https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Camouflaged_Instance_Segmentation_via_Explicit_De-Camouflaging_CVPR_2023_paper.pdf)
  * 无监督实例分割
    * [Exemplar-FreeSOLO: Enhancing Unsupervised Instance Segmentation With Exemplars](https://openaccess.thecvf.com/content/CVPR2023/papers/Ishtiak_Exemplar-FreeSOLO_Enhancing_Unsupervised_Instance_Segmentation_With_Exemplars_CVPR_2023_paper.pdf)
  * 弱监督实例分割
    * [SIM: Semantic-aware Instance Mask Generation for Box-Supervised Instance Segmentation](https://arxiv.org/abs/2303.08578)<br>:star:[code](https://github.com/lslrh/SIM)
    * [BoxTeacher: Exploring High-Quality Pseudo Labels for Weakly Supervised Instance Segmentation](https://arxiv.org/abs/2210.05174)<br>:star:[code](https://github.com/hustvl/BoxTeacher)
    * [The Devil is in the Points: Weakly Semi-Supervised Instance Segmentation via Point-Guided Mask Representation](http://arxiv.org/abs/2303.15062v1)<br>:star:[code](https://github.com/clovaai/PointWSSIS)
  * 开放词汇实例分割
    * [Mask-free OVIS: Open-Vocabulary Instance Segmentation without Manual Mask Annotations](http://arxiv.org/abs/2303.16891v1)<br>:star:[code](https://vibashan.github.io/ovis-web/)
  * 零样本实例分割
    * [Semantic-Promoted Debiasing and Background Disambiguation for Zero-Shot Instance Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/He_Semantic-Promoted_Debiasing_and_Background_Disambiguation_for_Zero-Shot_Instance_Segmentation_CVPR_2023_paper.pdf)<br>:house:[project](https://henghuiding.github.io/D2Zero)
* 语义分割
  * [IFSeg: Image-free Semantic Segmentation via Vision-Language Model](http://arxiv.org/abs/2303.14396v1)<br>:star:[code](https://github.com/alinlab/ifseg)
  * [Less Is More: Reducing Task and Model Complexity for 3D Point Cloud Semantic Segmentation](http://arxiv.org/abs/2303.11203)
  * [SemiCVT: Semi-Supervised Convolutional Vision Transformer for Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_SemiCVT_Semi-Supervised_Convolutional_Vision_Transformer_for_Semantic_Segmentation_CVPR_2023_paper.pdf)
  * [PIDNet: A Real-Time Semantic Segmentation Network Inspired by PID Controllers](https://arxiv.org/abs/2206.02066)
  * [Principles of Forgetting in Domain-Incremental Semantic Segmentation in Adverse Weather Conditions](https://arxiv.org/abs/2303.14115)
  * [PeakConv: Learning Peak Receptive Field for Radar Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_PeakConv_Learning_Peak_Receptive_Field_for_Radar_Semantic_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/zlw9161/PKC)
  * [Understanding Imbalanced Semantic Segmentation Through Neural Collapse](https://arxiv.org/abs/2301.01100)<br>:star:[code](https://github.com/dvlab-research/Imbalanced-Learning)
  * [Geometry and Uncertainty-Aware 3D Point Cloud Class-Incremental Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Geometry_and_Uncertainty-Aware_3D_Point_Cloud_Class-Incremental_Semantic_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/leolyj/3DPC-CISS)
  * [Single Domain Generalization for LiDAR Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Single_Domain_Generalization_for_LiDAR_Semantic_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/gzgzys9887/DGLSS)
  * [FedSeg: Class-Heterogeneous Federated Learning for Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Miao_FedSeg_Class-Heterogeneous_Federated_Learning_for_Semantic_Segmentation_CVPR_2023_paper.pdf)
  * [Proximal Splitting Adversarial Attack for Semantic Segmentation](https://arxiv.org/abs/2206.07179)<br>:star:[code](https://github.com/jeromerony/alma_prox_segmentation)
  * [On Calibrating Semantic Segmentation Models: Analyses and an Algorithm](https://arxiv.org/abs/2212.12053)
  * [Incrementer: Transformer for Class-Incremental Semantic Segmentation With Knowledge Distillation Focusing on Old Class](https://openaccess.thecvf.com/content/CVPR2023/papers/Shang_Incrementer_Transformer_for_Class-Incremental_Semantic_Segmentation_With_Knowledge_Distillation_Focusing_CVPR_2023_paper.pdf)
  * [Content-Aware Token Sharing for Efficient Semantic Segmentation With Vision Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Content-Aware_Token_Sharing_for_Efficient_Semantic_Segmentation_With_Vision_Transformers_CVPR_2023_paper.pdf)
  * [Endpoints Weight Fusion for Class Incremental Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_Endpoints_Weight_Fusion_for_Class_Incremental_Semantic_Segmentation_CVPR_2023_paper.pdf)
  * [Sparsely Annotated Semantic Segmentation With Adaptive Gaussian Mixtures](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Sparsely_Annotated_Semantic_Segmentation_With_Adaptive_Gaussian_Mixtures_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Luffy03/AGMM-SASS)
  * [ACSeg: Adaptive Conceptualization for Unsupervised Semantic Segmentation](https://arxiv.org/abs/2210.05944)
  * [Improving Robustness of Semantic Segmentation to Motion-Blur Using Class-Centric Augmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Aakanksha_Improving_Robustness_of_Semantic_Segmentation_to_Motion-Blur_Using_Class-Centric_Augmentation_CVPR_2023_paper.pdf)
  * [Dynamic Focus-Aware Positional Queries for Semantic Segmentation](https://arxiv.org/abs/2204.01244)<br>:star:[code](https://github.com/ziplab/FASeg)
  * [Continual Semantic Segmentation With Automatic Memory Sample Selection](https://arxiv.org/abs/2304.05015)
  * [Learning Open-Vocabulary Semantic Segmentation Models From Natural Language Supervision](https://arxiv.org/abs/2301.09121)
  * [Dynamically Instance-Guided Adaptation: A Backward-Free Approach for Test-Time Domain Adaptive Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Dynamically_Instance-Guided_Adaptation_A_Backward-Free_Approach_for_Test-Time_Domain_Adaptive_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Waybaba/DIGA)
  * [Federated Incremental Semantic Segmentation](http://arxiv.org/abs/2304.04620v1)<br>:star:[code](https://github.com/JiahuaDong/FISS)
  * [Delivering Arbitrary-Modal Semantic Segmentation](https://arxiv.org/abs/2303.01480)<br>:star:[code](https://jamycheung.github.io/DELIVER.html)
  * [Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation](https://arxiv.org/pdf/2302.14250.pdf)
  * [A Simple Framework for Text-Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_A_Simple_Framework_for_Text-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/muyangyi/SimSeg)<br>在 PASCAL VOC 2012、PASCAL Context 和 COCO 数据集上的表现明显优于之前最先进的方法。
  * [Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors](https://arxiv.org/pdf/2302.14746.pdf)
  * [Generative Semantic Segmentation](https://arxiv.org/abs/2303.11316)<br>:star:[code](https://github.com/fudan-zvg/GSS)
  * [Reliability in Semantic Segmentation: Are We on the Right Track?](https://arxiv.org/abs/2303.11298)<br>:star:[code](https://github.com/naver/relis)
  * [Both Style and Distortion Matter: Dual-Path Unsupervised Domain Adaptation for Panoramic Semantic Segmentation](http://arxiv.org/abs/2303.14360v1)
  * [Less is More: Reducing Task and Model Complexity for 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2303.11203)<br>:star:[code](https://github.com/l1997i/lim3d;)
  * [Instant Domain Augmentation for LiDAR Semantic Segmentation](http://arxiv.org/abs/2303.14378v1)<br>:house:[project](http://cvlab.postech.ac.kr/research/LiDomAug)
  * [Delving into Shape-aware Zero-shot Semantic Segmentation](http://arxiv.org/abs/2304.08491v1)<br>:star:[code](https://github.com/Liuxinyv/SAZS)
  * 开放词汇语义分割
    * [Open Vocabulary Semantic Segmentation With Patch Aligned Contrastive Learning](https://arxiv.org/abs/2212.04994)
    * [Open-Vocabulary Semantic Segmentation With Mask-Adapted CLIP](https://arxiv.org/abs/2210.04150)<br>:house:[project](https://jeff-liangf.github.io/projects/ovseg)
    * [Side Adapter Network for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2302.12242)<br>:star:[code](https://github.com/MendelXu/SAN)<br>:thumbsup:[CVPR2023 Highlight | Side Adapter Network – 极致轻薄却性能强劲的开放词汇语义分割器](https://mp.weixin.qq.com/s/yBmBniIMF9JG0RG6GdSQng)
  * 开放世界语义分割
    * [Learning to Generate Text-grounded Mask for Open-world Semantic Segmentation from Only Image-Text Pairs](https://arxiv.org/abs/2212.00785)<br>:star:[code](https://github.com/kakaobrain/tcl)
  * 域适应语义分割
    * [DiGA: Distil to Generalize and then Adapt for Domain Adaptive Semantic Segmentation](http://arxiv.org/abs/2304.02222v1)<br>:star:[code](https://github.com/fy-vision/DiGA)
    * [Weakly-Supervised Domain Adaptive Semantic Segmentation With Prototypical Contrastive Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Das_Weakly-Supervised_Domain_Adaptive_Semantic_Segmentation_With_Prototypical_Contrastive_Learning_CVPR_2023_paper.pdf)
    * [Continuous Pseudo-Label Rectified Domain Adaptive Semantic Segmentation With Implicit Neural Representations](https://openaccess.thecvf.com/content/CVPR2023/papers/Gong_Continuous_Pseudo-Label_Rectified_Domain_Adaptive_Semantic_Segmentation_With_Implicit_Neural_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ETHRuiGong/IR2F)
  * 域泛化语义分割
    * [HGFormer: Hierarchical Grouping Transformer for Domain Generalized Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/dingjiansw101/HGFormer)
    * [Style Projected Clustering for Domain Generalized Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Style_Projected_Clustering_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://gitee.com/mindspore/models/tree/master/research/cv/SPC-Net)
  * 小样本语义分割
    * [MIANet: Aggregating Unbiased Instance and General Information for Few-Shot Semantic Segmentation](https://arxiv.org/abs/2305.13864)<br>:star:[code](https://github.com/Aldrich2y/MIANet)
    * [A Strong Baseline for Generalized Few-Shot Semantic Segmentation](https://arxiv.org/abs/2211.14126)<br>:star:[code](https://github.com/sinahmr/DIaM)
  * 无监督语义分割
     * [Leveraging Hidden Positives for Unsupervised Semantic Segmentation](http://arxiv.org/abs/2303.15014v1)<br>:star:[code](https://github.com/hynnsk/HP)
  * 半监督语义分割
    * [Conflict-Based Cross-View Consistency for Semi-Supervised Semantic Segmentation](https://arxiv.org/pdf/2303.01276.pdf)<br>:star:[code](https://github.com/xiaoyao3302/CCVC)  
    * [Hunting Sparsity: Density-Guided Contrastive Learning for Semi-Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Hunting_Sparsity_Density-Guided_Contrastive_Learning_for_Semi-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Gavinwxy/DGCL)
    * [Instance-Specific and Model-Adaptive Supervision for Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2211.11335)
    * [LaserMix for Semi-Supervised LiDAR Semantic Segmentation](https://arxiv.org/abs/2207.00026)<br>:star:[code](https://github.com/ldkong1205/LaserMix)
    * [Augmentation Matters: A Simple-Yet-Effective Approach to Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2212.04976)
  * 弱监督语义分割
    * [Token Contrast for Weakly-Supervised Semantic Segmentation](https://arxiv.org/pdf/2303.01267.pdf)<br>:star:[code](https://github.com/rulixiang/ToCo)
    * [Boundary-Enhanced Co-Training for Weakly Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Rong_Boundary-Enhanced_Co-Training_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ShenghaiRong/BECO)
    * [Out-of-Candidate Rectification for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2211.12268)
    * [Weakly Supervised Semantic Segmentation via Adversarial Learning of Classifier and Reconstructor](https://openaccess.thecvf.com/content/CVPR2023/papers/Kweon_Weakly_Supervised_Semantic_Segmentation_via_Adversarial_Learning_of_Classifier_and_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/sangrockEG/ACR)
  * 点云语义分割
    * [Novel Class Discovery for 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2303.11610)<br>:star:[code](https://github.com/LuigiRiz/NOPS)
  * 零样本语义分割
    * [Delving Into Shape-Aware Zero-Shot Semantic Segmentation](https://arxiv.org/abs/2304.08491)<br>:star:[code](https://github.com/Liuxinyv/SAZS)
  * 长尾语义分割
    * [Balancing Logit Variation for Long-Tailed Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Balancing_Logit_Variation_for_Long-Tailed_Semantic_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/grantword8/BLV)
  * 3D 语义分割
    * [Seg3D: Multi-modal 3D Semantic Segmentation for Autonomous Driving](https://arxiv.org/abs/2303.08600)<br>:star:[code](https://github.com/jialeli1/lidarseg3d)
    * [3D Semantic Segmentation in the Wild: Learning Generalized Models for Adverse-Condition Point Clouds](http://arxiv.org/abs/2304.00690v1)<br>:star:[code](https://github.com/xiaoaoran/SemanticSTF)
  * 开集语义分割
    * [Open-Set Semantic Segmentation for Point Clouds via Adversarial Prototype Framework](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Open-Set_Semantic_Segmentation_for_Point_Clouds_via_Adversarial_Prototype_Framework_CVPR_2023_paper.pdf)
* 交互式分割
  * [Interactive Segmentation as Gaussian Process Classification](https://arxiv.org/pdf/2302.14578.pdf)<br>:star:[code](https://github.com/zmhhmz/GPCIS_CVPR2023)
  * [Interactive Segmentation of Radiance Fields](https://arxiv.org/abs/2212.13545)<br>:house:[project](https://rahul-goel.github.io/isrf/)
  * [Efficient Mask Correction for Click-Based Interactive Image Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Efficient_Mask_Correction_for_Click-Based_Interactive_Image_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/feiaxyt/EMC-Click)
* 小样本分割
  * [Hierarchical Dense Correlation Distillation for Few-Shot Segmentation](http://arxiv.org/abs/2303.14652v1)<br>:star:[code](https://github.com/Pbihao/HDMNet)
  * [Rethinking the Correlation in Few-Shot Segmentation: A Buoys View](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Rethinking_the_Correlation_in_Few-Shot_Segmentation_A_Buoys_View_CVPR_2023_paper.pdf)
* VSS
  * [Efficient Semantic Segmentation by Altering Resolutions for Compressed Videos](https://arxiv.org/abs/2303.07224)<br>:star:[code](https://github.com/THU-LYJ-Lab/AR-Seg)
  * [Simultaneously Short- and Long-Term Temporal Modeling for Semi-Supervised Video Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Lao_Simultaneously_Short-_and_Long-Term_Temporal_Modeling_for_Semi-Supervised_Video_Semantic_CVPR_2023_paper.pdf)
  * [Spatio-Temporal Pixel-Level Contrastive Learning-based Source-Free Domain Adaptation for Video Semantic Segmentation](http://arxiv.org/abs/2303.14361v1)<br>:star:[code](https://github.com/shaoyuanlo/STPL)
* VOS
  * [InstMove: Instance Motion for Object-centric Video Segmentation](https://arxiv.org/abs/2303.08132)<br>:star:[code](https://github.com/wjf5203/VNext)
  * [Breaking the "Object" in Video Object Segmentation](https://arxiv.org/abs/2212.06200)
  * [Look Before You Match: Instance Understanding Matters in Video Object Segmentation](https://arxiv.org/abs/2212.06826)
  * [MobileVOS: Real-Time Video Object Segmentation Contrastive Learning meets Knowledge Distillation](https://arxiv.org/abs/2303.07815)
  * [Boosting Video Object Segmentation via Space-time Correspondence Learning](http://arxiv.org/abs/2304.06211v1)<br>:star:[code](https://github.com/wenguanwang/VOS_Correspondence)
  * [Bootstrapping Objectness from Videos by Relaxed Common Fate and Visual Grouping](http://arxiv.org/abs/2304.08025v1)VOS
  * [Unified Mask Embedding and Correspondence Learning for Self-Supervised Video Segmentation](https://arxiv.org/abs/2303.10100)<br>:star:[code](https://github.com/0liliulei/Mask-VOS)
  * [Two-shot Video Object Segmentation](https://arxiv.org/abs/2303.12078)<br>:star:[code](https://github.com/yk-pku/Two-shot-Video-Object-Segmentation)
  * [Bootstrapping Objectness from Videos by Relaxed Common Fate and Visual Grouping](https://arxiv.org/abs/2304.08025)
 * VIS
   * [Mask-Free Video Instance Segmentation](http://arxiv.org/abs/2303.15904v1)<br>:star:[code](https://github.com/SysCV/MaskFreeVis)<br>:house:[project](http://vis.xyz/pub/maskfreevis)<br>:star:[code](https://github.com/SysCV/MaskFreeVis)
   * [MDQE: Mining Discriminative Query Embeddings to Segment Occluded Instances on Challenging Videos](https://arxiv.org/abs/2303.14395)<br>:star:[code](https://github.com/MinghanLi/MDQE_CVPR2023)
   * [A Generalized Framework for Video Instance Segmentation](https://arxiv.org/abs/2211.08834)<br>:star:[code](https://github.com/miranheo/GenVIS)
* 场景理解
  * [FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding](http://arxiv.org/abs/2304.02135v1)
  * [SceneTrilogy: On Human Scene-Sketch and its Complementarity with Photo and Text](https://arxiv.org/abs/2204.11964)<br>:house:[project](http://www.pinakinathc.me/scenetrilogy)
  * [Movies2Scenes: Using Movie Metadata To Learn Scene Representation](http://arxiv.org/abs/2202.10650)
  * [Seeing With Sound: Long-range Acoustic Beamforming for Multimodal Scene Understanding](https://openaccess.thecvf.com/content/CVPR2023/papers/Chakravarthula_Seeing_With_Sound_Long-range_Acoustic_Beamforming_for_Multimodal_Scene_Understanding_CVPR_2023_paper.pdf)
  * [Single View Scene Scale Estimation Using Scale Field](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Single_View_Scene_Scale_Estimation_Using_Scale_Field_CVPR_2023_paper.pdf)
  * [Neural Part Priors: Learning To Optimize Part-Based Object Completion in RGB-D Scans](https://openaccess.thecvf.com/content/CVPR2023/papers/Bokhovkin_Neural_Part_Priors_Learning_To_Optimize_Part-Based_Object_Completion_in_CVPR_2023_paper.pdf)
  * 3D 场景理解
    * [OpenScene: 3D Scene Understanding With Open Vocabularies](http://arxiv.org/abs/2211.15654)
    * [Long Range Pooling for 3D Large-Scale Scene Understanding](https://arxiv.org/abs/2301.06962)
    * [Panoptic Lifting for 3D Scene Understanding With Neural Fields](https://openaccess.thecvf.com/content/CVPR2023/papers/Siddiqui_Panoptic_Lifting_for_3D_Scene_Understanding_With_Neural_Fields_CVPR_2023_paper.pdf)<br>:house:[project](nihalsid.github.io/panoptic-lifting/)
    * [FAC: 3D Representation Learning via Foreground Aware Feature Contrast](https://arxiv.org/abs/2303.06388)
    * [Self-supervised Pre-training with Masked Shape Prediction for 3D Scene Understanding](http://arxiv.org/abs/2305.05026v1)
    * [CLIP2Scene: Towards Label-Efficient 3D Scene Understanding by CLIP](https://arxiv.org/abs/2301.04926)<br>:star:[code](https://github.com/runnanchen/CLIP2Scene)
    * [PLA:Language-driven Open-Vocabulary 3D Scene Understanding](https://arxiv.org/pdf/2211.16312.pdf)<br>:star:[code](https://github.com/CVMI-Lab/PLA)<br>:house:[project](https://dingry.github.io/projects/PLA.html)
    * [MM-3DScene: 3D Scene Understanding by Customizing Masked Modeling With Informative-Preserved Reconstruction and Self-Distilled Consistency](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_MM-3DScene_3D_Scene_Understanding_by_Customizing_Masked_Modeling_With_Informative-Preserved_CVPR_2023_paper.pdf)
* 抠图
  * [Adaptive Human Matting for Dynamic Videos](http://arxiv.org/abs/2304.06018v1)<br>:star:[code](https://github.com/microsoft/AdaM)
  * [Mask-Guided Matting in the Wild](https://openaccess.thecvf.com/content/CVPR2023/papers/Park_Mask-Guided_Matting_in_the_Wild_CVPR_2023_paper.pdf)
  * [Ultrahigh Resolution Image/Video Matting With Spatio-Temporal Sparsity](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Ultrahigh_Resolution_ImageVideo_Matting_With_Spatio-Temporal_Sparsity_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/nowsyn/SparseMat.git)
  * [End-to-End Video Matting With Trimap Propagation](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_End-to-End_Video_Matting_With_Trimap_Propagation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/csvt32745/FTP-VM)
  * [Referring Image Matting](https://arxiv.org/abs/2206.05149)<br>:star:[code](https://github.com/JizhiziLi/RIM)
* 指代图像分割
  * [PolyFormer: Referring Image Segmentation As Sequential Polygon Generation](https://arxiv.org/abs/2302.07387)<br>:house:[project](https://polyformer.github.io/)
  * [Zero-shot Referring Image Segmentation with Global-Local Context Features](http://arxiv.org/abs/2303.17811v1)<br>:star:[code](https://github.com/Seonghoon-Yu/Zero-shot-RIS)
* 引用表达分割  
  * [GRES: Generalized Referring Expression Segmentation](https://arxiv.org/abs/2306.00968)<br>:house:[project](https://henghuiding.github.io/GRES/)<br>:thumbsup:[CVPR23 Highlight 多模态新任务、新数据集：NTU提出广义引用分割问题GRES](https://mp.weixin.qq.com/s/YoL_8a_8OPHovFrfJSXm4A)
  * [Meta Compositional Referring Expression Segmentation](http://arxiv.org/abs/2304.04415v1)
  * [Learning to Segment Every Referring Object Point by Point](https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_Learning_To_Segment_Every_Referring_Object_Point_by_Point_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/qumengxue/Partial-RES.git.)
* 运动分割
  * [Unsupervised Space-Time Network for Temporally-Consistent Segmentation of Multiple Motions](https://openaccess.thecvf.com/content/CVPR2023/papers/Meunier_Unsupervised_Space-Time_Network_for_Temporally-Consistent_Segmentation_of_Multiple_Motions_CVPR_2023_paper.pdf)
* 视频分割
  * [TarViS: A Unified Approach for Target-Based Video Segmentation](https://arxiv.org/abs/2301.02657)<br>:star:[code](https://github.com/Ali2500/TarViS)
* 动作分割
  * [ASPnet: Action Segmentation With Shared-Private Representation of Multiple Data Sources](https://openaccess.thecvf.com/content/CVPR2023/papers/van_Amsterdam_ASPnet_Action_Segmentation_With_Shared-Private_Representation_of_Multiple_Data_Sources_CVPR_2023_paper.pdf)

<a name="1"/>

## 1.other(其它,待分类)
* [AdaMAE: Adaptive Masking for Efficient Spatiotemporal Learning With Masked Autoencoders](http://arxiv.org/abs/2211.09120)
* [Understanding and Improving Visual Prompting: A Label-Mapping Perspective](http://arxiv.org/abs/2211.11635)
* [DegAE: A New Pretraining Paradigm for Low-Level Vision](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_DegAE_A_New_Pretraining_Paradigm_for_Low-Level_Vision_CVPR_2023_paper.pdf)
* [LiDAR-in-the-Loop Hyperparameter Optimization](https://openaccess.thecvf.com/content/CVPR2023/papers/Goudreault_LiDAR-in-the-Loop_Hyperparameter_Optimization_CVPR_2023_paper.pdf)
* [Understanding Deep Generative Models With Generalized Empirical Likelihoods](https://openaccess.thecvf.com/content/CVPR2023/papers/Ravuri_Understanding_Deep_Generative_Models_With_Generalized_Empirical_Likelihoods_CVPR_2023_paper.pdf)
* [Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning](https://arxiv.org/abs/2212.03220)
* [Compressing Volumetric Radiance Fields to 1 MB](https://arxiv.org/abs/2211.16386)<br>:star:[code](https://github.com/AlgoHunt/VQRF)
* [Label Information Bottleneck for Label Enhancement](https://arxiv.org/abs/2303.06836)<br>:star:[code](https://github.com/qinghai-zheng/LIBLE)
* [DNF: Decouple and Feedback Network for Seeing in the Dark](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_DNF_Decouple_and_Feedback_Network_for_Seeing_in_the_Dark_CVPR_2023_paper.pdf)
* [Cloud-Device Collaborative Adaptation to Continual Changing Environments in the Real-World](https://arxiv.org/abs/2212.00972)
* [How To Prevent the Continuous Damage of Noises To Model Training?](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_How_To_Prevent_the_Continuous_Damage_of_Noises_To_Model_CVPR_2023_paper.pdf)
* [ActMAD: Activation Matching To Align Distributions for Test-Time-Training](https://arxiv.org/abs/2211.12870)<br>:house:[project](https://jmiemirza.github.io/ActMAD/)
* [Leveraging Temporal Context in Low Representational Power Regimes](https://openaccess.thecvf.com/content/CVPR2023/papers/Fosco_Leveraging_Temporal_Context_in_Low_Representational_Power_Regimes_CVPR_2023_paper.pdf)<br>:house:[project](https://camilofosco.com/etm_website)
* [Guided Recommendation for Model Fine-Tuning](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Guided_Recommendation_for_Model_Fine-Tuning_CVPR_2023_paper.pdf)
* [OT-Filter: An Optimal Transport Filter for Learning With Noisy Labels](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_OT-Filter_An_Optimal_Transport_Filter_for_Learning_With_Noisy_Labels_CVPR_2023_paper.pdf)
* [E2PN: Efficient SE(3)-Equivariant Point Network](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_E2PN_Efficient_SE3-Equivariant_Point_Network_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/minghanz/E2PN)
* [Understanding Masked Image Modeling via Learning Occlusion Invariant Feature](https://arxiv.org/abs/2208.04164)
* [Fine-Tuned CLIP Models Are Efficient Video Learners](https://arxiv.org/abs/2212.03640)<br>:star:[code](https://github.com/muzairkhattak/ViFi-CLIP)
* [Visual Recognition by Request](https://arxiv.org/abs/2207.14227)
* [Stitchable Neural Networks](https://arxiv.org/abs/2302.06586)<br>:house:[project](https://snnet.github.io/)
* [RUST: Latent Neural Scene Representations From Unposed Imagery](https://arxiv.org/abs/2211.14306)<br>:star:[code](https://rust-paper.github.io/)
* [Spatio-Focal Bidirectional Disparity Estimation From a Dual-Pixel Image](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Spatio-Focal_Bidirectional_Disparity_Estimation_From_a_Dual-Pixel_Image_CVPR_2023_paper.pdf)
* [Four-View Geometry With Unknown Radial Distortion](https://openaccess.thecvf.com/content/CVPR2023/papers/Hruby_Four-View_Geometry_With_Unknown_Radial_Distortion_CVPR_2023_paper.pdf)
* [Learning Optical Expansion From Scale Matching](https://openaccess.thecvf.com/content/CVPR2023/papers/Ling_Learning_Optical_Expansion_From_Scale_Matching_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/HanLingsgjk/TPCV)
* [Don't Lie to Me! Robust and Efficient Explainability With Verified Perturbation Analysis](https://openaccess.thecvf.com/content/CVPR2023/papers/Fel_Dont_Lie_to_Me_Robust_and_Efficient_Explainability_With_Verified_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/deel-ai/formal-explainability)
* [Learning Transformation-Predictive Representations for Detection and Description of Local Features](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Learning_Transformation-Predictive_Representations_for_Detection_and_Description_of_Local_Features_CVPR_2023_paper.pdf)
* [Two-Way Multi-Label Loss](https://openaccess.thecvf.com/content/CVPR2023/papers/Kobayashi_Two-Way_Multi-Label_Loss_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/tk1980/TwowayMultiLabelLoss)
* [Where is my Wallet? Modeling Object Proposal Sets for Egocentric Visual Query Localization](https://arxiv.org/abs/2211.10528)<br>:star:[code](https://github.com/facebookresearch/vq2d_cvpr)
* [Dionysus: Recovering Scene Structures by Dividing Into Semantic Pieces](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Dionysus_Recovering_Scene_Structures_by_Dividing_Into_Semantic_Pieces_CVPR_2023_paper.pdf)
* [Noisy Correspondence Learning With Meta Similarity Correction](https://arxiv.org/abs/2304.06275)
* [HOOD: Hierarchical Graphs for Generalized Modelling of Clothing Dynamics](https://arxiv.org/abs/2212.07242)
* [Modeling Entities As Semantic Points for Visual Information Extraction in the Wild](https://arxiv.org/abs/2303.13095)<br>:house:[project](https://www.modelscope.cn/datasets/damo/SIBR/summary)
* [NeAT: Learning Neural Implicit Surfaces With Arbitrary Topologies From Multi-View Images](https://arxiv.org/abs/2303.12012)
* [Learning a Deep Color Difference Metric for Photographic Images](https://arxiv.org/abs/2303.14964)
* [DINN360: Deformable Invertible Neural Network for Latitude-Aware 360deg Image Rescaling](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_DINN360_Deformable_Invertible_Neural_Network_for_Latitude-Aware_360deg_Image_Rescaling_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/gyc9709/DINN360)
* [Finetune Like You Pretrain: Improved Finetuning of Zero-Shot Vision Models](https://arxiv.org/abs/2212.00638)<br>:star:[code](https://github.com/locuslab/FLYP)
* [Learning a Practical SDR-to-HDRTV Up-Conversion Using New Dataset and Degradation Models](https://arxiv.org/abs/2303.13031)<br>:star:[code](https://github.com/AndreGuo/HDRTVDM)
* [DynaFed: Tackling Client Data Heterogeneity With Global Dynamics](https://arxiv.org/abs/2211.10878)
* [CUF: Continuous Upsampling Filters](https://arxiv.org/abs/2210.06965)
* [Learning Decorrelated Representations Efficiently Using Fast Fourier Transform](https://arxiv.org/abs/2301.01569)
* [Practical Network Acceleration With Tiny Sets](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Practical_Network_Acceleration_With_Tiny_Sets_CVPR_2023_paper.pdf)
* [AstroNet: When Astrocyte Meets Artificial Neural Network](https://openaccess.thecvf.com/content/CVPR2023/papers/Han_AstroNet_When_Astrocyte_Meets_Artificial_Neural_Network_CVPR_2023_paper.pdf)
* [NeuralLift-360: Lifting an In-the-Wild 2D Photo to a 3D Object With 360deg Views](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_NeuralLift-360_Lifting_an_In-the-Wild_2D_Photo_to_a_3D_Object_CVPR_2023_paper.pdf)<br>:star:[code](https://vita-group.github.io/NeuralLift-360/)
* [Modality-Invariant Visual Odometry for Embodied Vision](https://arxiv.org/abs/2305.00348)
* [Command-Driven Articulated Object Understanding and Manipulation](https://openaccess.thecvf.com/content/CVPR2023/papers/Chu_Command-Driven_Articulated_Object_Understanding_and_Manipulation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/dvlab-research/Cart)
* [HelixSurf: A Robust and Efficient Neural Implicit Surface Learning of Indoor Scenes With Iterative Intertwined Regularization](https://arxiv.org/abs/2302.14340)<br>:star:[code](https://github.com/Gorilla-Lab-SCUT/HelixSurf)
* [Joint Appearance and Motion Learning for Efficient Rolling Shutter Correction](https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_Joint_Appearance_and_Motion_Learning_for_Efficient_Rolling_Shutter_Correction_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/GitCVfb/JAMNet)
* [Gradient-Based Uncertainty Attribution for Explainable Bayesian Deep Learning](https://arxiv.org/abs/2304.04824)
* [Class Adaptive Network Calibration](https://arxiv.org/abs/2211.15088)<br>:star:[code](https://github.com/by-liu/CALS)
* [OCTET: Object-Aware Counterfactual Explanations](https://arxiv.org/abs/2211.12380)<br>:star:[code](https://github.com/valeoai/OCTET)
* [DNeRV: Modeling Inherent Dynamics via Difference Neural Representation for Videos](https://arxiv.org/abs/2304.06544)
* [FFF: Fragment-Guided Flexible Fitting for Building Complete Protein Structures](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_FFF_Fragment-Guided_Flexible_Fitting_for_Building_Complete_Protein_Structures_CVPR_2023_paper.pdf)
* [Open-Set Representation Learning Through Combinatorial Embedding](https://arxiv.org/abs/2106.15278)
* [A Unified HDR Imaging Method With Pixel and Patch Level](https://arxiv.org/abs/2304.06943)
* [Accelerated Coordinate Encoding: Learning to Relocalize in Minutes Using RGB and Poses](https://openaccess.thecvf.com/content/CVPR2023/papers/Brachmann_Accelerated_Coordinate_Encoding_Learning_to_Relocalize_in_Minutes_Using_RGB_CVPR_2023_paper.pdf)<br>:star:[code](https://nianticlabs.github.io/ace)
* [Switchable Representation Learning Framework With Self-Compatibility](https://arxiv.org/abs/2206.08289)
* [Exploring and Utilizing Pattern Imbalance](https://openaccess.thecvf.com/content/CVPR2023/papers/Mei_Exploring_and_Utilizing_Pattern_Imbalance_CVPR_2023_paper.pdf)
* [Top-Down Visual Attention From Analysis by Synthesis](https://arxiv.org/abs/2303.13043)<br>:house:[project](https://sites.google.com/view/absvit)
* [Interactive Cartoonization With Controllable Perceptual Factors](https://arxiv.org/abs/2212.09555)
* [Regularize Implicit Neural Representation by Itself](https://arxiv.org/abs/2303.15484)
* [Delving Into Discrete Normalizing Flows on SO(3) Manifold for Probabilistic Rotation Modeling](https://arxiv.org/abs/2304.03937)
* [Re-Basin via Implicit Sinkhorn Differentiation](https://openaccess.thecvf.com/content/CVPR2023/papers/Pena_Re-Basin_via_Implicit_Sinkhorn_Differentiation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/fagp/sinkhorn-rebasin)
* [Towards Effective Visual Representations for Partial-Label Learning](https://arxiv.org/abs/2305.06080)
* [Samples With Low Loss Curvature Improve Data Efficiency](https://openaccess.thecvf.com/content/CVPR2023/papers/Garg_Samples_With_Low_Loss_Curvature_Improve_Data_Efficiency_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/isha-garg/SLo-Curves)
* [Learning Correspondence Uncertainty via Differentiable Nonlinear Least Squares](https://openaccess.thecvf.com/content/CVPR2023/papers/Muhle_Learning_Correspondence_Uncertainty_via_Differentiable_Nonlinear_Least_Squares_CVPR_2023_paper.pdf)
* [Tunable Convolutions With Parametric Multi-Loss Optimization](https://arxiv.org/abs/2304.00898)
* [RelightableHands: Efficient Neural Relighting of Articulated Hand Models](https://arxiv.org/abs/2302.04866)<br>:house:[project](https://sh8.io/#/relightable_hands)
* [DyNCA: Real-Time Dynamic Texture Synthesis Using Neural Cellular Automata](https://arxiv.org/abs/2211.11417)<br>:house:[project](https://dynca.github.io/)
* [Token Turing Machines](https://arxiv.org/abs/2211.09119)<br>:star:[code](https://github.com/google-research/scenic/tree/main/scenic/projects/token_turing)
* [Probabilistic Debiasing of Scene Graphs](https://arxiv.org/abs/2211.06444)<br>:star:[code](https://github.com/bashirulazam/within-triplet-debias)
* [Few-Shot Non-Line-of-Sight Imaging With Signal-Surface Collaborative Regularization](https://arxiv.org/abs/2211.15367)
* [The Dark Side of Dynamic Routing Neural Networks: Towards Efficiency Backdoor Injection](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_The_Dark_Side_of_Dynamic_Routing_Neural_Networks_Towards_Efficiency_CVPR_2023_paper.pdf)
* [Generalized Decoding for Pixel, Image, and Language](https://arxiv.org/abs/2212.11270)<br>:house:[project](https://x-decoder-vl.github.io/)
* [EC2: Emergent Communication for Embodied Control](https://openaccess.thecvf.com/content/CVPR2023/papers/Mu_EC2_Emergent_Communication_for_Embodied_Control_CVPR_2023_paper.pdf)
* [Generalizable Local Feature Pre-Training for Deformable Shape Analysis](https://arxiv.org/abs/2303.15104)<br>:star:[code](https://github.com/pvnieo/vader)
* [On-the-Fly Category Discovery](https://openaccess.thecvf.com/content/CVPR2023/papers/Du_On-the-Fly_Category_Discovery_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/PRIS-CV/On-the-fly-Category-Discovery)
* [PyramidFlow: High-Resolution Defect Contrastive Localization Using Pyramid Normalizing Flow](https://arxiv.org/abs/2303.02595)
* [Efficient Verification of Neural Networks Against LVM-Based Specifications](https://openaccess.thecvf.com/content/CVPR2023/papers/Hanspal_Efficient_Verification_of_Neural_Networks_Against_LVM-Based_Specifications_CVPR_2023_paper.pdf)
* [TensoIR: Tensorial Inverse Rendering](https://arxiv.org/abs/2304.12461)<br>:house:[project](https://haian-jin.github.io/TensoIR)
* [Learning From Unique Perspectives: User-Aware Saliency Modeling](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_From_Unique_Perspectives_User-Aware_Saliency_Modeling_CVPR_2023_paper.pdf)
* [LargeKernel3D: Scaling Up Kernels in 3D Sparse CNNs](https://arxiv.org/abs/2206.10555)<br>:star:[code](https://github.com/dvlab-research/LargeKernel3D)
* [Learning Transferable Spatiotemporal Representations From Natural Script Knowledge](https://arxiv.org/abs/2209.15280)<br>:star:[code](https://github.com/TencentARC/TVTS)
* [FFCV: Accelerating Training by Removing Data Bottlenecks](https://openaccess.thecvf.com/content/CVPR2023/papers/Leclerc_FFCV_Accelerating_Training_by_Removing_Data_Bottlenecks_CVPR_2023_paper.pdf)<br>:house:[project](https://ffcv.io/)
* [Semidefinite Relaxations for Robust Multiview Triangulation](https://openaccess.thecvf.com/content/CVPR2023/papers/Harenstam-Nielsen_Semidefinite_Relaxations_for_Robust_Multiview_Triangulation_CVPR_2023_paper.pdf)
* [GradICON: Approximate Diffeomorphisms via Gradient Inverse Consistency](https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_GradICON_Approximate_Diffeomorphisms_via_Gradient_Inverse_Consistency_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/uncbiag/ICON)
* [Polynomial Implicit Neural Representations for Large Diverse Datasets](https://arxiv.org/abs/2303.11424)<br>:star:[code](https://github.com/Rajhans0/Poly_INR)
* [Back to the Source: Diffusion-Driven Adaptation To Test-Time Corruption](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Back_to_the_Source_Diffusion-Driven_Adaptation_To_Test-Time_Corruption_CVPR_2023_paper.pdf)
* [Learning To Zoom and Unzoom](https://arxiv.org/abs/2303.15390)<br>:house:[project](https://tchittesh.github.io/lzu/)
* [Masked Image Modeling With Local Multi-Scale Reconstruction](https://arxiv.org/abs/2303.05251)
* [Neural Vector Fields: Implicit Representation by Explicit Learning](https://arxiv.org/abs/2303.04341)<br>:star:[code](https://github.com/Wi-sc/NVF)
* [Rate Gradient Approximation Attack Threats Deep Spiking Neural Networks](https://openaccess.thecvf.com/content/CVPR2023/papers/Bu_Rate_Gradient_Approximation_Attack_Threats_Deep_Spiking_Neural_Networks_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/putshua/SNN_attack_RGA)
* [Critical Learning Periods for Multisensory Integration in Deep Networks](https://arxiv.org/abs/2210.04643)
* [Imitation Learning as State Matching via Differentiable Physics](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Imitation_Learning_As_State_Matching_via_Differentiable_Physics_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/sail-sg/ILD)
* [Probing Sentiment-Oriented Pre-Training Inspired by Human Sentiment Perception Mechanism](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Probing_Sentiment-Oriented_Pre-Training_Inspired_by_Human_Sentiment_Perception_Mechanism_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/tinglyfeng/sentiment_pretraining)
* [Relightable Neural Human Assets From Multi-View Gradient Illuminations](https://arxiv.org/abs/2212.07648)<br>:star:[code](https://miaoing.github.io/RNHA)
* [DINER: Disorder-Invariant Implicit Neural Representation](https://arxiv.org/abs/2211.07871)
* [Robust Mean Teacher for Continual and Gradual Test-Time Adaptation](https://openaccess.thecvf.com/content/CVPR2023/papers/Dobler_Robust_Mean_Teacher_for_Continual_and_Gradual_Test-Time_Adaptation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/mariodoebler/test-time-adaptation)
* [A Probabilistic Framework for Lifelong Test-Time Adaptation](https://arxiv.org/abs/2212.09713)<br>:star:[code](https://github.com/dhanajitb/petal)
* [Probing Neural Representations of Scene Perception in a Hippocampally Dependent Task Using Artificial Neural Networks](https://arxiv.org/abs/2303.06367)
* [Decoupling Human and Camera Motion From Videos in the Wild](https://arxiv.org/abs/2302.12827)<br>:house:[project](https://vye16.github.io/slahmr)
* [DISC: Learning From Noisy Labels via Dynamic Instance-Specific Selection and Correction](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DISC_Learning_From_Noisy_Labels_via_Dynamic_Instance-Specific_Selection_and_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/JackYFL/DISC)
* [DC2: Dual-Camera Defocus Control by Learning To Refocus](https://openaccess.thecvf.com/content/CVPR2023/papers/Alzayer_DC2_Dual-Camera_Defocus_Control_by_Learning_To_Refocus_CVPR_2023_paper.pdf)
* [FJMP: Factorized Joint Multi-Agent Motion Prediction over Learned Directed Acyclic Interaction Graphs](https://arxiv.org/abs/2211.16197)
* ["Seeing" Electric Network Frequency From Events](https://arxiv.org/abs/2305.02597)<br>:house:[project](https://xlx-creater.github.io/E-ENF)
* [Confidential and Private Decentralized Learning Based on Encryption-Friendly Distillation Loss](https://openaccess.thecvf.com/content/CVPR2023/papers/Tastan_CaPriDe_Learning_Confidential_and_Private_Decentralized_Learning_Based_on_Encryption-Friendly_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/tnurbek/capride-learning)
* [Revealing the Dark Secrets of Masked Image Modeling](https://arxiv.org/abs/2205.13543)
* [RIFormer: Keep Your Vision Backbone Effective but Removing Token Mixer](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_RIFormer_Keep_Your_Vision_Backbone_Effective_but_Removing_Token_Mixer_CVPR_2023_paper.pdf)
* [Adaptive Graph Convolutional Subspace Clustering](https://arxiv.org/abs/2305.03414)
* [Graph Representation for Order-Aware Visual Transformation](https://openaccess.thecvf.com/content/CVPR2023/papers/Qiu_Graph_Representation_for_Order-Aware_Visual_Transformation_CVPR_2023_paper.pdf)
* [Train-Once-for-All Personalization](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Train-Once-for-All_Personalization_CVPR_2023_paper.pdf)
* [Learning Sample Relationship for Exposure Correction](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Learning_Sample_Relationship_for_Exposure_Correction_CVPR_2023_paper.pdf)
* [EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata](https://arxiv.org/abs/2301.04647)<br>:house:[project](http://hellomuffin.github.io/exif-as-language)
* [Gradient norm aware minimization seeks first-order flatness and improves generalization](https://arxiv.org/abs/2303.03108)<br>:star:[code](https://github.com/xxgege/GAM)<br>:thumbsup:[CVPR2023｜清华大学提出GAM：神经网络“一阶平滑优化器”，显著提升模型“泛化能力”](https://mp.weixin.qq.com/s/TJ5tX1jOPAY_9S2KvgMOsA)
* [EXIF As Language: Learning Cross-Modal Associations Between Images and Camera Metadata](https://arxiv.org/abs/2301.04647)<br>:house:[project](http://hellomuffin.github.io/exif-as-language)
* [InstantAvatar: Learning Avatars From Monocular Video in 60 Seconds](https://arxiv.org/abs/2212.10550)
* [GAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable and Actionable Parts](https://arxiv.org/abs/2211.05272)
* [Deep Deterministic Uncertainty: A New Simple Baseline](https://openaccess.thecvf.com/content/CVPR2023/papers/Mukhoti_Deep_Deterministic_Uncertainty_A_New_Simple_Baseline_CVPR_2023_paper.pdf)
* [WIRE: Wavelet Implicit Neural Representations](https://arxiv.org/abs/2301.05187)
* [Learning From Noisy Labels With Decoupled Meta Label Purifier](https://arxiv.org/abs/2302.06810)
* [Architectural Backdoors in Neural Networks](https://arxiv.org/abs/2206.07840)
* [Event-Based Shape From Polarization](https://arxiv.org/abs/2301.06855)
* [Deep Hashing With Minimal-Distance-Separated Hash Centers](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Deep_Hashing_With_Minimal-Distance-Separated_Hash_Centers_CVPR_2023_paper.pdf)
* [Progressive Spatio-Temporal Alignment for Efficient Event-Based Motion Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Progressive_Spatio-Temporal_Alignment_for_Efficient_Event-Based_Motion_Estimation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/huangxueyan/PEME)
* [Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation](https://arxiv.org/abs/2212.00774)<br>:house:[project](https://pals.ttic.edu/p/score-jacobian-chaining)
* [MetaCLUE: Towards Comprehensive Visual Metaphors Research](https://arxiv.org/abs/2212.09898)<br>:house:[project](https://metaclue.github.io/)
* [EVA: Exploring the Limits of Masked Visual Representation Learning at Scale](https://arxiv.org/abs/2211.07636)<br>:star:[code](https://github.com/baaivision/EVA)
* [Sliced Optimal Partial Transport](https://arxiv.org/abs/2212.08049)
* [Deep Learning of Partial Graph Matching via Differentiable Top-K](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Deep_Learning_of_Partial_Graph_Matching_via_Differentiable_Top-K_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Thinklab-SJTU/ThinkMatch)
* [Unsupervised Volumetric Animation](https://arxiv.org/abs/2301.11326)<br>:house:[project](https://snap-research.github.io/unsupervised-volumetric-animation)
* [Passive Micron-Scale Time-of-Flight With Sunlight Interferometry](https://arxiv.org/abs/2211.10732)
* [Generalizable Implicit Neural Representations via Instance Pattern Composers](https://arxiv.org/abs/2211.13223)<br>:star:[code](https://github.com/kakaobrain/ginr-ipc)
* [On the Pitfall of Mixup for Uncertainty Calibration](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_On_the_Pitfall_of_Mixup_for_Uncertainty_Calibration_CVPR_2023_paper.pdf)
* [UMat: Uncertainty-Aware Single Image High Resolution Material Capture](https://openaccess.thecvf.com/content/CVPR2023/papers/Rodriguez-Pardo_UMat_Uncertainty-Aware_Single_Image_High_Resolution_Material_Capture_CVPR_2023_paper.pdf)
* [On Data Scaling in Masked Image Modeling](https://arxiv.org/abs/2206.04664)
* [End-to-End Vectorized HD-Map Construction With Piecewise Bezier Curve](https://openaccess.thecvf.com/content/CVPR2023/papers/Qiao_End-to-End_Vectorized_HD-Map_Construction_With_Piecewise_Bezier_Curve_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/er-muyue/BeMapNet)
* [Boundary Unlearning: Rapid Forgetting of Deep Networks via Shifting the Decision Boundary](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Boundary_Unlearning_Rapid_Forgetting_of_Deep_Networks_via_Shifting_the_CVPR_2023_paper.pdf)
* [MobileOne: An Improved One millisecond Mobile Backbone](https://arxiv.org/abs/2206.04040)<br>:star:[code](https://github.com/apple/ml-mobileone)
* [Improving Robust Generalization by Direct PAC-Bayesian Bound Minimization](https://arxiv.org/abs/2211.12624)
* [Shepherding Slots to Objects: Towards Stable and Robust Object-Centric Learning](https://arxiv.org/abs/2303.17842)<br>:star:[code](https://github.com/object-understanding/SLASH)
* [Residual Degradation Learning Unfolding Framework With Mixing Priors Across Spectral and Spatial for Compressive Spectral Imaging](https://arxiv.org/abs/2211.06891)
* [Robust and Scalable Gaussian Process Regression and Its Applications](https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Robust_and_Scalable_Gaussian_Process_Regression_and_Its_Applications_CVPR_2023_paper.pdf)<br>:star:[code](github.com/YifanLu2000/Robust-Scalable-GPR)
* [NeuralUDF: Learning Unsigned Distance Fields for Multi-View Reconstruction of Surfaces With Arbitrary Topologies](https://arxiv.org/abs/2211.14173)<br>:house:[project](https://www.xxlong.site/NeuralUDF/)
* [Shortcomings of Top-Down Randomization-Based Sanity Checks for Evaluations of Deep Neural Network Explanations](https://arxiv.org/abs/2211.12486)
* [Alias-Free Convnets: Fractional Shift Invariance via Polynomial Activations](https://arxiv.org/abs/2303.08085)<br>:star:[code](https://github.com/hmichaeli/alias_free_convnets/)
* [Multiplicative Fourier Level of Detail](https://openaccess.thecvf.com/content/CVPR2023/papers/Dou_Multiplicative_Fourier_Level_of_Detail_CVPR_2023_paper.pdf)
* [VGFlow: Visibility guided Flow Network for Human Reposing](https://arxiv.org/abs/2211.08540)
* [Neural Dependencies Emerging From Learning Massive Categories](https://arxiv.org/abs/2211.12339)
* [MaLP: Manipulation Localization Using a Proactive Scheme](https://arxiv.org/abs/2303.16976)<br>:house:[project](http://www.github.com/vishal3477/pro_loc)
* [Efficient Robust Principal Component Analysis via Block Krylov Iteration and CUR Decomposition](https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_Efficient_Robust_Principal_Component_Analysis_via_Block_Krylov_Iteration_and_CVPR_2023_paper.pdf)
* [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808)<br>:star:[code](https://github.com/facebookresearch/ConvNeXt-V2)
* [Learning 3D Representations From 2D Pre-Trained Models via Image-to-Point Masked Autoencoders](https://arxiv.org/abs/2212.06785)<br>:star:[code](https://github.com/ZrrSkywalker/I2P-MAE)
* [MEGANE: Morphable Eyeglass and Avatar Network](https://arxiv.org/abs/2302.04868)<br>:house:[project](https://junxuan-li.github.io/megane/)
* [Solving relaxations of MAP-MRF problems: Combinatorial in-face Frank-Wolfe directions](https://arxiv.org/abs/2010.09567)
* [EXCALIBUR: Encouraging and Evaluating Embodied Exploration](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_EXCALIBUR_Encouraging_and_Evaluating_Embodied_Exploration_CVPR_2023_paper.pdf)
* [Learning To Predict Scene-Level Implicit 3D From Posed RGBD Data](https://openaccess.thecvf.com/content/CVPR2023/papers/Kulkarni_Learning_To_Predict_Scene-Level_Implicit_3D_From_Posed_RGBD_Data_CVPR_2023_paper.pdf)
* [SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries](https://arxiv.org/abs/2302.12828)<br>:house:[project](http://bit.ly/splinecam)
* [Learning Neural Parametric Head Models](https://arxiv.org/abs/2212.02761)<br>:house:[project](https://simongiebenhain.github.io/NPHM)
* [Integral Neural Networks](https://openaccess.thecvf.com/content/CVPR2023/papers/Solodskikh_Integral_Neural_Networks_CVPR_2023_paper.pdf)
* [Simulated Annealing in Early Layers Leads to Better Generalization](https://arxiv.org/abs/2304.04858)
* [Fresnel Microfacet BRDF: Unification of Polari-Radiometric Surface-Body Reflection](https://arxiv.org/abs/2212.04483)
* [Improving Visual Representation Learning Through Perceptual Understanding](https://arxiv.org/abs/2212.14504)
* [Probability-Based Global Cross-Modal Upsampling for Pansharpening](https://arxiv.org/abs/2303.13659)<br>:star:[code](https://github.com/Zeyu-Zhu/PGCU)
* [SCConv: Spatial and Channel Reconstruction Convolution for Feature Redundancy](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SCConv_Spatial_and_Channel_Reconstruction_Convolution_for_Feature_Redundancy_CVPR_2023_paper.pdf)
* [Megahertz Light Steering Without Moving Parts](https://openaccess.thecvf.com/content/CVPR2023/papers/Pediredla_Megahertz_Light_Steering_Without_Moving_Parts_CVPR_2023_paper.pdf)
* [TempSAL - Uncovering Temporal Information for Deep Saliency Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Aydemir_TempSAL_-_Uncovering_Temporal_Information_for_Deep_Saliency_Prediction_CVPR_2023_paper.pdf)<br>:house:[project](https://ivrl.github.io/Tempsal/)
* [Affection: Learning Affective Explanations for Real-World Visual Data](https://arxiv.org/abs/2210.01946)<br>:house:[project](https://affective-explanations.org/)
* [Metadata-Based RAW Reconstruction via Implicit Neural Functions](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Metadata-Based_RAW_Reconstruction_via_Implicit_Neural_Functions_CVPR_2023_paper.pdf)
* [Coaching a Teachable Student](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Coaching_a_Teachable_Student_CVPR_2023_paper.pdf)
* [Progressive Transformation Learning for Leveraging Virtual Images in Training](https://arxiv.org/abs/2211.01778)
* [NIRVANA: Neural Implicit Representations of Videos with Adaptive Networks and Autoregressive Patch-wise Modeling](https://arxiv.org/abs/2212.14593)
* [Spatial-Temporal Concept Based Explanation of 3D ConvNets](https://arxiv.org/abs/2206.05275)
* [Overlooked Factors in Concept-Based Explanations: Dataset Choice, Concept Learnability, and Human Capability](https://arxiv.org/abs/2207.09615)<br>:star:[code](https://github.com/princetonvisualai/OverlookedFactors)
* [Neural Fourier Filter Bank](https://arxiv.org/abs/2212.01735)<br>:star:[code](https://github.com/ubc-vision/NFFB)
* [ECON: Explicit Clothed Humans Optimized via Normal Integration](https://arxiv.org/abs/2212.07422)<br>:star:[code](https://github.com/YuliangXiu/ECON)
* [Autonomous Manipulation Learning for Similar Deformable Objects via Only One Demonstration](https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Autonomous_Manipulation_Learning_for_Similar_Deformable_Objects_via_Only_One_CVPR_2023_paper.pdf)
* [Plateau-Reduced Differentiable Path Tracing](https://arxiv.org/abs/2211.17263)<br>:house:[project](https://mfischer-ucl.github.io/prdpt/)
* [Test Time Adaptation With Transformation Invariance](https://openaccess.thecvf.com/content/CVPR2023/papers/Nguyen_TIPI_Test_Time_Adaptation_With_Transformation_Invariance_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/atuannguyen/TIPI)
* [Learning To Exploit the Sequence-Specific Prior Knowledge for Image Processing Pipelines Optimization](https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Learning_To_Exploit_the_Sequence-Specific_Prior_Knowledge_for_Image_Processing_CVPR_2023_paper.pdf)
* [Deep Fair Clustering via Maximizing and Minimizing Mutual Information: Theory, Algorithm and Metric](https://arxiv.org/abs/2209.12396)<br>:house:[project](https://pengxi.me/)
* [CUDA: Convolution-based Unlearnable Datasets](https://arxiv.org/abs/2303.04278)
* [Efficient On-Device Training via Gradient Filtering](https://arxiv.org/abs/2301.00330)
* [Transfer Knowledge From Head to Tail: Uncertainty Calibration Under Long-Tailed Distribution](https://arxiv.org/abs/2304.06537)
* [Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive Learning](https://arxiv.org/abs/2206.12126)
* [Disentangled Representation Learning for Unsupervised Neural Quantization](https://openaccess.thecvf.com/content/CVPR2023/papers/Noh_Disentangled_Representation_Learning_for_Unsupervised_Neural_Quantization_CVPR_2023_paper.pdf)
* [DA Wand: Distortion-Aware Selection Using Neural Mesh Parameterization](https://arxiv.org/abs/2212.06344)<br>:star:[code](https://github.com/threedle/DA-Wand)<br>:house:[project](https://threedle.github.io/DA-Wand/)
* [On Distillation of Guided Diffusion Models](https://arxiv.org/abs/2210.03142)
* [Putting People in Their Place: Affordance-Aware Human Insertion Into Scenes](https://arxiv.org/abs/2304.14406)<br>:star:[code](https://sumith1896.github.io/affordance-insertion/)
* [K-Planes: Explicit Radiance Fields in Space, Time, and Appearance](https://openaccess.thecvf.com/content/CVPR2023/papers/Fridovich-Keil_K-Planes_Explicit_Radiance_Fields_in_Space_Time_and_Appearance_CVPR_2023_paper.pdf)<br>:house:[project](sarafridov.github.io/K-Plane)
* [Understanding Masked Autoencoders via Hierarchical Latent Variable Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Kong_Understanding_Masked_Autoencoders_via_Hierarchical_Latent_Variable_Models_CVPR_2023_paper.pdf)
* [Co-Training 2L Submodels for Visual Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Touvron_Co-Training_2L_Submodels_for_Visual_Recognition_CVPR_2023_paper.pdf)
* [Masked Images Are Counterfactual Samples for Robust Fine-Tuning](https://arxiv.org/abs/2303.03052)<br>:star:[code](https://github.com/Coxy7/robust-finetuning)
* [Learning Customized Visual Models With Retrieval-Augmented Knowledge](https://arxiv.org/abs/2301.07094)
* [A Unified Spatial-Angular Structured Light for Single-View Acquisition of Shape and Reflectance](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_A_Unified_Spatial-Angular_Structured_Light_for_Single-View_Acquisition_of_Shape_CVPR_2023_paper.pdf)
* [PromptCAL: Contrastive Affinity Learning via Auxiliary Prompts for Generalized Novel Category Discovery](https://arxiv.org/abs/2212.05590)<br>:star:[code](https://github.com/sheng-eatamath/PromptCAL)
* [Reproducible Scaling Laws for Contrastive Language-Image Learning](https://arxiv.org/abs/2212.07143)<br>:star:[code](https://github.com/LAION-AI/scaling-laws-openclip)
* [Intrinsic Physical Concepts Discovery With Object-Centric Predictive Models](https://arxiv.org/abs/2303.01869)
* [Invertible Neural Skinning](https://arxiv.org/abs/2302.09227)<br>:house:[project](https://yashkant.github.io/invertible-neural-skinning/)
* [Multi-Object Manipulation via Object-Centric Neural Scattering Functions](https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_Multi-Object_Manipulation_via_Object-Centric_Neural_Scattering_Functions_CVPR_2023_paper.pdf)
* [Fair Scratch Tickets: Finding Fair Sparse Networks Without Weight Training](https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Fair_Scratch_Tickets_Finding_Fair_Sparse_Networks_Without_Weight_Training_CVPR_2023_paper.pdf)
* [Backdoor Cleansing With Unlabeled Data](https://arxiv.org/abs/2211.12044)<br>:star:[code](https://github.com/luluppang/BCU)
* [Full or Weak Annotations? An Adaptive Strategy for Budget-Constrained Annotation Campaigns](https://arxiv.org/abs/2303.11678)
* [Extracting Class Activation Maps From Non-Discriminative Features As Well](https://arxiv.org/abs/2303.10334)
* [Executing Your Commands via Motion Diffusion in Latent Space](https://arxiv.org/abs/2212.04048)
* [Chat2Map: Efficient Scene Mapping From Multi-Ego Conversations](https://arxiv.org/abs/2301.02184)<br>:house:[project](http://vision.cs.utexas.edu/projects/chat2map)
* [Learning To Generate Image Embeddings With User-Level Differential Privacy](https://arxiv.org/abs/2211.10844)
* [Revisiting the Stack-Based Inverse Tone Mapping](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Revisiting_the_Stack-Based_Inverse_Tone_Mapping_CVPR_2023_paper.pdf)
* [PACO: Parts and Attributes of Common Objects](https://arxiv.org/abs/2301.01795)<br>:star:[code](https://github.com/facebookresearch/paco)
* [Teacher-Generated Spatial-Attention Labels Boost Robustness and Accuracy of Contrastive Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Teacher-Generated_Spatial-Attention_Labels_Boost_Robustness_and_Accuracy_of_Contrastive_Models_CVPR_2023_paper.pdf)
* [A General Regret Bound of Preconditioned Gradient Method for DNN Training](https://openaccess.thecvf.com/content/CVPR2023/papers/Yong_A_General_Regret_Bound_of_Preconditioned_Gradient_Method_for_DNN_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Yonghongwei/AdaBK)
* [A Practical Upper Bound for the Worst-Case Attribution Deviations](https://arxiv.org/abs/2303.00340)
* [Perception and Semantic Aware Regularization for Sequential Confidence Calibration](https://arxiv.org/abs/2305.19498)<br>:star:[code](https://github.com/husterpzh/PSSR)
* [Deep Random Projector: Accelerated Deep Image Prior](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Deep_Random_Projector_Accelerated_Deep_Image_Prior_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/sun- umn/DeepRandom-Projector)
* [Bias Mimicking: A Simple Sampling Approach for Bias Mitigation](https://arxiv.org/abs/2209.15605)<br>:star:[code](https://github.com/mqraitem/Bias-Mimicking)
* [DeCo: Decomposition and Reconstruction for Compositional Temporal Grounding via Coarse-To-Fine Contrastive Ranking](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_DeCo_Decomposition_and_Reconstruction_for_Compositional_Temporal_Grounding_via_Coarse-To-Fine_CVPR_2023_paper.pdf)
* [Structured Kernel Estimation for Photon-Limited Deconvolution](https://arxiv.org/abs/2303.03472)<br>:star:[code](https://github.com/sanghviyashiitb/structured-kernel-cvpr23)
* [FlexiViT: One Model for All Patch Sizes](https://arxiv.org/abs/2212.08013)<br>:star:[code](https://github.com/google-research/big_vision)
* [BiasBed - Rigorous Texture Bias Evaluation](https://openaccess.thecvf.com/content/CVPR2023/papers/Kalischek_BiasBed_-_Rigorous_Texture_Bias_Evaluation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/D1noFuzi/BiasBed)
* [GeoLayoutLM: Geometric Pre-Training for Visual Information Extraction](https://arxiv.org/abs/2304.10759)<br>:star:[code](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/GeoLayoutLM)
* [Finding Geometric Models by Clustering in the Consensus Space](https://openaccess.thecvf.com/content/CVPR2023/papers/Barath_Finding_Geometric_Models_by_Clustering_in_the_Consensus_Space_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/danini/clustering-in-consensus-space)
* [Hierarchical Neural Memory Network for Low Latency Event Processing](https://openaccess.thecvf.com/content/CVPR2023/papers/Hamaguchi_Hierarchical_Neural_Memory_Network_for_Low_Latency_Event_Processing_CVPR_2023_paper.pdf)<br>:house:[project](https://hamarh.github.io/)
* [Connecting the Dots: Floorplan Reconstruction Using Two-Level Queries](https://arxiv.org/abs/2211.15658)<br>:star:[code](https://github.com/ywyue/RoomFormer)
* [PointConvFormer: Revenge of the Point-Based Convolution](https://arxiv.org/abs/2208.02879)
* [A Practical Stereo Depth System for Smart Glasses](https://arxiv.org/abs/2211.10551)
* [Differentiable Shadow Mapping for Efficient Inverse Graphics](https://openaccess.thecvf.com/content/CVPR2023/papers/Worchel_Differentiable_Shadow_Mapping_for_Efficient_Inverse_Graphics_CVPR_2023_paper.pdf)
* [Multi Domain Learning for Motion Magnification](https://openaccess.thecvf.com/content/CVPR2023/papers/Singh_Multi_Domain_Learning_for_Motion_Magnification_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/jasdeep-singh-007/Multi-Domain-Learning-for-Motion-Magnification)
* [Re-Thinking Model Inversion Attacks Against Deep Neural Networks](https://arxiv.org/abs/2304.01669)<br>:star:[code](https://ngoc-nguyen-0.github.io/re-thinking_model_inversion_attacks/)
* [DexArt: Benchmarking Generalizable Dexterous Manipulation With Articulated Objects](https://arxiv.org/abs/2305.05706)<br>:house:[project](https://www.chenbao.tech/dexart/)
* [Two-View Geometry Scoring Without Correspondences](https://openaccess.thecvf.com/content/CVPR2023/papers/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.pdf)<br>:house:[project](http://www.github.com/nianticlabs/scoring-without-correspondences)
* [ScanDMM: A Deep Markov Model of Scanpath Prediction for 360deg Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Sui_ScanDMM_A_Deep_Markov_Model_of_Scanpath_Prediction_for_360deg_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/xiangjieSui/ScanDMM)
* [Zero-Shot Text-to-Parameter Translation for Game Character Auto-Creation](https://arxiv.org/abs/2303.01311)
* [Analyzing Physical Impacts Using Transient Surface Wave Imaging](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Analyzing_Physical_Impacts_Using_Transient_Surface_Wave_Imaging_CVPR_2023_paper.pdf)
* [Adaptive Global Decay Process for Event Cameras](https://openaccess.thecvf.com/content/CVPR2023/papers/Nunes_Adaptive_Global_Decay_Process_for_Event_Cameras_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/neuromorphic-paris/event)
* [Leveraging Inter-Rater Agreement for Classification in the Presence of Noisy Labels](https://openaccess.thecvf.com/content/CVPR2023/papers/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.pdf)
* [Towards Better Gradient Consistency for Neural Signed Distance Functions via Level Set Alignment](http://arxiv.org/abs/2305.11601v1)<br>:star:[code](https://github.com/mabaorui/TowardsBetterGradient)
* [Swept-Angle Synthetic Wavelength Interferometry](https://arxiv.org/abs/2205.10655)
* [Shape, Pose, and Appearance From a Single Image via Bootstrapped Radiance Field Inversion](https://arxiv.org/abs/2211.11674)<br>:house:[project](https://github.com/google-research/nerf-from-image)
* [Unlearnable Clusters: Towards Label-Agnostic Unlearnable Examples](https://arxiv.org/abs/2301.01217)<br>:star:[code](https://github.com/jiamingzhang94/Unlearnable-Clusters)
* [3D-Aware Object Goal Navigation via Simultaneous Exploration and Identification](https://arxiv.org/abs/2212.00338)
* [EcoTTA: Memory-Efficient Continual Test-Time Adaptation via Self-Distilled Regularization](https://arxiv.org/abs/2303.01904)
* [Text-Guided Unsupervised Latent Transformation for Multi-Attribute Image Manipulation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Text-Guided_Unsupervised_Latent_Transformation_for_Multi-Attribute_Image_Manipulation_CVPR_2023_paper.pdf)
* [Minimizing the Accumulated Trajectory Error To Improve Dataset Distillation](https://arxiv.org/abs/2211.11004)<br>:star:[code](https://github.com/AngusDujw/FTD-distillation) 
* [DisCoScene: Spatially Disentangled Generative Radiance Fields for Controllable 3D-Aware Scene Synthesis](https://arxiv.org/abs/2212.11984)<br>:house:[project](https://snap-research.github.io/discoscene/)
* [Virtual Occlusions Through Implicit Depth](http://arxiv.org/abs/2305.07014v1)
* [StyleSync: High-Fidelity Generalized and Personalized Lip Sync in Style-based Generator](http://arxiv.org/abs/2305.05445v1)<br>:star:[code](https://hangz-nju-cuhk.github.io/projects/StyleSync)
* [Putting People in Their Place: Affordance-Aware Human Insertion into Scenes](http://arxiv.org/abs/2304.14406v1)<br>:star:[code](https://sumith1896.github.io/affordance-insertion/)
* [Inverting the Imaging Process by Learning an Implicit Camera Model](http://arxiv.org/abs/2304.12748v1)<br>:star:[code](https://xhuangcv.github.io/neucam/)
* [Visual DNA: Representing and Comparing Images using Distributions of Neuron Activations](http://arxiv.org/abs/2304.10036v1)<br>:star:[code](https://bramtoula.github.io/vdna/)
* [GeoLayoutLM: Geometric Pre-training for Visual Information Extraction](http://arxiv.org/abs/2304.10759v1)<br>:star:[code](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/GeoLayoutLM)
* [Gradient-based Uncertainty Attribution for Explainable Bayesian Deep Learning](http://arxiv.org/abs/2304.04824v1)
* [Noisy Correspondence Learning with Meta Similarity Correction](http://arxiv.org/abs/2304.06275v1)
* [Efficient Multimodal Fusion via Interactive Prompting](http://arxiv.org/abs/2304.06306v1)
* [Representing Volumetric Videos as Dynamic MLP Maps](http://arxiv.org/abs/2304.06717v1)<br>:star:[code](https://zju3dv.github.io/mlp_maps/)
* [Neuro-Modulated Hebbian Learning for Fully Test-Time Adaptation](https://arxiv.org/pdf/2303.00914.pdf)
* [Disentangling Orthogonal Planes for Indoor Panoramic Room Layout Estimation with Cross-Scale Distortion Awareness](https://arxiv.org/abs/2303.00971)
* [DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks](https://arxiv.org/pdf/2302.14685.pdf)
* [EcoTTA: Memory-Efficient Continual Test-time Adaptation via Self-distilled Regularization](https://arxiv.org/abs/2303.01904)
* [Intrinsic Physical Concepts Discovery with Object-Centric Predictive Models](https://arxiv.org/pdf/2303.01869.pdf)
* [A Meta-Learning Approach to Predicting Performance and Data Requirements](https://arxiv.org/abs/2303.01598)
* [Multimodal Prompting with Missing Modalities for Visual Recognition](https://arxiv.org/abs/2303.03369)<br>:star:[code](https://github.com/YiLunLee/Missing_aware_prompts)
* [Masked Images Are Counterfactual Samples for Robust Fine-tuning](https://arxiv.org/abs/2303.03052)
* [UniHCP: A Unified Model for Human-Centric Perceptions](https://arxiv.org/abs/2303.02936)<br>:star:[code](https://github.com/OpenGVLab/UniHCP)
* [DeepMAD: Mathematical Architecture Design for Deep Convolutional Neural Network](https://arxiv.org/abs/2303.02165)<br>:star:[code](https://github.com/alibaba/lightweight-neural-architecture-search)
* [Progressive Open Space Expansion for Open-Set Model Attribution](https://arxiv.org/abs/2303.06877)<br>:star:[code](https://github.com/TianyunYoung/POSE)
* [TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets](https://arxiv.org/abs/2303.05762)<br>:star:[code](https://github.com/chenweixin107/TrojDiff)
* [HumanBench: Towards General Human-centric Perception with Projector Assisted Pretraining](https://arxiv.org/abs/2303.05675)<br>:star:[code](https://github.com/OpenGVLab/HumanBench)
* [3D Cinemagraphy from a Single Image](https://arxiv.org/abs/2303.05724)<br>:house:[project](https://xingyi-li.github.io/3d-cinemagraphy)
* [Masked Image Modeling with Local Multi-Scale Reconstruction](https://arxiv.org/abs/2303.05251)<br>:star:[code](https://gitee.com/mindspore/hub/blob/fa2a3270aa36673f835e524fa55c5a4c67262eb2/mshub_res/assets/noah-cvlab/gpu/1.8/localmim_v1.0_imagenet2012.md)
* [Revisiting Rotation Averaging: Uncertainties and Robust Losses](https://arxiv.org/abs/2303.05195)<br>:star:[code](https://github.com/zhangganlin/GlobalSfMpy)
* [Unifying Layout Generation with a Decoupled Diffusion Model](https://arxiv.org/abs/2303.05049)
* [Adversarial Counterfactual Visual Explanations](https://arxiv.org/abs/2303.09962)<br>:star:[code](https://github.com/guillaumejs2403/ACE)
* [Trainable Projected Gradient Method for Robust Fine-tuning](https://arxiv.org/abs/2303.10720)<br>:star:[code](https://github.com/PotatoTian/TPGM)
* [Partial Network Cloning](https://arxiv.org/abs/2303.10597)<br>:star:[code](https://github.com/JngwenYe/PNCloning)
* [Extracting Class Activation Maps from Non-Discriminative Features as well](https://arxiv.org/abs/2303.10334)<br>:star:[code](https://github.com/zhaozhengChen/LPCAM)
* [TWINS: A Fine-Tuning Framework for Improved Transferability of Adversarial Robustness and Generalization](https://arxiv.org/abs/2303.11135)<br>:star:[code](https://github.com/ziquanliu/CVPR2023-TWINS)
* [Visibility Constrained Wide-band Illumination Spectrum Design for Seeing-in-the-Dark](https://arxiv.org/abs/2303.11642)<br>:star:[code](https://github.com/MyNiuuu/VCSD)
* [PRISE: Demystifying Deep Lucas-Kanade with Strongly Star-Convex Constraints for Multimodel Image Alignment](https://arxiv.org/abs/2303.11526)<br>:star:[code](https://github.com/Zhang-VISLab)
* [Boundary Unlearning](https://arxiv.org/abs/2303.11570)<br>:house:[project](https://www.dropbox.com/s/bwu543qsdy4s32i/Boundary-Unlearning-Code.zip?dl=0)
* [ProphNet: Efficient Agent-Centric Motion Forecasting with Anchor-Informed Proposals](https://arxiv.org/abs/2303.12071)
* [VecFontSDF: Learning to Reconstruct and Synthesize High-quality Vector Fonts via Signed Distance Functions](http://arxiv.org/abs/2303.12675v1)
* [Learning a Depth Covariance Function](http://arxiv.org/abs/2303.12157v1)<br>:star:[code](https://edexheim.github.io/depth_cov/)
* [A Bag-of-Prototypes Representation for Dataset-Level Applications](http://arxiv.org/abs/2303.13251v1)
* [CrOC: Cross-View Online Clustering for Dense Visual Representation Learning](http://arxiv.org/abs/2303.13245v1)<br>:star:[code](https://github.com/stegmuel/CrOC)
* [Exploring Structured Semantic Prior for Multi Label Recognition with Incomplete Labels](http://arxiv.org/abs/2303.13223v1)<br>:star:[code](https://github.com/jameslahm/SCPNet)
* [Marching-Primitives: Shape Abstraction from Signed Distance Function](http://arxiv.org/abs/2303.13190v1)<br>:star:[code](https://github.com/ChirikjianLab/Marching-Primitives.git)
* [Robust Generalization against Photon-Limited Corruptions via Worst-Case Sharpness Minimization](http://arxiv.org/abs/2303.13087v1)
* [Robust Test-Time Adaptation in Dynamic Scenarios](http://arxiv.org/abs/2303.13899v1)<br>:star:[code](https://github.com/BIT-DA/RoTTA)
* [Enhancing Multiple Reliability Measures via Nuisance-extended Information Bottleneck](http://arxiv.org/abs/2303.14096v1)<br>:star:[code](https://github.com/jh-jeong/nuisance_ib)
* [IDGI: A Framework to Eliminate Explanation Noise from Integrated Gradients](http://arxiv.org/abs/2303.14242v1)
* [Compacting Binary Neural Networks by Sparse Kernel Selection](http://arxiv.org/abs/2303.14470v1)
* [PDPP:Projected Diffusion for Procedure Planning in Instructional Videos](http://arxiv.org/abs/2303.14676v1)<br>:star:[code](https://github.com/MCG-NJU/PDPP)
* [Multi-Granularity Archaeological Dating of Chinese Bronze Dings Based on a Knowledge-Guided Relation Graph](http://arxiv.org/abs/2303.15266v1)<br>:star:[code](https://github.com/zhourixin/bronze-Ding)
* [Quantum Multi-Model Fitting](http://arxiv.org/abs/2303.15444v1)<br>:star:[code](https://github.com/FarinaMatteo/qmmf)
* [Continuous Intermediate Token Learning with Implicit Motion Manifold for Keyframe Based Motion Interpolation](http://arxiv.org/abs/2303.14926v1)
* [PMatch: Paired Masked Image Modeling for Dense Geometric Matching](http://arxiv.org/abs/2303.17342v1)<br>:star:[code](https://github.com/ShngJZ/PMatch)
* [ImageNet-E: Benchmarking Neural Network Robustness via Attribute Editing](http://arxiv.org/abs/2303.17096v1)<br>:star:[code](https://github.com/alibaba/easyrobust)
* [Single Image Depth Prediction Made Better: A Multivariate Gaussian Take](http://arxiv.org/abs/2303.18164v1)
* [Why is the winner the best?](http://arxiv.org/abs/2303.17719v1)
* [Disorder-invariant Implicit Neural Representation](http://arxiv.org/abs/2304.00837v1)<br>:star:[code](https://ezio77.github.io/DINER-website/)
* [HypLiLoc: Towards Effective LiDAR Pose Regression with Hyperbolic Fusion](http://arxiv.org/abs/2304.00932v1)<br>:star:[code](https://github.com/sijieaaa/HypLiLoc)
* [Enhancing Deformable Local Features by Jointly Learning to Detect and Describe Keypoints](http://arxiv.org/abs/2304.00583v1)<br>:house:[project](https://verlab.dcc.ufmg.br/descriptors/dalf_cvpr23)
* [SMPConv: Self-moving Point Representations for Continuous Convolution](http://arxiv.org/abs/2304.02330v1)<br>:star:[code](https://github.com/sangnekim/SMPConv)
* [VNE: An Effective Method for Improving Deep Representation by Manipulating Eigenvalue Distribution](http://arxiv.org/abs/2304.01434v1)<br>:star:[code](https://github.com/jaeill/CVPR23-VNE)
* [Delving into Discrete Normalizing Flows on SO(3) Manifold for Probabilistic Rotation Modeling](http://arxiv.org/abs/2304.03937v1)
* [Wide-Angle Rectification via Content-Aware Conformal Mapping](https://arxiv.org/abs/2303.16624)<br>:house:[project](https://astr2023.github.io/)
* [Large-capacity and Flexible Video Steganography via Invertible Neural Network](http://arxiv.org/abs/2304.12300v1)<br>:star:[code](https://github.com/MC-E/LF-VSN)
* [SketchXAI: A First Look at Explainability for Human Sketches](http://arxiv.org/abs/2304.11744v1)<br>:star:[code](https://sketchxai.github.io)
* [Hard Patches Mining for Masked Image Modeling](http://arxiv.org/abs/2304.05919v1)<br>:thumbsup:[CVPR 2023 | HPM：在掩码学习中挖掘困难样本，带来稳固性能提升！](https://mp.weixin.qq.com/s/I9XGSDwkkmkmIE4tBaIK3g)
* [Learning Geometry-aware Representations by Sketching](http://arxiv.org/abs/2304.08204v1)
* [DisCo-CLIP: A Distributed Contrastive Loss for Memory Efficient CLIP Training](http://arxiv.org/abs/2304.08480v1)<br>:star:[code](https://github.com/IDEA-Research/DisCo-CLIP)
* [Investigating the Nature of 3D Generalization in Deep Neural Networks](https://arxiv.org/abs/2304.09358)<br>:star:[code](https://github.com/shoaibahmed/investigating_3d_generalization.git)
* [EC^2: Emergent Communication for Embodied Control](http://arxiv.org/abs/2304.09448v1)
* [Generalizing Dataset Distillation via Deep Generative Prior](https://arxiv.org/abs/2305.01649)<br>:star:[code](https://github.com/GeorgeCazenavette/glad)<br>:house:[project](https://georgecazenavette.github.io/glad)
* [Learning Locally Editable Virtual Humans](https://arxiv.org/abs/2305.00121)<br>:house:[project](https://custom-humans.github.io)
* [Class-Balancing Diffusion Models](https://arxiv.org/abs/2305.00562)
* [SFD2: Semantic-guided Feature Detection and Description](https://arxiv.org/abs/2304.14845)<br>:star:[code](https://github.com/feixue94/sfd2)
* [Computational Flash Photography Through Intrinsics](https://openaccess.thecvf.com/content/CVPR2023/papers/Maralan_Computational_Flash_Photography_Through_Intrinsics_CVPR_2023_paper.pdf)
* [Deep Graph Reprogramming](https://arxiv.org/abs/2304.14593)
* [LayoutDM: Transformer-based Diffusion Model for Layout Generation](http://arxiv.org/abs/2305.02567v1)
* [MetaViewer: Towards a Unified Multi-View Representation](https://arxiv.org/abs/2303.06329)
* [Learning Compact Representations for LiDAR Completion and Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_Learning_Compact_Representations_for_LiDAR_Completion_and_Generation_CVPR_2023_paper.pdf)<br>:house:[project](https://waabi.ai/research/ultralidar/)
* 多模态
  * [Understanding and Constructing Latent Modality Structures in Multi-Modal Representation Learning](https://arxiv.org/abs/2303.05952)
  * [Towards Flexible Multi-Modal Document Models](http://arxiv.org/abs/2303.18248)
  * [Multi-Modal Representation Learning With Text-Driven Soft Masks](http://arxiv.org/abs/2304.00719)
  * [Align and Attend: Multimodal Summarization With Dual Contrastive Losses](https://arxiv.org/abs/2303.07284)<br>:house:[project](https://boheumd.github.io/A2Summ/)
  * [Improving Zero-Shot Generalization and Robustness of Multi-Modal Models](https://arxiv.org/abs/2212.01758)<br>:star:[code](https://github.com/gyhandy/Hierarchy-CLIP)
  * [BEV-Guided Multi-Modality Fusion for Driving Perception](https://openaccess.thecvf.com/content/CVPR2023/papers/Man_BEV-Guided_Multi-Modality_Fusion_for_Driving_Perception_CVPR_2023_paper.pdf)<br>:star:[code](https://yunzeman.github.io/BEVGuide)
  * [BiCro: Noisy Correspondence Rectification for Multi-modality Data via Bi-directional Cross-modal Similarity Consistency](http://arxiv.org/abs/2303.12419v1)
  * [Towards All-in-One Pre-Training via Maximizing Multi-Modal Mutual Information](https://arxiv.org/abs/2211.09807)<br>:star:[code](https://github.com/OpenGVLab/M3I-Pretraining)
  * [Learning Instance-Level Representation for Large-Scale Multi-Modal Pretraining in E-commerce](http://arxiv.org/abs/2304.02853v1)多模态预训练
  * [MMANet: Margin-Aware Distillation and Modality-Aware Regularization for Incomplete Multimodal Learning](https://arxiv.org/abs/2304.08028)<br>:star:[code](https://github.com/shicaiwei123/MMANet)
* Affordance Learning(启示学习)
  * [Leverage Interactive Affinity for Affordance Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Leverage_Interactive_Affinity_for_Affordance_Learning_CVPR_2023_paper.pdf)<br>:star:[code](github.com/lhc1224/PIAL-Net)
* Feature Matching(特征匹配)
  * [PATS: Patch Area Transportation with Subdivision for Local Feature Matching](https://arxiv.org/abs/2303.07700)<br>:house:[project](https://zju3dv.github.io/pats/)
  * [Adaptive Spot-Guided Transformer for Consistent Local Feature Matching](http://arxiv.org/abs/2303.16624v1)<br>:star:[code](https://astr2023.github.io)<br>:star:[code](https://astr2023.github.io/)
  * [Adaptive Assignment for Geometry Aware Local Feature Matching](https://arxiv.org/abs/2207.08427)<br>:star:[code](https://github.com/AbyssGaze/AdaMatcher)特征匹配
  * [DKM: Dense Kernelized Feature Matching for Geometry Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Edstedt_DKM_Dense_Kernelized_Feature_Matching_for_Geometry_Estimation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Parskatt/DKM)
* 紫外线预测
  * [Normal-Guided Garment UV Prediction for Human Re-Texturing](https://arxiv.org/abs/2303.06504)
* vector quantization(矢量量化)
  * [Vector Quantization With Self-Attention for Quality-Independent Representation Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Vector_Quantization_With_Self-Attention_for_Quality-Independent_Representation_Learning_CVPR_2023_paper.pdf)<br>:house:[project](https://see.xidian.edu.cn/faculty/wsdong/Projects/VQSA.htm)
### 扫码CV君微信(注明：CVPR)入微信交流群：
![9475fa20fd5e95235d9fa23ae9587a2](https://user-images.githubusercontent.com/62801906/156720309-de92964f-a6da-464a-b21f-cfb270c13e27.png)

